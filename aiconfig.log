2023-12-09 20:44:25,124 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702172657644256400
2023-12-09 20:44:25,124 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702172657644256400
2023-12-09 20:44:25,124 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702172657644256400
2023-12-09 20:44:25,124 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:44:25,124 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:44:25,124 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:44:25,124 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:44:25,124 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:44:25,124 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:44:25,124 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'temperature': 0.75, 'presence_penalty': 0, 'top_p': 1, 'frequency_penalty': 0, 'model': 'gpt-4', 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702172657644256400
2023-12-09 20:44:25,124 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'temperature': 0.75, 'presence_penalty': 0, 'top_p': 1, 'frequency_penalty': 0, 'model': 'gpt-4', 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702172657644256400
2023-12-09 20:44:25,124 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'temperature': 0.75, 'presence_penalty': 0, 'top_p': 1, 'frequency_penalty': 0, 'model': 'gpt-4', 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702172657644256400
2023-12-09 20:44:38,295 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The GitHub repository named "generative-models" is maintained by Stability AI. It provides a comprehensive suite of generative models including "SDXL-Turbo" for fast text-to-image conversion, "Stable Video Diffusion" for image-to-video conversion, and several versions of "SDXL" diffusion models. All these models are released for research purposes and some require users to request access via Hugging Face.\n\nThe repository leverages a modular, config-driven architecture and uses PyTorch Lightning for training. It has undergone improvements in order to handle all types of conditioning inputs in a single class "GeneralConditioner". It also separates "guiders" from "samplers", making the latter independent of the model. It supports the "denoiser framework" for both training and inference, enabling the option to train continuous time models.\n\nIn addition to providing a streamlit demo for text-to-image and image-to-image sampling, the repository includes features like invisible watermark detection for images generated through its code. The repository also provides guidance on how to build new diffusion models, handle datasets, and train models using provided config examples.\n\nThe repository also provides a comprehensive guide for installation, which requires Python 3.10 and PyTorch 2.0. It uses a PEP 517 compliant packaging method using Hatch.\n\nThis repository is designed for researchers interested in generative models, text-to-image, and image-to-video conversion.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:44:38,295 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The GitHub repository named "generative-models" is maintained by Stability AI. It provides a comprehensive suite of generative models including "SDXL-Turbo" for fast text-to-image conversion, "Stable Video Diffusion" for image-to-video conversion, and several versions of "SDXL" diffusion models. All these models are released for research purposes and some require users to request access via Hugging Face.\n\nThe repository leverages a modular, config-driven architecture and uses PyTorch Lightning for training. It has undergone improvements in order to handle all types of conditioning inputs in a single class "GeneralConditioner". It also separates "guiders" from "samplers", making the latter independent of the model. It supports the "denoiser framework" for both training and inference, enabling the option to train continuous time models.\n\nIn addition to providing a streamlit demo for text-to-image and image-to-image sampling, the repository includes features like invisible watermark detection for images generated through its code. The repository also provides guidance on how to build new diffusion models, handle datasets, and train models using provided config examples.\n\nThe repository also provides a comprehensive guide for installation, which requires Python 3.10 and PyTorch 2.0. It uses a PEP 517 compliant packaging method using Hatch.\n\nThis repository is designed for researchers interested in generative models, text-to-image, and image-to-video conversion.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:44:38,295 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The GitHub repository named "generative-models" is maintained by Stability AI. It provides a comprehensive suite of generative models including "SDXL-Turbo" for fast text-to-image conversion, "Stable Video Diffusion" for image-to-video conversion, and several versions of "SDXL" diffusion models. All these models are released for research purposes and some require users to request access via Hugging Face.\n\nThe repository leverages a modular, config-driven architecture and uses PyTorch Lightning for training. It has undergone improvements in order to handle all types of conditioning inputs in a single class "GeneralConditioner". It also separates "guiders" from "samplers", making the latter independent of the model. It supports the "denoiser framework" for both training and inference, enabling the option to train continuous time models.\n\nIn addition to providing a streamlit demo for text-to-image and image-to-image sampling, the repository includes features like invisible watermark detection for images generated through its code. The repository also provides guidance on how to build new diffusion models, handle datasets, and train models using provided config examples.\n\nThe repository also provides a comprehensive guide for installation, which requires Python 3.10 and PyTorch 2.0. It uses a PEP 517 compliant packaging method using Hatch.\n\nThis repository is designed for researchers interested in generative models, text-to-image, and image-to-video conversion.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:44:38,295 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The GitHub repository named "generative-models" is maintained by Stability AI. It provides a comprehensive suite of generative models including "SDXL-Turbo" for fast text-to-image conversion, "Stable Video Diffusion" for image-to-video conversion, and several versions of "SDXL" diffusion models. All these models are released for research purposes and some require users to request access via Hugging Face.\n\nThe repository leverages a modular, config-driven architecture and uses PyTorch Lightning for training. It has undergone improvements in order to handle all types of conditioning inputs in a single class "GeneralConditioner". It also separates "guiders" from "samplers", making the latter independent of the model. It supports the "denoiser framework" for both training and inference, enabling the option to train continuous time models.\n\nIn addition to providing a streamlit demo for text-to-image and image-to-image sampling, the repository includes features like invisible watermark detection for images generated through its code. The repository also provides guidance on how to build new diffusion models, handle datasets, and train models using provided config examples.\n\nThe repository also provides a comprehensive guide for installation, which requires Python 3.10 and PyTorch 2.0. It uses a PEP 517 compliant packaging method using Hatch.\n\nThis repository is designed for researchers interested in generative models, text-to-image, and image-to-video conversion.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:44:38,295 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The GitHub repository named "generative-models" is maintained by Stability AI. It provides a comprehensive suite of generative models including "SDXL-Turbo" for fast text-to-image conversion, "Stable Video Diffusion" for image-to-video conversion, and several versions of "SDXL" diffusion models. All these models are released for research purposes and some require users to request access via Hugging Face.\n\nThe repository leverages a modular, config-driven architecture and uses PyTorch Lightning for training. It has undergone improvements in order to handle all types of conditioning inputs in a single class "GeneralConditioner". It also separates "guiders" from "samplers", making the latter independent of the model. It supports the "denoiser framework" for both training and inference, enabling the option to train continuous time models.\n\nIn addition to providing a streamlit demo for text-to-image and image-to-image sampling, the repository includes features like invisible watermark detection for images generated through its code. The repository also provides guidance on how to build new diffusion models, handle datasets, and train models using provided config examples.\n\nThe repository also provides a comprehensive guide for installation, which requires Python 3.10 and PyTorch 2.0. It uses a PEP 517 compliant packaging method using Hatch.\n\nThis repository is designed for researchers interested in generative models, text-to-image, and image-to-video conversion.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:44:38,295 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The GitHub repository named "generative-models" is maintained by Stability AI. It provides a comprehensive suite of generative models including "SDXL-Turbo" for fast text-to-image conversion, "Stable Video Diffusion" for image-to-video conversion, and several versions of "SDXL" diffusion models. All these models are released for research purposes and some require users to request access via Hugging Face.\n\nThe repository leverages a modular, config-driven architecture and uses PyTorch Lightning for training. It has undergone improvements in order to handle all types of conditioning inputs in a single class "GeneralConditioner". It also separates "guiders" from "samplers", making the latter independent of the model. It supports the "denoiser framework" for both training and inference, enabling the option to train continuous time models.\n\nIn addition to providing a streamlit demo for text-to-image and image-to-image sampling, the repository includes features like invisible watermark detection for images generated through its code. The repository also provides guidance on how to build new diffusion models, handle datasets, and train models using provided config examples.\n\nThe repository also provides a comprehensive guide for installation, which requires Python 3.10 and PyTorch 2.0. It uses a PEP 517 compliant packaging method using Hatch.\n\nThis repository is designed for researchers interested in generative models, text-to-image, and image-to-video conversion.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:44:38,295 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The GitHub repository named "generative-models" is maintained by Stability AI. It provides a comprehensive suite of generative models including "SDXL-Turbo" for fast text-to-image conversion, "Stable Video Diffusion" for image-to-video conversion, and several versions of "SDXL" diffusion models. All these models are released for research purposes and some require users to request access via Hugging Face.\n\nThe repository leverages a modular, config-driven architecture and uses PyTorch Lightning for training. It has undergone improvements in order to handle all types of conditioning inputs in a single class "GeneralConditioner". It also separates "guiders" from "samplers", making the latter independent of the model. It supports the "denoiser framework" for both training and inference, enabling the option to train continuous time models.\n\nIn addition to providing a streamlit demo for text-to-image and image-to-image sampling, the repository includes features like invisible watermark detection for images generated through its code. The repository also provides guidance on how to build new diffusion models, handle datasets, and train models using provided config examples.\n\nThe repository also provides a comprehensive guide for installation, which requires Python 3.10 and PyTorch 2.0. It uses a PEP 517 compliant packaging method using Hatch.\n\nThis repository is designed for researchers interested in generative models, text-to-image, and image-to-video conversion.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:44:38,295 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The GitHub repository named "generative-models" is maintained by Stability AI. It provides a comprehensive suite of generative models including "SDXL-Turbo" for fast text-to-image conversion, "Stable Video Diffusion" for image-to-video conversion, and several versions of "SDXL" diffusion models. All these models are released for research purposes and some require users to request access via Hugging Face.\n\nThe repository leverages a modular, config-driven architecture and uses PyTorch Lightning for training. It has undergone improvements in order to handle all types of conditioning inputs in a single class "GeneralConditioner". It also separates "guiders" from "samplers", making the latter independent of the model. It supports the "denoiser framework" for both training and inference, enabling the option to train continuous time models.\n\nIn addition to providing a streamlit demo for text-to-image and image-to-image sampling, the repository includes features like invisible watermark detection for images generated through its code. The repository also provides guidance on how to build new diffusion models, handle datasets, and train models using provided config examples.\n\nThe repository also provides a comprehensive guide for installation, which requires Python 3.10 and PyTorch 2.0. It uses a PEP 517 compliant packaging method using Hatch.\n\nThis repository is designed for researchers interested in generative models, text-to-image, and image-to-video conversion.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'temperature': 0.75, 'presence_penalty': 0, 'top_p': 1, 'frequency_penalty': 0, 'model': 'gpt-4', 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'temperature': 0.75, 'presence_penalty': 0, 'top_p': 1, 'frequency_penalty': 0, 'model': 'gpt-4', 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'temperature': 0.75, 'presence_penalty': 0, 'top_p': 1, 'frequency_penalty': 0, 'model': 'gpt-4', 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'temperature': 0.75, 'presence_penalty': 0, 'top_p': 1, 'frequency_penalty': 0, 'model': 'gpt-4', 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'temperature': 0.75, 'presence_penalty': 0, 'top_p': 1, 'frequency_penalty': 0, 'model': 'gpt-4', 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'temperature': 0.75, 'presence_penalty': 0, 'top_p': 1, 'frequency_penalty': 0, 'model': 'gpt-4', 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702172657644256400
2023-12-09 20:49:27,251 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The "generative-models" repository by Stability AI is a collection of generative models, with a focus on text-to-image and image-to-image diffusion models. The repository provides a series of models and updates, the latest being "SDXL-Turbo" and "Stable Video Diffusion". \n\nKey features of the repository:\n\n1. "SDXL-Turbo" is a text-to-image model that is very fast. It comes with a technical report and instructions for installation and usage.\n2. "Stable Video Diffusion" is an image-to-video model intended for research. It was trained to generate 14 frames at 576x1024 resolution given a context frame of the same size. It uses the standard image encoder from SD 2.1, with a "deflickering decoder" replacing the original decoder.\n3. Previous releases include open models under a "CreativeML Open RAIL++-M" license, and diffusion models for research purposes.\n4. The codebase follows a config-driven approach, where submodules are built and combined based on yaml configs. It supports both PyTorch 1.13 and 2.0 for training generative models.\n5. The repository provides detailed instructions for the installation of the models and any necessary dependencies. \n6. It provides a PEP 517 compliant packaging using Hatch for building a distributable wheel.\n7. Inference can be done via a streamlit demo for text-to-image and image-to-image sampling. There is also a script provided to detect invisible watermarks in the generated model output. \n8. Training is supported with example training configs provided in the repository. The repository also supports building new diffusion models.\n9. For large-scale training, it recommends using the data pipelines from their "data pipelines" project. \n\nThe repository provides comprehensive instructions and necessary scripts for the usage, training, and building of new models based on the provided generative models. It is intended to be used for research and development in generative AI and machine learning.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:49:27,251 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The "generative-models" repository by Stability AI is a collection of generative models, with a focus on text-to-image and image-to-image diffusion models. The repository provides a series of models and updates, the latest being "SDXL-Turbo" and "Stable Video Diffusion". \n\nKey features of the repository:\n\n1. "SDXL-Turbo" is a text-to-image model that is very fast. It comes with a technical report and instructions for installation and usage.\n2. "Stable Video Diffusion" is an image-to-video model intended for research. It was trained to generate 14 frames at 576x1024 resolution given a context frame of the same size. It uses the standard image encoder from SD 2.1, with a "deflickering decoder" replacing the original decoder.\n3. Previous releases include open models under a "CreativeML Open RAIL++-M" license, and diffusion models for research purposes.\n4. The codebase follows a config-driven approach, where submodules are built and combined based on yaml configs. It supports both PyTorch 1.13 and 2.0 for training generative models.\n5. The repository provides detailed instructions for the installation of the models and any necessary dependencies. \n6. It provides a PEP 517 compliant packaging using Hatch for building a distributable wheel.\n7. Inference can be done via a streamlit demo for text-to-image and image-to-image sampling. There is also a script provided to detect invisible watermarks in the generated model output. \n8. Training is supported with example training configs provided in the repository. The repository also supports building new diffusion models.\n9. For large-scale training, it recommends using the data pipelines from their "data pipelines" project. \n\nThe repository provides comprehensive instructions and necessary scripts for the usage, training, and building of new models based on the provided generative models. It is intended to be used for research and development in generative AI and machine learning.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:49:27,251 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The "generative-models" repository by Stability AI is a collection of generative models, with a focus on text-to-image and image-to-image diffusion models. The repository provides a series of models and updates, the latest being "SDXL-Turbo" and "Stable Video Diffusion". \n\nKey features of the repository:\n\n1. "SDXL-Turbo" is a text-to-image model that is very fast. It comes with a technical report and instructions for installation and usage.\n2. "Stable Video Diffusion" is an image-to-video model intended for research. It was trained to generate 14 frames at 576x1024 resolution given a context frame of the same size. It uses the standard image encoder from SD 2.1, with a "deflickering decoder" replacing the original decoder.\n3. Previous releases include open models under a "CreativeML Open RAIL++-M" license, and diffusion models for research purposes.\n4. The codebase follows a config-driven approach, where submodules are built and combined based on yaml configs. It supports both PyTorch 1.13 and 2.0 for training generative models.\n5. The repository provides detailed instructions for the installation of the models and any necessary dependencies. \n6. It provides a PEP 517 compliant packaging using Hatch for building a distributable wheel.\n7. Inference can be done via a streamlit demo for text-to-image and image-to-image sampling. There is also a script provided to detect invisible watermarks in the generated model output. \n8. Training is supported with example training configs provided in the repository. The repository also supports building new diffusion models.\n9. For large-scale training, it recommends using the data pipelines from their "data pipelines" project. \n\nThe repository provides comprehensive instructions and necessary scripts for the usage, training, and building of new models based on the provided generative models. It is intended to be used for research and development in generative AI and machine learning.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:49:27,251 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The "generative-models" repository by Stability AI is a collection of generative models, with a focus on text-to-image and image-to-image diffusion models. The repository provides a series of models and updates, the latest being "SDXL-Turbo" and "Stable Video Diffusion". \n\nKey features of the repository:\n\n1. "SDXL-Turbo" is a text-to-image model that is very fast. It comes with a technical report and instructions for installation and usage.\n2. "Stable Video Diffusion" is an image-to-video model intended for research. It was trained to generate 14 frames at 576x1024 resolution given a context frame of the same size. It uses the standard image encoder from SD 2.1, with a "deflickering decoder" replacing the original decoder.\n3. Previous releases include open models under a "CreativeML Open RAIL++-M" license, and diffusion models for research purposes.\n4. The codebase follows a config-driven approach, where submodules are built and combined based on yaml configs. It supports both PyTorch 1.13 and 2.0 for training generative models.\n5. The repository provides detailed instructions for the installation of the models and any necessary dependencies. \n6. It provides a PEP 517 compliant packaging using Hatch for building a distributable wheel.\n7. Inference can be done via a streamlit demo for text-to-image and image-to-image sampling. There is also a script provided to detect invisible watermarks in the generated model output. \n8. Training is supported with example training configs provided in the repository. The repository also supports building new diffusion models.\n9. For large-scale training, it recommends using the data pipelines from their "data pipelines" project. \n\nThe repository provides comprehensive instructions and necessary scripts for the usage, training, and building of new models based on the provided generative models. It is intended to be used for research and development in generative AI and machine learning.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:49:27,251 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The "generative-models" repository by Stability AI is a collection of generative models, with a focus on text-to-image and image-to-image diffusion models. The repository provides a series of models and updates, the latest being "SDXL-Turbo" and "Stable Video Diffusion". \n\nKey features of the repository:\n\n1. "SDXL-Turbo" is a text-to-image model that is very fast. It comes with a technical report and instructions for installation and usage.\n2. "Stable Video Diffusion" is an image-to-video model intended for research. It was trained to generate 14 frames at 576x1024 resolution given a context frame of the same size. It uses the standard image encoder from SD 2.1, with a "deflickering decoder" replacing the original decoder.\n3. Previous releases include open models under a "CreativeML Open RAIL++-M" license, and diffusion models for research purposes.\n4. The codebase follows a config-driven approach, where submodules are built and combined based on yaml configs. It supports both PyTorch 1.13 and 2.0 for training generative models.\n5. The repository provides detailed instructions for the installation of the models and any necessary dependencies. \n6. It provides a PEP 517 compliant packaging using Hatch for building a distributable wheel.\n7. Inference can be done via a streamlit demo for text-to-image and image-to-image sampling. There is also a script provided to detect invisible watermarks in the generated model output. \n8. Training is supported with example training configs provided in the repository. The repository also supports building new diffusion models.\n9. For large-scale training, it recommends using the data pipelines from their "data pipelines" project. \n\nThe repository provides comprehensive instructions and necessary scripts for the usage, training, and building of new models based on the provided generative models. It is intended to be used for research and development in generative AI and machine learning.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:49:27,251 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The "generative-models" repository by Stability AI is a collection of generative models, with a focus on text-to-image and image-to-image diffusion models. The repository provides a series of models and updates, the latest being "SDXL-Turbo" and "Stable Video Diffusion". \n\nKey features of the repository:\n\n1. "SDXL-Turbo" is a text-to-image model that is very fast. It comes with a technical report and instructions for installation and usage.\n2. "Stable Video Diffusion" is an image-to-video model intended for research. It was trained to generate 14 frames at 576x1024 resolution given a context frame of the same size. It uses the standard image encoder from SD 2.1, with a "deflickering decoder" replacing the original decoder.\n3. Previous releases include open models under a "CreativeML Open RAIL++-M" license, and diffusion models for research purposes.\n4. The codebase follows a config-driven approach, where submodules are built and combined based on yaml configs. It supports both PyTorch 1.13 and 2.0 for training generative models.\n5. The repository provides detailed instructions for the installation of the models and any necessary dependencies. \n6. It provides a PEP 517 compliant packaging using Hatch for building a distributable wheel.\n7. Inference can be done via a streamlit demo for text-to-image and image-to-image sampling. There is also a script provided to detect invisible watermarks in the generated model output. \n8. Training is supported with example training configs provided in the repository. The repository also supports building new diffusion models.\n9. For large-scale training, it recommends using the data pipelines from their "data pipelines" project. \n\nThe repository provides comprehensive instructions and necessary scripts for the usage, training, and building of new models based on the provided generative models. It is intended to be used for research and development in generative AI and machine learning.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:49:27,251 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The "generative-models" repository by Stability AI is a collection of generative models, with a focus on text-to-image and image-to-image diffusion models. The repository provides a series of models and updates, the latest being "SDXL-Turbo" and "Stable Video Diffusion". \n\nKey features of the repository:\n\n1. "SDXL-Turbo" is a text-to-image model that is very fast. It comes with a technical report and instructions for installation and usage.\n2. "Stable Video Diffusion" is an image-to-video model intended for research. It was trained to generate 14 frames at 576x1024 resolution given a context frame of the same size. It uses the standard image encoder from SD 2.1, with a "deflickering decoder" replacing the original decoder.\n3. Previous releases include open models under a "CreativeML Open RAIL++-M" license, and diffusion models for research purposes.\n4. The codebase follows a config-driven approach, where submodules are built and combined based on yaml configs. It supports both PyTorch 1.13 and 2.0 for training generative models.\n5. The repository provides detailed instructions for the installation of the models and any necessary dependencies. \n6. It provides a PEP 517 compliant packaging using Hatch for building a distributable wheel.\n7. Inference can be done via a streamlit demo for text-to-image and image-to-image sampling. There is also a script provided to detect invisible watermarks in the generated model output. \n8. Training is supported with example training configs provided in the repository. The repository also supports building new diffusion models.\n9. For large-scale training, it recommends using the data pipelines from their "data pipelines" project. \n\nThe repository provides comprehensive instructions and necessary scripts for the usage, training, and building of new models based on the provided generative models. It is intended to be used for research and development in generative AI and machine learning.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:49:27,251 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The "generative-models" repository by Stability AI is a collection of generative models, with a focus on text-to-image and image-to-image diffusion models. The repository provides a series of models and updates, the latest being "SDXL-Turbo" and "Stable Video Diffusion". \n\nKey features of the repository:\n\n1. "SDXL-Turbo" is a text-to-image model that is very fast. It comes with a technical report and instructions for installation and usage.\n2. "Stable Video Diffusion" is an image-to-video model intended for research. It was trained to generate 14 frames at 576x1024 resolution given a context frame of the same size. It uses the standard image encoder from SD 2.1, with a "deflickering decoder" replacing the original decoder.\n3. Previous releases include open models under a "CreativeML Open RAIL++-M" license, and diffusion models for research purposes.\n4. The codebase follows a config-driven approach, where submodules are built and combined based on yaml configs. It supports both PyTorch 1.13 and 2.0 for training generative models.\n5. The repository provides detailed instructions for the installation of the models and any necessary dependencies. \n6. It provides a PEP 517 compliant packaging using Hatch for building a distributable wheel.\n7. Inference can be done via a streamlit demo for text-to-image and image-to-image sampling. There is also a script provided to detect invisible watermarks in the generated model output. \n8. Training is supported with example training configs provided in the repository. The repository also supports building new diffusion models.\n9. For large-scale training, it recommends using the data pipelines from their "data pipelines" project. \n\nThe repository provides comprehensive instructions and necessary scripts for the usage, training, and building of new models based on the provided generative models. It is intended to be used for research and development in generative AI and machine learning.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:49:27,251 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The "generative-models" repository by Stability AI is a collection of generative models, with a focus on text-to-image and image-to-image diffusion models. The repository provides a series of models and updates, the latest being "SDXL-Turbo" and "Stable Video Diffusion". \n\nKey features of the repository:\n\n1. "SDXL-Turbo" is a text-to-image model that is very fast. It comes with a technical report and instructions for installation and usage.\n2. "Stable Video Diffusion" is an image-to-video model intended for research. It was trained to generate 14 frames at 576x1024 resolution given a context frame of the same size. It uses the standard image encoder from SD 2.1, with a "deflickering decoder" replacing the original decoder.\n3. Previous releases include open models under a "CreativeML Open RAIL++-M" license, and diffusion models for research purposes.\n4. The codebase follows a config-driven approach, where submodules are built and combined based on yaml configs. It supports both PyTorch 1.13 and 2.0 for training generative models.\n5. The repository provides detailed instructions for the installation of the models and any necessary dependencies. \n6. It provides a PEP 517 compliant packaging using Hatch for building a distributable wheel.\n7. Inference can be done via a streamlit demo for text-to-image and image-to-image sampling. There is also a script provided to detect invisible watermarks in the generated model output. \n8. Training is supported with example training configs provided in the repository. The repository also supports building new diffusion models.\n9. For large-scale training, it recommends using the data pipelines from their "data pipelines" project. \n\nThe repository provides comprehensive instructions and necessary scripts for the usage, training, and building of new models based on the provided generative models. It is intended to be used for research and development in generative AI and machine learning.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:49:27,251 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The "generative-models" repository by Stability AI is a collection of generative models, with a focus on text-to-image and image-to-image diffusion models. The repository provides a series of models and updates, the latest being "SDXL-Turbo" and "Stable Video Diffusion". \n\nKey features of the repository:\n\n1. "SDXL-Turbo" is a text-to-image model that is very fast. It comes with a technical report and instructions for installation and usage.\n2. "Stable Video Diffusion" is an image-to-video model intended for research. It was trained to generate 14 frames at 576x1024 resolution given a context frame of the same size. It uses the standard image encoder from SD 2.1, with a "deflickering decoder" replacing the original decoder.\n3. Previous releases include open models under a "CreativeML Open RAIL++-M" license, and diffusion models for research purposes.\n4. The codebase follows a config-driven approach, where submodules are built and combined based on yaml configs. It supports both PyTorch 1.13 and 2.0 for training generative models.\n5. The repository provides detailed instructions for the installation of the models and any necessary dependencies. \n6. It provides a PEP 517 compliant packaging using Hatch for building a distributable wheel.\n7. Inference can be done via a streamlit demo for text-to-image and image-to-image sampling. There is also a script provided to detect invisible watermarks in the generated model output. \n8. Training is supported with example training configs provided in the repository. The repository also supports building new diffusion models.\n9. For large-scale training, it recommends using the data pipelines from their "data pipelines" project. \n\nThe repository provides comprehensive instructions and necessary scripts for the usage, training, and building of new models based on the provided generative models. It is intended to be used for research and development in generative AI and machine learning.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:49:27,251 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The "generative-models" repository by Stability AI is a collection of generative models, with a focus on text-to-image and image-to-image diffusion models. The repository provides a series of models and updates, the latest being "SDXL-Turbo" and "Stable Video Diffusion". \n\nKey features of the repository:\n\n1. "SDXL-Turbo" is a text-to-image model that is very fast. It comes with a technical report and instructions for installation and usage.\n2. "Stable Video Diffusion" is an image-to-video model intended for research. It was trained to generate 14 frames at 576x1024 resolution given a context frame of the same size. It uses the standard image encoder from SD 2.1, with a "deflickering decoder" replacing the original decoder.\n3. Previous releases include open models under a "CreativeML Open RAIL++-M" license, and diffusion models for research purposes.\n4. The codebase follows a config-driven approach, where submodules are built and combined based on yaml configs. It supports both PyTorch 1.13 and 2.0 for training generative models.\n5. The repository provides detailed instructions for the installation of the models and any necessary dependencies. \n6. It provides a PEP 517 compliant packaging using Hatch for building a distributable wheel.\n7. Inference can be done via a streamlit demo for text-to-image and image-to-image sampling. There is also a script provided to detect invisible watermarks in the generated model output. \n8. Training is supported with example training configs provided in the repository. The repository also supports building new diffusion models.\n9. For large-scale training, it recommends using the data pipelines from their "data pipelines" project. \n\nThe repository provides comprehensive instructions and necessary scripts for the usage, training, and building of new models based on the provided generative models. It is intended to be used for research and development in generative AI and machine learning.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:49:27,251 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The "generative-models" repository by Stability AI is a collection of generative models, with a focus on text-to-image and image-to-image diffusion models. The repository provides a series of models and updates, the latest being "SDXL-Turbo" and "Stable Video Diffusion". \n\nKey features of the repository:\n\n1. "SDXL-Turbo" is a text-to-image model that is very fast. It comes with a technical report and instructions for installation and usage.\n2. "Stable Video Diffusion" is an image-to-video model intended for research. It was trained to generate 14 frames at 576x1024 resolution given a context frame of the same size. It uses the standard image encoder from SD 2.1, with a "deflickering decoder" replacing the original decoder.\n3. Previous releases include open models under a "CreativeML Open RAIL++-M" license, and diffusion models for research purposes.\n4. The codebase follows a config-driven approach, where submodules are built and combined based on yaml configs. It supports both PyTorch 1.13 and 2.0 for training generative models.\n5. The repository provides detailed instructions for the installation of the models and any necessary dependencies. \n6. It provides a PEP 517 compliant packaging using Hatch for building a distributable wheel.\n7. Inference can be done via a streamlit demo for text-to-image and image-to-image sampling. There is also a script provided to detect invisible watermarks in the generated model output. \n8. Training is supported with example training configs provided in the repository. The repository also supports building new diffusion models.\n9. For large-scale training, it recommends using the data pipelines from their "data pipelines" project. \n\nThe repository provides comprehensive instructions and necessary scripts for the usage, training, and building of new models based on the provided generative models. It is intended to be used for research and development in generative AI and machine learning.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'chatbot', 'params': {'user_question': 'Hi bob', 'file_structure': ''}, 'options': None, 'kwargs': {}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'chatbot', 'params': {'user_question': 'Hi bob', 'file_structure': ''}, 'options': None, 'kwargs': {}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'chatbot', 'params': {'user_question': 'Hi bob', 'file_structure': ''}, 'options': None, 'kwargs': {}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'chatbot', 'params': {'user_question': 'Hi bob', 'file_structure': ''}, 'options': None, 'kwargs': {}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'chatbot', 'params': {'user_question': 'Hi bob', 'file_structure': ''}, 'options': None, 'kwargs': {}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'chatbot', 'params': {'user_question': 'Hi bob', 'file_structure': ''}, 'options': None, 'kwargs': {}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'options': None, 'parameters': {'user_question': 'Hi bob', 'file_structure': ''}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'options': None, 'parameters': {'user_question': 'Hi bob', 'file_structure': ''}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'options': None, 'parameters': {'user_question': 'Hi bob', 'file_structure': ''}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'options': None, 'parameters': {'user_question': 'Hi bob', 'file_structure': ''}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'options': None, 'parameters': {'user_question': 'Hi bob', 'file_structure': ''}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'options': None, 'parameters': {'user_question': 'Hi bob', 'file_structure': ''}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'params': {'user_question': 'Hi bob', 'file_structure': ''}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'params': {'user_question': 'Hi bob', 'file_structure': ''}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'params': {'user_question': 'Hi bob', 'file_structure': ''}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'params': {'user_question': 'Hi bob', 'file_structure': ''}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'params': {'user_question': 'Hi bob', 'file_structure': ''}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'params': {'user_question': 'Hi bob', 'file_structure': ''}} ts_ns=1702173609569775400
2023-12-09 21:02:38,051 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'presence_penalty': 0, 'temperature': 0.75, 'model': 'gpt-4', 'top_p': 1, 'frequency_penalty': 0, 'messages': [{'content': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is . The READMEfile is:\n', 'role': 'user'}, {'content': 'README Summary: \n\nFile Structure:\n\n\nUser Question: Hi bob', 'role': 'user'}]}} ts_ns=1702173609569775400
2023-12-09 21:02:38,051 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'presence_penalty': 0, 'temperature': 0.75, 'model': 'gpt-4', 'top_p': 1, 'frequency_penalty': 0, 'messages': [{'content': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is . The READMEfile is:\n', 'role': 'user'}, {'content': 'README Summary: \n\nFile Structure:\n\n\nUser Question: Hi bob', 'role': 'user'}]}} ts_ns=1702173609569775400
2023-12-09 21:02:38,051 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'presence_penalty': 0, 'temperature': 0.75, 'model': 'gpt-4', 'top_p': 1, 'frequency_penalty': 0, 'messages': [{'content': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is . The READMEfile is:\n', 'role': 'user'}, {'content': 'README Summary: \n\nFile Structure:\n\n\nUser Question: Hi bob', 'role': 'user'}]}} ts_ns=1702173609569775400
2023-12-09 21:02:38,051 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'presence_penalty': 0, 'temperature': 0.75, 'model': 'gpt-4', 'top_p': 1, 'frequency_penalty': 0, 'messages': [{'content': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is . The READMEfile is:\n', 'role': 'user'}, {'content': 'README Summary: \n\nFile Structure:\n\n\nUser Question: Hi bob', 'role': 'user'}]}} ts_ns=1702173609569775400
2023-12-09 21:02:38,051 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'presence_penalty': 0, 'temperature': 0.75, 'model': 'gpt-4', 'top_p': 1, 'frequency_penalty': 0, 'messages': [{'content': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is . The READMEfile is:\n', 'role': 'user'}, {'content': 'README Summary: \n\nFile Structure:\n\n\nUser Question: Hi bob', 'role': 'user'}]}} ts_ns=1702173609569775400
2023-12-09 21:02:38,051 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'presence_penalty': 0, 'temperature': 0.75, 'model': 'gpt-4', 'top_p': 1, 'frequency_penalty': 0, 'messages': [{'content': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is . The READMEfile is:\n', 'role': 'user'}, {'content': 'README Summary: \n\nFile Structure:\n\n\nUser Question: Hi bob', 'role': 'user'}]}} ts_ns=1702173609569775400
2023-12-09 21:02:40,247 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but the information provided is incomplete. To give a summary of the repository, I need the content of the README file and the file structure of the repository. Could you please provide those?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173609569775400
2023-12-09 21:02:40,247 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but the information provided is incomplete. To give a summary of the repository, I need the content of the README file and the file structure of the repository. Could you please provide those?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173609569775400
2023-12-09 21:02:40,247 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but the information provided is incomplete. To give a summary of the repository, I need the content of the README file and the file structure of the repository. Could you please provide those?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173609569775400
2023-12-09 21:02:40,247 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but the information provided is incomplete. To give a summary of the repository, I need the content of the README file and the file structure of the repository. Could you please provide those?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173609569775400
2023-12-09 21:02:40,247 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but the information provided is incomplete. To give a summary of the repository, I need the content of the README file and the file structure of the repository. Could you please provide those?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173609569775400
2023-12-09 21:02:40,247 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but the information provided is incomplete. To give a summary of the repository, I need the content of the README file and the file structure of the repository. Could you please provide those?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173609569775400
2023-12-09 21:02:40,247 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but the information provided is incomplete. To give a summary of the repository, I need the content of the README file and the file structure of the repository. Could you please provide those?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173609569775400
2023-12-09 21:02:40,247 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but the information provided is incomplete. To give a summary of the repository, I need the content of the README file and the file structure of the repository. Could you please provide those?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173609569775400
2023-12-09 21:02:40,247 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but the information provided is incomplete. To give a summary of the repository, I need the content of the README file and the file structure of the repository. Could you please provide those?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173609569775400
2023-12-09 21:02:40,247 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but the information provided is incomplete. To give a summary of the repository, I need the content of the README file and the file structure of the repository. Could you please provide those?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173609569775400
2023-12-09 21:02:40,247 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but the information provided is incomplete. To give a summary of the repository, I need the content of the README file and the file structure of the repository. Could you please provide those?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173609569775400
2023-12-09 21:02:40,247 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but the information provided is incomplete. To give a summary of the repository, I need the content of the README file and the file structure of the repository. Could you please provide those?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173609569775400
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702173808368226500
2023-12-09 21:03:48,645 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'frequency_penalty': 0, 'model': 'gpt-4', 'top_p': 1, 'presence_penalty': 0, 'temperature': 0.75, 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702173808368226500
2023-12-09 21:03:48,645 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'frequency_penalty': 0, 'model': 'gpt-4', 'top_p': 1, 'presence_penalty': 0, 'temperature': 0.75, 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702173808368226500
2023-12-09 21:03:48,645 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'frequency_penalty': 0, 'model': 'gpt-4', 'top_p': 1, 'presence_penalty': 0, 'temperature': 0.75, 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702173808368226500
2023-12-09 21:03:48,645 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'frequency_penalty': 0, 'model': 'gpt-4', 'top_p': 1, 'presence_penalty': 0, 'temperature': 0.75, 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702173808368226500
2023-12-09 21:03:48,645 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'frequency_penalty': 0, 'model': 'gpt-4', 'top_p': 1, 'presence_penalty': 0, 'temperature': 0.75, 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702173808368226500
2023-12-09 21:03:48,645 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'frequency_penalty': 0, 'model': 'gpt-4', 'top_p': 1, 'presence_penalty': 0, 'temperature': 0.75, 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702173808368226500
2023-12-09 21:04:05,339 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The generative-models repository by Stability AI contains research and development work focusing on generative models, specifically in the area of text-to-image and image-to-image model generation. The repository includes various model releases, such as SDXL-Turbo, Stable Video Diffusion, SDXL-base-1.0, SDXL-refiner-1.0, SDXL-base-0.9, and SDXL-refiner-0.9. \n\nThe SDXL-Turbo and Stable Video Diffusion models are the latest releases, with SDXL-Turbo providing a lightning-fast text-to-image model, and Stable Video Diffusion providing an image-to-video model for research purposes. The SDXL models are open-source and come with a CreativeML Open RAIL++-M license. \n\nThe repository adopts a config-driven approach for the construction and combination of submodules, using the instantiate_from_config() function on objects defined in yaml configs. The core diffusion model class is referred to as DiffusionEngine.\n\nThe repository also provides detailed installation instructions for setting up the environment, including steps for cloning the repository, setting up virtualenv, installing sgm, and installing sdata for training. After installation, sample training configurations can be found under the 'configs/example_training' directory.\n\nThe project uses PEP 517 compliant packaging using Hatch for building a distributable wheel. The repository also includes an Inference section, providing details on how to run the demo for text-to-image and image-to-image sampling. \n\nFor watermark detection, the repository includes a script that detects an invisible watermark embedded into the model output. \n\nLastly, the repository contains a training section, detailing how to launch training and build new diffusion models. It also provides a guide on dataset handling for large scale training.", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:05,339 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The generative-models repository by Stability AI contains research and development work focusing on generative models, specifically in the area of text-to-image and image-to-image model generation. The repository includes various model releases, such as SDXL-Turbo, Stable Video Diffusion, SDXL-base-1.0, SDXL-refiner-1.0, SDXL-base-0.9, and SDXL-refiner-0.9. \n\nThe SDXL-Turbo and Stable Video Diffusion models are the latest releases, with SDXL-Turbo providing a lightning-fast text-to-image model, and Stable Video Diffusion providing an image-to-video model for research purposes. The SDXL models are open-source and come with a CreativeML Open RAIL++-M license. \n\nThe repository adopts a config-driven approach for the construction and combination of submodules, using the instantiate_from_config() function on objects defined in yaml configs. The core diffusion model class is referred to as DiffusionEngine.\n\nThe repository also provides detailed installation instructions for setting up the environment, including steps for cloning the repository, setting up virtualenv, installing sgm, and installing sdata for training. After installation, sample training configurations can be found under the 'configs/example_training' directory.\n\nThe project uses PEP 517 compliant packaging using Hatch for building a distributable wheel. The repository also includes an Inference section, providing details on how to run the demo for text-to-image and image-to-image sampling. \n\nFor watermark detection, the repository includes a script that detects an invisible watermark embedded into the model output. \n\nLastly, the repository contains a training section, detailing how to launch training and build new diffusion models. It also provides a guide on dataset handling for large scale training.", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:05,339 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The generative-models repository by Stability AI contains research and development work focusing on generative models, specifically in the area of text-to-image and image-to-image model generation. The repository includes various model releases, such as SDXL-Turbo, Stable Video Diffusion, SDXL-base-1.0, SDXL-refiner-1.0, SDXL-base-0.9, and SDXL-refiner-0.9. \n\nThe SDXL-Turbo and Stable Video Diffusion models are the latest releases, with SDXL-Turbo providing a lightning-fast text-to-image model, and Stable Video Diffusion providing an image-to-video model for research purposes. The SDXL models are open-source and come with a CreativeML Open RAIL++-M license. \n\nThe repository adopts a config-driven approach for the construction and combination of submodules, using the instantiate_from_config() function on objects defined in yaml configs. The core diffusion model class is referred to as DiffusionEngine.\n\nThe repository also provides detailed installation instructions for setting up the environment, including steps for cloning the repository, setting up virtualenv, installing sgm, and installing sdata for training. After installation, sample training configurations can be found under the 'configs/example_training' directory.\n\nThe project uses PEP 517 compliant packaging using Hatch for building a distributable wheel. The repository also includes an Inference section, providing details on how to run the demo for text-to-image and image-to-image sampling. \n\nFor watermark detection, the repository includes a script that detects an invisible watermark embedded into the model output. \n\nLastly, the repository contains a training section, detailing how to launch training and build new diffusion models. It also provides a guide on dataset handling for large scale training.", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:05,339 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The generative-models repository by Stability AI contains research and development work focusing on generative models, specifically in the area of text-to-image and image-to-image model generation. The repository includes various model releases, such as SDXL-Turbo, Stable Video Diffusion, SDXL-base-1.0, SDXL-refiner-1.0, SDXL-base-0.9, and SDXL-refiner-0.9. \n\nThe SDXL-Turbo and Stable Video Diffusion models are the latest releases, with SDXL-Turbo providing a lightning-fast text-to-image model, and Stable Video Diffusion providing an image-to-video model for research purposes. The SDXL models are open-source and come with a CreativeML Open RAIL++-M license. \n\nThe repository adopts a config-driven approach for the construction and combination of submodules, using the instantiate_from_config() function on objects defined in yaml configs. The core diffusion model class is referred to as DiffusionEngine.\n\nThe repository also provides detailed installation instructions for setting up the environment, including steps for cloning the repository, setting up virtualenv, installing sgm, and installing sdata for training. After installation, sample training configurations can be found under the 'configs/example_training' directory.\n\nThe project uses PEP 517 compliant packaging using Hatch for building a distributable wheel. The repository also includes an Inference section, providing details on how to run the demo for text-to-image and image-to-image sampling. \n\nFor watermark detection, the repository includes a script that detects an invisible watermark embedded into the model output. \n\nLastly, the repository contains a training section, detailing how to launch training and build new diffusion models. It also provides a guide on dataset handling for large scale training.", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:05,339 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The generative-models repository by Stability AI contains research and development work focusing on generative models, specifically in the area of text-to-image and image-to-image model generation. The repository includes various model releases, such as SDXL-Turbo, Stable Video Diffusion, SDXL-base-1.0, SDXL-refiner-1.0, SDXL-base-0.9, and SDXL-refiner-0.9. \n\nThe SDXL-Turbo and Stable Video Diffusion models are the latest releases, with SDXL-Turbo providing a lightning-fast text-to-image model, and Stable Video Diffusion providing an image-to-video model for research purposes. The SDXL models are open-source and come with a CreativeML Open RAIL++-M license. \n\nThe repository adopts a config-driven approach for the construction and combination of submodules, using the instantiate_from_config() function on objects defined in yaml configs. The core diffusion model class is referred to as DiffusionEngine.\n\nThe repository also provides detailed installation instructions for setting up the environment, including steps for cloning the repository, setting up virtualenv, installing sgm, and installing sdata for training. After installation, sample training configurations can be found under the 'configs/example_training' directory.\n\nThe project uses PEP 517 compliant packaging using Hatch for building a distributable wheel. The repository also includes an Inference section, providing details on how to run the demo for text-to-image and image-to-image sampling. \n\nFor watermark detection, the repository includes a script that detects an invisible watermark embedded into the model output. \n\nLastly, the repository contains a training section, detailing how to launch training and build new diffusion models. It also provides a guide on dataset handling for large scale training.", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:05,339 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The generative-models repository by Stability AI contains research and development work focusing on generative models, specifically in the area of text-to-image and image-to-image model generation. The repository includes various model releases, such as SDXL-Turbo, Stable Video Diffusion, SDXL-base-1.0, SDXL-refiner-1.0, SDXL-base-0.9, and SDXL-refiner-0.9. \n\nThe SDXL-Turbo and Stable Video Diffusion models are the latest releases, with SDXL-Turbo providing a lightning-fast text-to-image model, and Stable Video Diffusion providing an image-to-video model for research purposes. The SDXL models are open-source and come with a CreativeML Open RAIL++-M license. \n\nThe repository adopts a config-driven approach for the construction and combination of submodules, using the instantiate_from_config() function on objects defined in yaml configs. The core diffusion model class is referred to as DiffusionEngine.\n\nThe repository also provides detailed installation instructions for setting up the environment, including steps for cloning the repository, setting up virtualenv, installing sgm, and installing sdata for training. After installation, sample training configurations can be found under the 'configs/example_training' directory.\n\nThe project uses PEP 517 compliant packaging using Hatch for building a distributable wheel. The repository also includes an Inference section, providing details on how to run the demo for text-to-image and image-to-image sampling. \n\nFor watermark detection, the repository includes a script that detects an invisible watermark embedded into the model output. \n\nLastly, the repository contains a training section, detailing how to launch training and build new diffusion models. It also provides a guide on dataset handling for large scale training.", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:05,339 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The generative-models repository by Stability AI contains research and development work focusing on generative models, specifically in the area of text-to-image and image-to-image model generation. The repository includes various model releases, such as SDXL-Turbo, Stable Video Diffusion, SDXL-base-1.0, SDXL-refiner-1.0, SDXL-base-0.9, and SDXL-refiner-0.9. \n\nThe SDXL-Turbo and Stable Video Diffusion models are the latest releases, with SDXL-Turbo providing a lightning-fast text-to-image model, and Stable Video Diffusion providing an image-to-video model for research purposes. The SDXL models are open-source and come with a CreativeML Open RAIL++-M license. \n\nThe repository adopts a config-driven approach for the construction and combination of submodules, using the instantiate_from_config() function on objects defined in yaml configs. The core diffusion model class is referred to as DiffusionEngine.\n\nThe repository also provides detailed installation instructions for setting up the environment, including steps for cloning the repository, setting up virtualenv, installing sgm, and installing sdata for training. After installation, sample training configurations can be found under the 'configs/example_training' directory.\n\nThe project uses PEP 517 compliant packaging using Hatch for building a distributable wheel. The repository also includes an Inference section, providing details on how to run the demo for text-to-image and image-to-image sampling. \n\nFor watermark detection, the repository includes a script that detects an invisible watermark embedded into the model output. \n\nLastly, the repository contains a training section, detailing how to launch training and build new diffusion models. It also provides a guide on dataset handling for large scale training.", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:05,339 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The generative-models repository by Stability AI contains research and development work focusing on generative models, specifically in the area of text-to-image and image-to-image model generation. The repository includes various model releases, such as SDXL-Turbo, Stable Video Diffusion, SDXL-base-1.0, SDXL-refiner-1.0, SDXL-base-0.9, and SDXL-refiner-0.9. \n\nThe SDXL-Turbo and Stable Video Diffusion models are the latest releases, with SDXL-Turbo providing a lightning-fast text-to-image model, and Stable Video Diffusion providing an image-to-video model for research purposes. The SDXL models are open-source and come with a CreativeML Open RAIL++-M license. \n\nThe repository adopts a config-driven approach for the construction and combination of submodules, using the instantiate_from_config() function on objects defined in yaml configs. The core diffusion model class is referred to as DiffusionEngine.\n\nThe repository also provides detailed installation instructions for setting up the environment, including steps for cloning the repository, setting up virtualenv, installing sgm, and installing sdata for training. After installation, sample training configurations can be found under the 'configs/example_training' directory.\n\nThe project uses PEP 517 compliant packaging using Hatch for building a distributable wheel. The repository also includes an Inference section, providing details on how to run the demo for text-to-image and image-to-image sampling. \n\nFor watermark detection, the repository includes a script that detects an invisible watermark embedded into the model output. \n\nLastly, the repository contains a training section, detailing how to launch training and build new diffusion models. It also provides a guide on dataset handling for large scale training.", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:05,339 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The generative-models repository by Stability AI contains research and development work focusing on generative models, specifically in the area of text-to-image and image-to-image model generation. The repository includes various model releases, such as SDXL-Turbo, Stable Video Diffusion, SDXL-base-1.0, SDXL-refiner-1.0, SDXL-base-0.9, and SDXL-refiner-0.9. \n\nThe SDXL-Turbo and Stable Video Diffusion models are the latest releases, with SDXL-Turbo providing a lightning-fast text-to-image model, and Stable Video Diffusion providing an image-to-video model for research purposes. The SDXL models are open-source and come with a CreativeML Open RAIL++-M license. \n\nThe repository adopts a config-driven approach for the construction and combination of submodules, using the instantiate_from_config() function on objects defined in yaml configs. The core diffusion model class is referred to as DiffusionEngine.\n\nThe repository also provides detailed installation instructions for setting up the environment, including steps for cloning the repository, setting up virtualenv, installing sgm, and installing sdata for training. After installation, sample training configurations can be found under the 'configs/example_training' directory.\n\nThe project uses PEP 517 compliant packaging using Hatch for building a distributable wheel. The repository also includes an Inference section, providing details on how to run the demo for text-to-image and image-to-image sampling. \n\nFor watermark detection, the repository includes a script that detects an invisible watermark embedded into the model output. \n\nLastly, the repository contains a training section, detailing how to launch training and build new diffusion models. It also provides a guide on dataset handling for large scale training.", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:05,339 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The generative-models repository by Stability AI contains research and development work focusing on generative models, specifically in the area of text-to-image and image-to-image model generation. The repository includes various model releases, such as SDXL-Turbo, Stable Video Diffusion, SDXL-base-1.0, SDXL-refiner-1.0, SDXL-base-0.9, and SDXL-refiner-0.9. \n\nThe SDXL-Turbo and Stable Video Diffusion models are the latest releases, with SDXL-Turbo providing a lightning-fast text-to-image model, and Stable Video Diffusion providing an image-to-video model for research purposes. The SDXL models are open-source and come with a CreativeML Open RAIL++-M license. \n\nThe repository adopts a config-driven approach for the construction and combination of submodules, using the instantiate_from_config() function on objects defined in yaml configs. The core diffusion model class is referred to as DiffusionEngine.\n\nThe repository also provides detailed installation instructions for setting up the environment, including steps for cloning the repository, setting up virtualenv, installing sgm, and installing sdata for training. After installation, sample training configurations can be found under the 'configs/example_training' directory.\n\nThe project uses PEP 517 compliant packaging using Hatch for building a distributable wheel. The repository also includes an Inference section, providing details on how to run the demo for text-to-image and image-to-image sampling. \n\nFor watermark detection, the repository includes a script that detects an invisible watermark embedded into the model output. \n\nLastly, the repository contains a training section, detailing how to launch training and build new diffusion models. It also provides a guide on dataset handling for large scale training.", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:05,339 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The generative-models repository by Stability AI contains research and development work focusing on generative models, specifically in the area of text-to-image and image-to-image model generation. The repository includes various model releases, such as SDXL-Turbo, Stable Video Diffusion, SDXL-base-1.0, SDXL-refiner-1.0, SDXL-base-0.9, and SDXL-refiner-0.9. \n\nThe SDXL-Turbo and Stable Video Diffusion models are the latest releases, with SDXL-Turbo providing a lightning-fast text-to-image model, and Stable Video Diffusion providing an image-to-video model for research purposes. The SDXL models are open-source and come with a CreativeML Open RAIL++-M license. \n\nThe repository adopts a config-driven approach for the construction and combination of submodules, using the instantiate_from_config() function on objects defined in yaml configs. The core diffusion model class is referred to as DiffusionEngine.\n\nThe repository also provides detailed installation instructions for setting up the environment, including steps for cloning the repository, setting up virtualenv, installing sgm, and installing sdata for training. After installation, sample training configurations can be found under the 'configs/example_training' directory.\n\nThe project uses PEP 517 compliant packaging using Hatch for building a distributable wheel. The repository also includes an Inference section, providing details on how to run the demo for text-to-image and image-to-image sampling. \n\nFor watermark detection, the repository includes a script that detects an invisible watermark embedded into the model output. \n\nLastly, the repository contains a training section, detailing how to launch training and build new diffusion models. It also provides a guide on dataset handling for large scale training.", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:05,339 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The generative-models repository by Stability AI contains research and development work focusing on generative models, specifically in the area of text-to-image and image-to-image model generation. The repository includes various model releases, such as SDXL-Turbo, Stable Video Diffusion, SDXL-base-1.0, SDXL-refiner-1.0, SDXL-base-0.9, and SDXL-refiner-0.9. \n\nThe SDXL-Turbo and Stable Video Diffusion models are the latest releases, with SDXL-Turbo providing a lightning-fast text-to-image model, and Stable Video Diffusion providing an image-to-video model for research purposes. The SDXL models are open-source and come with a CreativeML Open RAIL++-M license. \n\nThe repository adopts a config-driven approach for the construction and combination of submodules, using the instantiate_from_config() function on objects defined in yaml configs. The core diffusion model class is referred to as DiffusionEngine.\n\nThe repository also provides detailed installation instructions for setting up the environment, including steps for cloning the repository, setting up virtualenv, installing sgm, and installing sdata for training. After installation, sample training configurations can be found under the 'configs/example_training' directory.\n\nThe project uses PEP 517 compliant packaging using Hatch for building a distributable wheel. The repository also includes an Inference section, providing details on how to run the demo for text-to-image and image-to-image sampling. \n\nFor watermark detection, the repository includes a script that detects an invisible watermark embedded into the model output. \n\nLastly, the repository contains a training section, detailing how to launch training and build new diffusion models. It also provides a guide on dataset handling for large scale training.", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'chatbot', 'params': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}, 'options': None, 'kwargs': {}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'chatbot', 'params': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}, 'options': None, 'kwargs': {}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'chatbot', 'params': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}, 'options': None, 'kwargs': {}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'chatbot', 'params': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}, 'options': None, 'kwargs': {}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'chatbot', 'params': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}, 'options': None, 'kwargs': {}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'chatbot', 'params': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}, 'options': None, 'kwargs': {}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'chatbot', 'params': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}, 'options': None, 'kwargs': {}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'options': None, 'parameters': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'options': None, 'parameters': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'options': None, 'parameters': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'options': None, 'parameters': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'options': None, 'parameters': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'options': None, 'parameters': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'options': None, 'parameters': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'params': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'params': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'params': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'params': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'params': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'params': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'params': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}} ts_ns=1702173808368226500
2023-12-09 21:04:26,899 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'frequency_penalty': 0, 'model': 'gpt-4', 'top_p': 1, 'presence_penalty': 0, 'temperature': 0.75, 'messages': [{'content': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is . The READMEfile is:\n', 'role': 'user'}, {'content': 'README Summary: \n\nFile Structure:\n\n\nUser Question: Where can i update wrappers for diffusion models', 'role': 'user'}]}} ts_ns=1702173808368226500
2023-12-09 21:04:26,899 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'frequency_penalty': 0, 'model': 'gpt-4', 'top_p': 1, 'presence_penalty': 0, 'temperature': 0.75, 'messages': [{'content': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is . The READMEfile is:\n', 'role': 'user'}, {'content': 'README Summary: \n\nFile Structure:\n\n\nUser Question: Where can i update wrappers for diffusion models', 'role': 'user'}]}} ts_ns=1702173808368226500
2023-12-09 21:04:26,899 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'frequency_penalty': 0, 'model': 'gpt-4', 'top_p': 1, 'presence_penalty': 0, 'temperature': 0.75, 'messages': [{'content': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is . The READMEfile is:\n', 'role': 'user'}, {'content': 'README Summary: \n\nFile Structure:\n\n\nUser Question: Where can i update wrappers for diffusion models', 'role': 'user'}]}} ts_ns=1702173808368226500
2023-12-09 21:04:26,899 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'frequency_penalty': 0, 'model': 'gpt-4', 'top_p': 1, 'presence_penalty': 0, 'temperature': 0.75, 'messages': [{'content': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is . The READMEfile is:\n', 'role': 'user'}, {'content': 'README Summary: \n\nFile Structure:\n\n\nUser Question: Where can i update wrappers for diffusion models', 'role': 'user'}]}} ts_ns=1702173808368226500
2023-12-09 21:04:26,899 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'frequency_penalty': 0, 'model': 'gpt-4', 'top_p': 1, 'presence_penalty': 0, 'temperature': 0.75, 'messages': [{'content': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is . The READMEfile is:\n', 'role': 'user'}, {'content': 'README Summary: \n\nFile Structure:\n\n\nUser Question: Where can i update wrappers for diffusion models', 'role': 'user'}]}} ts_ns=1702173808368226500
2023-12-09 21:04:26,899 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'frequency_penalty': 0, 'model': 'gpt-4', 'top_p': 1, 'presence_penalty': 0, 'temperature': 0.75, 'messages': [{'content': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is . The READMEfile is:\n', 'role': 'user'}, {'content': 'README Summary: \n\nFile Structure:\n\n\nUser Question: Where can i update wrappers for diffusion models', 'role': 'user'}]}} ts_ns=1702173808368226500
2023-12-09 21:04:26,899 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'frequency_penalty': 0, 'model': 'gpt-4', 'top_p': 1, 'presence_penalty': 0, 'temperature': 0.75, 'messages': [{'content': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is . The READMEfile is:\n', 'role': 'user'}, {'content': 'README Summary: \n\nFile Structure:\n\n\nUser Question: Where can i update wrappers for diffusion models', 'role': 'user'}]}} ts_ns=1702173808368226500
2023-12-09 21:04:29,194 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but you haven't provided any content for the README or the file structure of the repository. I need this information to be able to help you. Could you please provide the details?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:29,194 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but you haven't provided any content for the README or the file structure of the repository. I need this information to be able to help you. Could you please provide the details?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:29,194 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but you haven't provided any content for the README or the file structure of the repository. I need this information to be able to help you. Could you please provide the details?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:29,194 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but you haven't provided any content for the README or the file structure of the repository. I need this information to be able to help you. Could you please provide the details?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:29,194 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but you haven't provided any content for the README or the file structure of the repository. I need this information to be able to help you. Could you please provide the details?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:29,194 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but you haven't provided any content for the README or the file structure of the repository. I need this information to be able to help you. Could you please provide the details?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:29,194 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but you haven't provided any content for the README or the file structure of the repository. I need this information to be able to help you. Could you please provide the details?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:29,195 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but you haven't provided any content for the README or the file structure of the repository. I need this information to be able to help you. Could you please provide the details?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:29,195 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but you haven't provided any content for the README or the file structure of the repository. I need this information to be able to help you. Could you please provide the details?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:29,195 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but you haven't provided any content for the README or the file structure of the repository. I need this information to be able to help you. Could you please provide the details?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:29,195 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but you haven't provided any content for the README or the file structure of the repository. I need this information to be able to help you. Could you please provide the details?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:29,195 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but you haven't provided any content for the README or the file structure of the repository. I need this information to be able to help you. Could you please provide the details?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:29,195 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but you haven't provided any content for the README or the file structure of the repository. I need this information to be able to help you. Could you please provide the details?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:29,195 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but you haven't provided any content for the README or the file structure of the repository. I need this information to be able to help you. Could you please provide the details?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
