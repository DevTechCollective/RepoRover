<<<<<<< HEAD
2023-12-09 20:23:19,709 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702171389905502100
2023-12-09 20:23:19,709 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702171389905502100
2023-12-09 20:23:19,709 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702171389905502100
2023-12-09 20:23:19,709 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702171389905502100
2023-12-09 20:23:19,709 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': 'Generative Models', 'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.'}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702171389905502100
2023-12-09 20:23:19,709 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': 'Generative Models', 'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.'}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702171389905502100
2023-12-09 20:23:19,709 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': 'Generative Models', 'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.'}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702171389905502100
2023-12-09 20:23:19,709 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': 'Generative Models', 'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.'}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702171389905502100
2023-12-09 20:28:36,327 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702171702219891300
2023-12-09 20:28:36,327 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702171702219891300
2023-12-09 20:28:36,327 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702171702219891300
2023-12-09 20:28:36,327 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702171702219891300
2023-12-09 20:28:36,327 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': 'Generative Models', 'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.'}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702171702219891300
2023-12-09 20:28:36,327 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': 'Generative Models', 'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.'}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702171702219891300
2023-12-09 20:28:36,327 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': 'Generative Models', 'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.'}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702171702219891300
2023-12-09 20:28:36,327 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': 'Generative Models', 'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.'}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702171702219891300
2023-12-09 20:28:36,327 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': 'Generative Models', 'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.'}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702171702219891300
2023-12-09 20:28:36,327 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': 'Generative Models', 'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.'}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702171702219891300
2023-12-09 20:28:36,327 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': 'Generative Models', 'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.'}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702171702219891300
2023-12-09 20:28:36,327 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': 'Generative Models', 'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.'}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702171702219891300
2023-12-09 20:28:36,340 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'top_p': 1, 'presence_penalty': 0, 'temperature': 0.75, 'frequency_penalty': 0, 'model': 'gpt-4', 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702171702219891300
2023-12-09 20:28:36,340 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'top_p': 1, 'presence_penalty': 0, 'temperature': 0.75, 'frequency_penalty': 0, 'model': 'gpt-4', 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702171702219891300
2023-12-09 20:28:36,340 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'top_p': 1, 'presence_penalty': 0, 'temperature': 0.75, 'frequency_penalty': 0, 'model': 'gpt-4', 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702171702219891300
2023-12-09 20:28:36,340 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'top_p': 1, 'presence_penalty': 0, 'temperature': 0.75, 'frequency_penalty': 0, 'model': 'gpt-4', 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702171702219891300
2023-12-09 20:28:54,195 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The generative-models repository by Stability AI contains a variety of generative models that use a config-driven approach to build and combine submodules. The repository has a strong emphasis on modularity and has numerous examples in the `configs/` directory. It uses the PyTorch Lightning framework for training.\n\nSeveral releases are announced in the README file, such as SDXL-Turbo, a fast text-to-image model, Stable Video Diffusion, an image-to-video model, and several versions of SDXL. Instructions for accessing these models for research are provided, including guidance on using Hugging Face to request access to certain models. \n\nThe README also contains a comprehensive guide on installing and setting up the repository. This includes cloning the repo, setting up a virtual environment, and installing required packages from PyPI. The repository supports both `pytorch1.13` and `pytorch2`.\n\nThere are also guidelines on how to use the models for inferencing, including instructions to run a Streamlit demo for text-to-image and image-to-image sampling. The repository uses invisible watermarking to embed an invisible watermark into the model output, and a script is provided for detecting this watermark.\n\nDetails are provided on how to train the models, including example training configurations in `configs/example_training`. There are also detailed instructions on how to build new diffusion models, including how to configure the conditioner, network, and loss.\n\nFor large scale training, the repository recommends using data pipelines from their data pipelines project. For smaller datasets, they should be defined within the repository itself.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702171702219891300
2023-12-09 20:28:54,195 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The generative-models repository by Stability AI contains a variety of generative models that use a config-driven approach to build and combine submodules. The repository has a strong emphasis on modularity and has numerous examples in the `configs/` directory. It uses the PyTorch Lightning framework for training.\n\nSeveral releases are announced in the README file, such as SDXL-Turbo, a fast text-to-image model, Stable Video Diffusion, an image-to-video model, and several versions of SDXL. Instructions for accessing these models for research are provided, including guidance on using Hugging Face to request access to certain models. \n\nThe README also contains a comprehensive guide on installing and setting up the repository. This includes cloning the repo, setting up a virtual environment, and installing required packages from PyPI. The repository supports both `pytorch1.13` and `pytorch2`.\n\nThere are also guidelines on how to use the models for inferencing, including instructions to run a Streamlit demo for text-to-image and image-to-image sampling. The repository uses invisible watermarking to embed an invisible watermark into the model output, and a script is provided for detecting this watermark.\n\nDetails are provided on how to train the models, including example training configurations in `configs/example_training`. There are also detailed instructions on how to build new diffusion models, including how to configure the conditioner, network, and loss.\n\nFor large scale training, the repository recommends using data pipelines from their data pipelines project. For smaller datasets, they should be defined within the repository itself.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702171702219891300
2023-12-09 20:28:54,195 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The generative-models repository by Stability AI contains a variety of generative models that use a config-driven approach to build and combine submodules. The repository has a strong emphasis on modularity and has numerous examples in the `configs/` directory. It uses the PyTorch Lightning framework for training.\n\nSeveral releases are announced in the README file, such as SDXL-Turbo, a fast text-to-image model, Stable Video Diffusion, an image-to-video model, and several versions of SDXL. Instructions for accessing these models for research are provided, including guidance on using Hugging Face to request access to certain models. \n\nThe README also contains a comprehensive guide on installing and setting up the repository. This includes cloning the repo, setting up a virtual environment, and installing required packages from PyPI. The repository supports both `pytorch1.13` and `pytorch2`.\n\nThere are also guidelines on how to use the models for inferencing, including instructions to run a Streamlit demo for text-to-image and image-to-image sampling. The repository uses invisible watermarking to embed an invisible watermark into the model output, and a script is provided for detecting this watermark.\n\nDetails are provided on how to train the models, including example training configurations in `configs/example_training`. There are also detailed instructions on how to build new diffusion models, including how to configure the conditioner, network, and loss.\n\nFor large scale training, the repository recommends using data pipelines from their data pipelines project. For smaller datasets, they should be defined within the repository itself.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702171702219891300
2023-12-09 20:28:54,195 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The generative-models repository by Stability AI contains a variety of generative models that use a config-driven approach to build and combine submodules. The repository has a strong emphasis on modularity and has numerous examples in the `configs/` directory. It uses the PyTorch Lightning framework for training.\n\nSeveral releases are announced in the README file, such as SDXL-Turbo, a fast text-to-image model, Stable Video Diffusion, an image-to-video model, and several versions of SDXL. Instructions for accessing these models for research are provided, including guidance on using Hugging Face to request access to certain models. \n\nThe README also contains a comprehensive guide on installing and setting up the repository. This includes cloning the repo, setting up a virtual environment, and installing required packages from PyPI. The repository supports both `pytorch1.13` and `pytorch2`.\n\nThere are also guidelines on how to use the models for inferencing, including instructions to run a Streamlit demo for text-to-image and image-to-image sampling. The repository uses invisible watermarking to embed an invisible watermark into the model output, and a script is provided for detecting this watermark.\n\nDetails are provided on how to train the models, including example training configurations in `configs/example_training`. There are also detailed instructions on how to build new diffusion models, including how to configure the conditioner, network, and loss.\n\nFor large scale training, the repository recommends using data pipelines from their data pipelines project. For smaller datasets, they should be defined within the repository itself.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702171702219891300
2023-12-09 20:28:54,195 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The generative-models repository by Stability AI contains a variety of generative models that use a config-driven approach to build and combine submodules. The repository has a strong emphasis on modularity and has numerous examples in the `configs/` directory. It uses the PyTorch Lightning framework for training.\n\nSeveral releases are announced in the README file, such as SDXL-Turbo, a fast text-to-image model, Stable Video Diffusion, an image-to-video model, and several versions of SDXL. Instructions for accessing these models for research are provided, including guidance on using Hugging Face to request access to certain models. \n\nThe README also contains a comprehensive guide on installing and setting up the repository. This includes cloning the repo, setting up a virtual environment, and installing required packages from PyPI. The repository supports both `pytorch1.13` and `pytorch2`.\n\nThere are also guidelines on how to use the models for inferencing, including instructions to run a Streamlit demo for text-to-image and image-to-image sampling. The repository uses invisible watermarking to embed an invisible watermark into the model output, and a script is provided for detecting this watermark.\n\nDetails are provided on how to train the models, including example training configurations in `configs/example_training`. There are also detailed instructions on how to build new diffusion models, including how to configure the conditioner, network, and loss.\n\nFor large scale training, the repository recommends using data pipelines from their data pipelines project. For smaller datasets, they should be defined within the repository itself.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702171702219891300
2023-12-09 20:28:54,195 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The generative-models repository by Stability AI contains a variety of generative models that use a config-driven approach to build and combine submodules. The repository has a strong emphasis on modularity and has numerous examples in the `configs/` directory. It uses the PyTorch Lightning framework for training.\n\nSeveral releases are announced in the README file, such as SDXL-Turbo, a fast text-to-image model, Stable Video Diffusion, an image-to-video model, and several versions of SDXL. Instructions for accessing these models for research are provided, including guidance on using Hugging Face to request access to certain models. \n\nThe README also contains a comprehensive guide on installing and setting up the repository. This includes cloning the repo, setting up a virtual environment, and installing required packages from PyPI. The repository supports both `pytorch1.13` and `pytorch2`.\n\nThere are also guidelines on how to use the models for inferencing, including instructions to run a Streamlit demo for text-to-image and image-to-image sampling. The repository uses invisible watermarking to embed an invisible watermark into the model output, and a script is provided for detecting this watermark.\n\nDetails are provided on how to train the models, including example training configurations in `configs/example_training`. There are also detailed instructions on how to build new diffusion models, including how to configure the conditioner, network, and loss.\n\nFor large scale training, the repository recommends using data pipelines from their data pipelines project. For smaller datasets, they should be defined within the repository itself.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702171702219891300
2023-12-09 20:28:54,195 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The generative-models repository by Stability AI contains a variety of generative models that use a config-driven approach to build and combine submodules. The repository has a strong emphasis on modularity and has numerous examples in the `configs/` directory. It uses the PyTorch Lightning framework for training.\n\nSeveral releases are announced in the README file, such as SDXL-Turbo, a fast text-to-image model, Stable Video Diffusion, an image-to-video model, and several versions of SDXL. Instructions for accessing these models for research are provided, including guidance on using Hugging Face to request access to certain models. \n\nThe README also contains a comprehensive guide on installing and setting up the repository. This includes cloning the repo, setting up a virtual environment, and installing required packages from PyPI. The repository supports both `pytorch1.13` and `pytorch2`.\n\nThere are also guidelines on how to use the models for inferencing, including instructions to run a Streamlit demo for text-to-image and image-to-image sampling. The repository uses invisible watermarking to embed an invisible watermark into the model output, and a script is provided for detecting this watermark.\n\nDetails are provided on how to train the models, including example training configurations in `configs/example_training`. There are also detailed instructions on how to build new diffusion models, including how to configure the conditioner, network, and loss.\n\nFor large scale training, the repository recommends using data pipelines from their data pipelines project. For smaller datasets, they should be defined within the repository itself.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702171702219891300
2023-12-09 20:28:54,195 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The generative-models repository by Stability AI contains a variety of generative models that use a config-driven approach to build and combine submodules. The repository has a strong emphasis on modularity and has numerous examples in the `configs/` directory. It uses the PyTorch Lightning framework for training.\n\nSeveral releases are announced in the README file, such as SDXL-Turbo, a fast text-to-image model, Stable Video Diffusion, an image-to-video model, and several versions of SDXL. Instructions for accessing these models for research are provided, including guidance on using Hugging Face to request access to certain models. \n\nThe README also contains a comprehensive guide on installing and setting up the repository. This includes cloning the repo, setting up a virtual environment, and installing required packages from PyPI. The repository supports both `pytorch1.13` and `pytorch2`.\n\nThere are also guidelines on how to use the models for inferencing, including instructions to run a Streamlit demo for text-to-image and image-to-image sampling. The repository uses invisible watermarking to embed an invisible watermark into the model output, and a script is provided for detecting this watermark.\n\nDetails are provided on how to train the models, including example training configurations in `configs/example_training`. There are also detailed instructions on how to build new diffusion models, including how to configure the conditioner, network, and loss.\n\nFor large scale training, the repository recommends using data pipelines from their data pipelines project. For smaller datasets, they should be defined within the repository itself.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702171702219891300
2023-12-09 20:36:32,054 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702172180153144200
2023-12-09 20:36:32,054 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702172180153144200
2023-12-09 20:36:32,054 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702172180153144200
2023-12-09 20:36:32,054 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702172180153144200
2023-12-09 20:36:32,055 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': 'Generative Models', 'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.'}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172180153144200
2023-12-09 20:36:32,055 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': 'Generative Models', 'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.'}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172180153144200
2023-12-09 20:36:32,055 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': 'Generative Models', 'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.'}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172180153144200
2023-12-09 20:36:32,055 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': 'Generative Models', 'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.'}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172180153144200
2023-12-09 20:36:32,055 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': 'Generative Models', 'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.'}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172180153144200
2023-12-09 20:36:32,055 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': 'Generative Models', 'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.'}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172180153144200
2023-12-09 20:36:32,055 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': 'Generative Models', 'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.'}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172180153144200
2023-12-09 20:36:32,055 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': 'Generative Models', 'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.'}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172180153144200
2023-12-09 20:36:32,063 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'presence_penalty': 0, 'frequency_penalty': 0, 'model': 'gpt-4', 'temperature': 0.75, 'top_p': 1, 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702172180153144200
2023-12-09 20:36:32,063 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'presence_penalty': 0, 'frequency_penalty': 0, 'model': 'gpt-4', 'temperature': 0.75, 'top_p': 1, 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702172180153144200
2023-12-09 20:36:32,063 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'presence_penalty': 0, 'frequency_penalty': 0, 'model': 'gpt-4', 'temperature': 0.75, 'top_p': 1, 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702172180153144200
2023-12-09 20:36:32,063 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'presence_penalty': 0, 'frequency_penalty': 0, 'model': 'gpt-4', 'temperature': 0.75, 'top_p': 1, 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702172180153144200
2023-12-09 20:36:45,726 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The repository, "generative-models," is owned by Stability AI and contains a series of generative models and their corresponding configurations for creating different types of data. These models are primarily used for text-to-image and image-to-video generation tasks. The repository includes details of model releases, such as SDXL-Turbo, Stable Video Diffusion, and various versions of SDXL models, along with their weights and usage instructions. \n\nThe README file provides a thorough walk-through starting from the installation, packaging, inference, and training processes. Following the instructions, one can clone the repository, set up the virtual environment, install the required packages and libraries, and use the provided scripts for various tasks. \n\nThe repository also provides information about the changes made from the old codebase, a description of the modularity of the codebase, and guidelines on how to build new diffusion models. In addition, the repository offers guidelines on how to detect invisible watermarks embedded in the generated images.\n\nThe repository also provides access to the training configurations and explains how to launch a training session. It also provides configurations for combining model, training, and data configurations. It supports both PyTorch 1.13 and PyTorch 2.0 for training generative models but only supports PyTorch 1.13 for autoencoder training.\n\nLastly, the repository gives detailed instructions on how to use their data pipelines for large scale training and how to define small map-style datasets.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172180153144200
2023-12-09 20:36:45,726 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The repository, "generative-models," is owned by Stability AI and contains a series of generative models and their corresponding configurations for creating different types of data. These models are primarily used for text-to-image and image-to-video generation tasks. The repository includes details of model releases, such as SDXL-Turbo, Stable Video Diffusion, and various versions of SDXL models, along with their weights and usage instructions. \n\nThe README file provides a thorough walk-through starting from the installation, packaging, inference, and training processes. Following the instructions, one can clone the repository, set up the virtual environment, install the required packages and libraries, and use the provided scripts for various tasks. \n\nThe repository also provides information about the changes made from the old codebase, a description of the modularity of the codebase, and guidelines on how to build new diffusion models. In addition, the repository offers guidelines on how to detect invisible watermarks embedded in the generated images.\n\nThe repository also provides access to the training configurations and explains how to launch a training session. It also provides configurations for combining model, training, and data configurations. It supports both PyTorch 1.13 and PyTorch 2.0 for training generative models but only supports PyTorch 1.13 for autoencoder training.\n\nLastly, the repository gives detailed instructions on how to use their data pipelines for large scale training and how to define small map-style datasets.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172180153144200
2023-12-09 20:36:45,726 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The repository, "generative-models," is owned by Stability AI and contains a series of generative models and their corresponding configurations for creating different types of data. These models are primarily used for text-to-image and image-to-video generation tasks. The repository includes details of model releases, such as SDXL-Turbo, Stable Video Diffusion, and various versions of SDXL models, along with their weights and usage instructions. \n\nThe README file provides a thorough walk-through starting from the installation, packaging, inference, and training processes. Following the instructions, one can clone the repository, set up the virtual environment, install the required packages and libraries, and use the provided scripts for various tasks. \n\nThe repository also provides information about the changes made from the old codebase, a description of the modularity of the codebase, and guidelines on how to build new diffusion models. In addition, the repository offers guidelines on how to detect invisible watermarks embedded in the generated images.\n\nThe repository also provides access to the training configurations and explains how to launch a training session. It also provides configurations for combining model, training, and data configurations. It supports both PyTorch 1.13 and PyTorch 2.0 for training generative models but only supports PyTorch 1.13 for autoencoder training.\n\nLastly, the repository gives detailed instructions on how to use their data pipelines for large scale training and how to define small map-style datasets.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172180153144200
2023-12-09 20:36:45,726 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The repository, "generative-models," is owned by Stability AI and contains a series of generative models and their corresponding configurations for creating different types of data. These models are primarily used for text-to-image and image-to-video generation tasks. The repository includes details of model releases, such as SDXL-Turbo, Stable Video Diffusion, and various versions of SDXL models, along with their weights and usage instructions. \n\nThe README file provides a thorough walk-through starting from the installation, packaging, inference, and training processes. Following the instructions, one can clone the repository, set up the virtual environment, install the required packages and libraries, and use the provided scripts for various tasks. \n\nThe repository also provides information about the changes made from the old codebase, a description of the modularity of the codebase, and guidelines on how to build new diffusion models. In addition, the repository offers guidelines on how to detect invisible watermarks embedded in the generated images.\n\nThe repository also provides access to the training configurations and explains how to launch a training session. It also provides configurations for combining model, training, and data configurations. It supports both PyTorch 1.13 and PyTorch 2.0 for training generative models but only supports PyTorch 1.13 for autoencoder training.\n\nLastly, the repository gives detailed instructions on how to use their data pipelines for large scale training and how to define small map-style datasets.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172180153144200
2023-12-09 20:36:45,726 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The repository, "generative-models," is owned by Stability AI and contains a series of generative models and their corresponding configurations for creating different types of data. These models are primarily used for text-to-image and image-to-video generation tasks. The repository includes details of model releases, such as SDXL-Turbo, Stable Video Diffusion, and various versions of SDXL models, along with their weights and usage instructions. \n\nThe README file provides a thorough walk-through starting from the installation, packaging, inference, and training processes. Following the instructions, one can clone the repository, set up the virtual environment, install the required packages and libraries, and use the provided scripts for various tasks. \n\nThe repository also provides information about the changes made from the old codebase, a description of the modularity of the codebase, and guidelines on how to build new diffusion models. In addition, the repository offers guidelines on how to detect invisible watermarks embedded in the generated images.\n\nThe repository also provides access to the training configurations and explains how to launch a training session. It also provides configurations for combining model, training, and data configurations. It supports both PyTorch 1.13 and PyTorch 2.0 for training generative models but only supports PyTorch 1.13 for autoencoder training.\n\nLastly, the repository gives detailed instructions on how to use their data pipelines for large scale training and how to define small map-style datasets.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172180153144200
2023-12-09 20:36:45,726 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The repository, "generative-models," is owned by Stability AI and contains a series of generative models and their corresponding configurations for creating different types of data. These models are primarily used for text-to-image and image-to-video generation tasks. The repository includes details of model releases, such as SDXL-Turbo, Stable Video Diffusion, and various versions of SDXL models, along with their weights and usage instructions. \n\nThe README file provides a thorough walk-through starting from the installation, packaging, inference, and training processes. Following the instructions, one can clone the repository, set up the virtual environment, install the required packages and libraries, and use the provided scripts for various tasks. \n\nThe repository also provides information about the changes made from the old codebase, a description of the modularity of the codebase, and guidelines on how to build new diffusion models. In addition, the repository offers guidelines on how to detect invisible watermarks embedded in the generated images.\n\nThe repository also provides access to the training configurations and explains how to launch a training session. It also provides configurations for combining model, training, and data configurations. It supports both PyTorch 1.13 and PyTorch 2.0 for training generative models but only supports PyTorch 1.13 for autoencoder training.\n\nLastly, the repository gives detailed instructions on how to use their data pipelines for large scale training and how to define small map-style datasets.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172180153144200
2023-12-09 20:36:45,726 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The repository, "generative-models," is owned by Stability AI and contains a series of generative models and their corresponding configurations for creating different types of data. These models are primarily used for text-to-image and image-to-video generation tasks. The repository includes details of model releases, such as SDXL-Turbo, Stable Video Diffusion, and various versions of SDXL models, along with their weights and usage instructions. \n\nThe README file provides a thorough walk-through starting from the installation, packaging, inference, and training processes. Following the instructions, one can clone the repository, set up the virtual environment, install the required packages and libraries, and use the provided scripts for various tasks. \n\nThe repository also provides information about the changes made from the old codebase, a description of the modularity of the codebase, and guidelines on how to build new diffusion models. In addition, the repository offers guidelines on how to detect invisible watermarks embedded in the generated images.\n\nThe repository also provides access to the training configurations and explains how to launch a training session. It also provides configurations for combining model, training, and data configurations. It supports both PyTorch 1.13 and PyTorch 2.0 for training generative models but only supports PyTorch 1.13 for autoencoder training.\n\nLastly, the repository gives detailed instructions on how to use their data pipelines for large scale training and how to define small map-style datasets.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172180153144200
2023-12-09 20:36:45,726 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The repository, "generative-models," is owned by Stability AI and contains a series of generative models and their corresponding configurations for creating different types of data. These models are primarily used for text-to-image and image-to-video generation tasks. The repository includes details of model releases, such as SDXL-Turbo, Stable Video Diffusion, and various versions of SDXL models, along with their weights and usage instructions. \n\nThe README file provides a thorough walk-through starting from the installation, packaging, inference, and training processes. Following the instructions, one can clone the repository, set up the virtual environment, install the required packages and libraries, and use the provided scripts for various tasks. \n\nThe repository also provides information about the changes made from the old codebase, a description of the modularity of the codebase, and guidelines on how to build new diffusion models. In addition, the repository offers guidelines on how to detect invisible watermarks embedded in the generated images.\n\nThe repository also provides access to the training configurations and explains how to launch a training session. It also provides configurations for combining model, training, and data configurations. It supports both PyTorch 1.13 and PyTorch 2.0 for training generative models but only supports PyTorch 1.13 for autoencoder training.\n\nLastly, the repository gives detailed instructions on how to use their data pipelines for large scale training and how to define small map-style datasets.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172180153144200
2023-12-09 20:36:45,726 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The repository, "generative-models," is owned by Stability AI and contains a series of generative models and their corresponding configurations for creating different types of data. These models are primarily used for text-to-image and image-to-video generation tasks. The repository includes details of model releases, such as SDXL-Turbo, Stable Video Diffusion, and various versions of SDXL models, along with their weights and usage instructions. \n\nThe README file provides a thorough walk-through starting from the installation, packaging, inference, and training processes. Following the instructions, one can clone the repository, set up the virtual environment, install the required packages and libraries, and use the provided scripts for various tasks. \n\nThe repository also provides information about the changes made from the old codebase, a description of the modularity of the codebase, and guidelines on how to build new diffusion models. In addition, the repository offers guidelines on how to detect invisible watermarks embedded in the generated images.\n\nThe repository also provides access to the training configurations and explains how to launch a training session. It also provides configurations for combining model, training, and data configurations. It supports both PyTorch 1.13 and PyTorch 2.0 for training generative models but only supports PyTorch 1.13 for autoencoder training.\n\nLastly, the repository gives detailed instructions on how to use their data pipelines for large scale training and how to define small map-style datasets.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172180153144200
2023-12-09 20:36:45,726 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The repository, "generative-models," is owned by Stability AI and contains a series of generative models and their corresponding configurations for creating different types of data. These models are primarily used for text-to-image and image-to-video generation tasks. The repository includes details of model releases, such as SDXL-Turbo, Stable Video Diffusion, and various versions of SDXL models, along with their weights and usage instructions. \n\nThe README file provides a thorough walk-through starting from the installation, packaging, inference, and training processes. Following the instructions, one can clone the repository, set up the virtual environment, install the required packages and libraries, and use the provided scripts for various tasks. \n\nThe repository also provides information about the changes made from the old codebase, a description of the modularity of the codebase, and guidelines on how to build new diffusion models. In addition, the repository offers guidelines on how to detect invisible watermarks embedded in the generated images.\n\nThe repository also provides access to the training configurations and explains how to launch a training session. It also provides configurations for combining model, training, and data configurations. It supports both PyTorch 1.13 and PyTorch 2.0 for training generative models but only supports PyTorch 1.13 for autoencoder training.\n\nLastly, the repository gives detailed instructions on how to use their data pipelines for large scale training and how to define small map-style datasets.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172180153144200
2023-12-09 20:36:45,726 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The repository, "generative-models," is owned by Stability AI and contains a series of generative models and their corresponding configurations for creating different types of data. These models are primarily used for text-to-image and image-to-video generation tasks. The repository includes details of model releases, such as SDXL-Turbo, Stable Video Diffusion, and various versions of SDXL models, along with their weights and usage instructions. \n\nThe README file provides a thorough walk-through starting from the installation, packaging, inference, and training processes. Following the instructions, one can clone the repository, set up the virtual environment, install the required packages and libraries, and use the provided scripts for various tasks. \n\nThe repository also provides information about the changes made from the old codebase, a description of the modularity of the codebase, and guidelines on how to build new diffusion models. In addition, the repository offers guidelines on how to detect invisible watermarks embedded in the generated images.\n\nThe repository also provides access to the training configurations and explains how to launch a training session. It also provides configurations for combining model, training, and data configurations. It supports both PyTorch 1.13 and PyTorch 2.0 for training generative models but only supports PyTorch 1.13 for autoencoder training.\n\nLastly, the repository gives detailed instructions on how to use their data pipelines for large scale training and how to define small map-style datasets.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172180153144200
2023-12-09 20:36:45,726 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The repository, "generative-models," is owned by Stability AI and contains a series of generative models and their corresponding configurations for creating different types of data. These models are primarily used for text-to-image and image-to-video generation tasks. The repository includes details of model releases, such as SDXL-Turbo, Stable Video Diffusion, and various versions of SDXL models, along with their weights and usage instructions. \n\nThe README file provides a thorough walk-through starting from the installation, packaging, inference, and training processes. Following the instructions, one can clone the repository, set up the virtual environment, install the required packages and libraries, and use the provided scripts for various tasks. \n\nThe repository also provides information about the changes made from the old codebase, a description of the modularity of the codebase, and guidelines on how to build new diffusion models. In addition, the repository offers guidelines on how to detect invisible watermarks embedded in the generated images.\n\nThe repository also provides access to the training configurations and explains how to launch a training session. It also provides configurations for combining model, training, and data configurations. It supports both PyTorch 1.13 and PyTorch 2.0 for training generative models but only supports PyTorch 1.13 for autoencoder training.\n\nLastly, the repository gives detailed instructions on how to use their data pipelines for large scale training and how to define small map-style datasets.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172180153144200
2023-12-09 20:36:45,726 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The repository, "generative-models," is owned by Stability AI and contains a series of generative models and their corresponding configurations for creating different types of data. These models are primarily used for text-to-image and image-to-video generation tasks. The repository includes details of model releases, such as SDXL-Turbo, Stable Video Diffusion, and various versions of SDXL models, along with their weights and usage instructions. \n\nThe README file provides a thorough walk-through starting from the installation, packaging, inference, and training processes. Following the instructions, one can clone the repository, set up the virtual environment, install the required packages and libraries, and use the provided scripts for various tasks. \n\nThe repository also provides information about the changes made from the old codebase, a description of the modularity of the codebase, and guidelines on how to build new diffusion models. In addition, the repository offers guidelines on how to detect invisible watermarks embedded in the generated images.\n\nThe repository also provides access to the training configurations and explains how to launch a training session. It also provides configurations for combining model, training, and data configurations. It supports both PyTorch 1.13 and PyTorch 2.0 for training generative models but only supports PyTorch 1.13 for autoencoder training.\n\nLastly, the repository gives detailed instructions on how to use their data pipelines for large scale training and how to define small map-style datasets.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172180153144200
2023-12-09 20:36:45,726 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The repository, "generative-models," is owned by Stability AI and contains a series of generative models and their corresponding configurations for creating different types of data. These models are primarily used for text-to-image and image-to-video generation tasks. The repository includes details of model releases, such as SDXL-Turbo, Stable Video Diffusion, and various versions of SDXL models, along with their weights and usage instructions. \n\nThe README file provides a thorough walk-through starting from the installation, packaging, inference, and training processes. Following the instructions, one can clone the repository, set up the virtual environment, install the required packages and libraries, and use the provided scripts for various tasks. \n\nThe repository also provides information about the changes made from the old codebase, a description of the modularity of the codebase, and guidelines on how to build new diffusion models. In addition, the repository offers guidelines on how to detect invisible watermarks embedded in the generated images.\n\nThe repository also provides access to the training configurations and explains how to launch a training session. It also provides configurations for combining model, training, and data configurations. It supports both PyTorch 1.13 and PyTorch 2.0 for training generative models but only supports PyTorch 1.13 for autoencoder training.\n\nLastly, the repository gives detailed instructions on how to use their data pipelines for large scale training and how to define small map-style datasets.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172180153144200
2023-12-09 21:21:26,742 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '<div align="center"><picture>\n  <img alt="aiconfig" src="aiconfig-docs/static/img/readme_logo.png" />\n</picture></div>\n<br/>\n\n![Python](https://github.com/lastmile-ai/aiconfig/actions/workflows/main_python.yml/badge.svg)\n![Node](https://github.com/lastmile-ai/aiconfig/actions/workflows/main-typescript.yml/badge.svg)\n![Docs](https://github.com/lastmile-ai/aiconfig/actions/workflows/test-deploy-docs.yml/badge.svg)\n[![Discord](<https://img.shields.io/badge/Discord-LastMile%20AI-Blue?color=rgb(37%2C%20150%2C%20190)>)](https://discord.gg/qMqgzDae)\n\n> Full documentation: **[aiconfig.lastmileai.dev](https://aiconfig.lastmileai.dev/)**\n\n## Overview\n\nAIConfig saves prompts, models and model parameters as source control friendly configs. This allows you to iterate on prompts and model parameters _separately from your application code_.\n\n1. **Prompts as configs**: a [standardized JSON format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to store generative AI model settings, prompt inputs/outputs, and flexible metadata.\n2. **Model-agnostic SDK**: Python & Node SDKs to use `aiconfig` in your application code. AIConfig is designed to be **model-agnostic** and **multi-modal**, so you can extend it to work with any generative AI model, including text, image and audio.\n3. **AI Workbook editor**: A [notebook-like playground](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to edit `aiconfig` files visually, run prompts, tweak models and model settings, and chain things together.\n\n### What problem it solves\n\nToday, application code is tightly coupled with the gen AI settings for the application -- prompts, parameters, and model-specific logic is all jumbled in with app code.\n\n- results in increased complexity\n- makes it hard to iterate on the prompts or try different models easily\n- makes it hard to evaluate prompt/model performance\n\nAIConfig helps unwind complexity by separating prompts, model parameters, and model-specific logic from your application.\n\n- simplifies application code -- simply call `config.run()`\n- open the `aiconfig` in a playground to iterate quickly\n- version control and evaluate the `aiconfig` - it\'s the AI artifact for your application.\n\n![AIConfig flow](aiconfig-docs/static/img/aiconfig_dataflow.png)\n\n### Quicknav\n\n<ul style="margin-bottom:0; padding-bottom:0;">\n  <li><a href="#install">Getting Started</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig">Create an AIConfig</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig">Run a prompt</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/parameters">Pass data into prompts</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain">Prompt Chains</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig">Callbacks and monitoring</a></li>\n  </ul>\n  <li><a href="#aiconfig-sdk">SDK Cheatsheet</a></li>\n  <li><a href="#cookbooks">Cookbooks and guides</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT">CLI Chatbot</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig">RAG with AIConfig</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing">Prompt routing</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI">OpenAI function calling</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification">Chain of Verification</a></li>\n  </ul>\n  <li><a href="#supported-models">Supported models</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama">LLaMA2 example</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace">Hugging Face (Mistral-7B) example</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency">PaLM</a></li>\n  </ul>\n  <li><a href="#extensibility">Extensibility</a></li>\n  <li><a href="#contributing-to-aiconfig">Contributing</a></li>\n  <li><a href="#roadmap">Roadmap</a></li>\n  <li><a href="#faqs">FAQ</a></li>\n</ul>\n\n## Features\n\n- [x] **Source-control friendly** [`aiconfig` format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to save prompts and model settings, which you can use for evaluation, reproducibility and simplifying your application code.\n- [x] **Multi-modal and model agnostic**. Use with any model, and serialize/deserialize data with the same `aiconfig` format.\n- [x] **Prompt chaining and parameterization** with [{{handlebars}}](https://handlebarsjs.com/) templating syntax, allowing you to pass dynamic data into prompts (as well as between prompts).\n- [x] **Streaming** supported out of the box, allowing you to get playground-like streaming wherever you use `aiconfig`.\n- [x] **Notebook editor**. [AI Workbooks editor](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to visually create your `aiconfig`, and use the SDK to connect it to your application code.\n\n## Install\n\nInstall with your favorite package manager for Node or Python.\n\n### Node.js\n\n#### `npm` or `yarn`\n\n```bash\nnpm install aiconfig\n```\n\n```bash\nyarn add aiconfig\n```\n\n### Python\n\n#### `pip3` or `poetry`\n\n```bash\npip3 install python-aiconfig\n```\n\n```bash\npoetry add python-aiconfig\n```\n\n[Detailed installation instructions](https://aiconfig.lastmileai.dev/docs/getting-started/#installation).\n\n### Set your OpenAI API Key\n\n> **Note**: Make sure to specify the API keys (such as [`OPENAI_API_KEY`](https://platform.openai.com/api-keys)) in your environment before proceeding.\n\nIn your CLI, set the environment variable:\n\n```bash\nexport OPENAI_API_KEY=my_key\n```\n\n## Getting Started\n\n> We cover Python instructions here, for Node.js please see the [detailed Getting Started guide](https://aiconfig.lastmileai.dev/docs/getting-started)\n\nIn this quickstart, you will create a customizable NYC travel itinerary using `aiconfig`.\n\nThis AIConfig contains a prompt chain to get a list of travel activities from an LLM and then generate an itinerary in an order specified by the user.\n\n> **Link to tutorial code: [here](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Getting-Started)**\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/805173d1-0f83-44c5-b570-c776bb7dba66\n\n### Download `travel.aiconfig.json`\n\n> **Note**: Don\'t worry if you don\'t understand all the pieces of this yet, we\'ll go over it step by step.\n\n```json\n{\n  "name": "NYC Trip Planner",\n  "description": "Intrepid explorer with ChatGPT and AIConfig",\n  "schema_version": "latest",\n  "metadata": {\n    "models": {\n      "gpt-3.5-turbo": {\n        "model": "gpt-3.5-turbo",\n        "top_p": 1,\n        "temperature": 1\n      },\n      "gpt-4": {\n        "model": "gpt-4",\n        "max_tokens": 3000,\n        "system_prompt": "You are an expert travel coordinator with exquisite taste."\n      }\n    },\n    "default_model": "gpt-3.5-turbo"\n  },\n  "prompts": [\n    {\n      "name": "get_activities",\n      "input": "Tell me 10 fun attractions to do in NYC."\n    },\n    {\n      "name": "gen_itinerary",\n      "input": "Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}.",\n      "metadata": {\n        "model": "gpt-4",\n        "parameters": {\n          "order_by": "geographic location"\n        }\n      }\n    }\n  ]\n}\n```\n\n### Run the `get_activities` prompt.\n\nYou don\'t need to worry about how to run inference for the model; it\'s all handled by AIConfig. The prompt runs with gpt-3.5-turbo since that is the `default_model` for this AIConfig.\n\nCreate a new file called `app.py` and and enter the following code:\n\n```python\nimport asyncio\nfrom aiconfig import AIConfigRuntime, InferenceOptions\n\nasync def main():\n  # Load the aiconfig\n  config = AIConfigRuntime.load(\'travel.aiconfig.json\')\n\n  # Run a single prompt (with streaming)\n  inference_options = InferenceOptions(stream=True)\n  await config.run("get_activities", options=inference_options)\n\nasyncio.run(main())\n```\n\nNow run this in your terminal with the command:\n\n```bash\npython3 app.py\n```\n\n### Run the `gen_itinerary` prompt.\n\nIn your `app.py` file, change the last line to below:\n\n```python\nawait config.run("gen_itinerary", params=None, options=inference_options)\n```\n\nRe-run the command in your terminal:\n\n```bash\npython3 app.py\n```\n\nThis prompt depends on the output of `get_activities`. It also takes in parameters (user input) to determine the customized itinerary.\n\nLet\'s take a closer look:\n\n**`gen_itinerary` prompt:**\n\n```\n"Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}."\n```\n\n**prompt metadata:**\n\n```json\n{\n  "metadata": {\n    "model": "gpt-4",\n    "parameters": {\n      "order_by": "geographic location"\n    }\n  }\n}\n```\n\nObserve the following:\n\n1. The prompt depends on the output of the `get_activities` prompt.\n2. It also depends on an `order_by` parameter (using {{handlebars}} syntax)\n3. It uses **gpt-4**, whereas the `get_activities` prompt it depends on uses **gpt-3.5-turbo**.\n\n> Effectively, this is a prompt chain between `gen_itinerary` and `get_activities` prompts, _as well as_ as a model chain between **gpt-3.5-turbo** and **gpt-4**.\n\nLet\'s run this with AIConfig:\n\nReplace `config.run` above with this:\n\n```python\nawait config.run("gen_itinerary", params={"order_by": "duration"}, options=inference_options, run_with_dependencies=True)\n```\n\nNotice how simple the syntax is to perform a fairly complex task - running 2 different prompts across 2 different models and chaining one\'s output as part of the input of another.\n\nThe code will just run `get_activities`, then pipe its output as an input to `gen_itinerary`, and finally run `gen_itinerary`.\n\n### Save the AIConfig\n\nLet\'s save the AIConfig back to disk, and serialize the outputs from the latest inference run as well:\n\n```python\n# Save the aiconfig to disk. and serialize outputs from the model run\nconfig.save(\'updated.aiconfig.json\', include_outputs=True)\n```\n\n### Edit `aiconfig` in a notebook editor\n\nWe can iterate on an `aiconfig` using a notebook-like editor called an **AI Workbook**. Now that we have an `aiconfig` file artifact that encapsulates the generative AI part of our application, we can iterate on it separately from the application code that uses it.\n\n1. Go to https://lastmileai.dev.\n2. Go to Workbooks page: https://lastmileai.dev/workbooks\n3. Click dropdown from \'+ New Workbook\' and select \'Create from AIConfig\'\n4. Upload `travel.aiconfig.json`\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/5d901493-bbda-4f8e-93c7-dd9a91bf242e\n\nTry out the workbook playground here: **[NYC Travel Workbook](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9)**\n\n> **We are working on a local editor that you can run yourself. For now, please use the hosted version on https://lastmileai.dev.**\n\n### Additional Guides\n\nThere is a lot you can do with `aiconfig`. We have several other tutorials to help get you started:\n\n- [Create an AIConfig from scratch](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig)\n- [Run a prompt](https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig)\n- [Pass data into prompts](https://aiconfig.lastmileai.dev/docs/overview/parameters)\n- [Prompt chains](https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain)\n- [Callbacks and monitoring](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\nHere are some example uses:\n\n- [CLI Chatbot](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT)\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [Chain of thought](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### OpenAI Introspection API\n\nIf you are already using OpenAI completion API\'s in your application, you can get started very quickly to start saving the messages in an `aiconfig`.\n\nUsage: see openai_wrapper.ipynb.\n\nNow you can continue using `openai` completion API as normal. When you want to save the config, just call `new_config.save()` and all your openai completion calls will get serialized to disk.\n\n> [**Detailed guide here**](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig#openai-api-python-wrapper)\n\n## Supported Models\n\nAIConfig supports the following models out of the box:\n\n- OpenAI chat models (GPT-3, GPT-3.5, GPT-4)\n- LLaMA2 (running locally)\n- Google PaLM models (PaLM chat)\n- Hugging Face text generation models (e.g. Mistral-7B)\n\n### Examples\n\n- [OpenAI](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n> If you need to use a model that isn\'t provided out of the box, you can implement a `ModelParser` for it (see [Extending AIConfig](#extending-aiconfig)). **We welcome [contributions](https://aiconfig.lastmileai.dev/docs/contributing)**\n\n## AIConfig Schema\n\n[AIConfig specification](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format)\n\n## AIConfig SDK\n\n> Read the [Usage Guide](https://aiconfig.lastmileai.dev/docs/usage-guide) for more details.\n\nThe AIConfig SDK supports CRUD operations for prompts, models, parameters and metadata. Here are some common examples.\n\nThe root interface is the `AIConfigRuntime` object. That is the entrypoint for interacting with an AIConfig programmatically.\n\nLet\'s go over a few key CRUD operations to give a glimpse.\n\n### AIConfig `create`\n\n```python\nconfig = AIConfigRuntime.create("aiconfig name", "description")\n```\n\n### Prompt `resolve`\n\n`resolve` deserializes an existing `Prompt` into the data object that its model expects.\n\n```python\nconfig.resolve("prompt_name", params)\n```\n\n`params` are overrides you can specify to resolve any `{{handlebars}}` templates in the prompt. See the `gen_itinerary` prompt in the Getting Started example.\n\n### Prompt `serialize`\n\n`serialize` is the inverse of `resolve` -- it serializes the data object that a model understands into a `Prompt` object that can be serialized into the `aiconfig` format.\n\n```python\nconfig.serialize("model_name", data, "prompt_name")\n```\n\n### Prompt `run`\n\n`run` is used to run inference for the specified `Prompt`.\n\n```python\nconfig.run("prompt_name", params)\n```\n\n### `run_with_dependencies`\n\nThis is a variant of `run` -- this re-runs all prompt dependencies.\nFor example, in [`travel.aiconfig.json`](#download-travelaiconfigjson), the `gen_itinerary` prompt references the output of the `get_activities` prompt using `{{get_activities.output}}`.\n\nRunning this function will first execute `get_activities`, and use its output to resolve the `gen_itinerary` prompt before executing it.\nThis is transitive, so it computes the Directed Acyclic Graph of dependencies to execute. Complex relationships can be modeled this way.\n\n```python\nconfig.run_with_dependencies("gen_itinerary")\n```\n\n### Updating metadata and parameters\n\nUse the `get/set_metadata` and `get/set_parameter` methods to interact with metadata and parameters (`set_parameter` is just syntactic sugar to update `"metadata.parameters"`)\n\n```python\nconfig.set_metadata("key", data, "prompt_name")\n```\n\nNote: if `"prompt_name"` is specified, the metadata is updated specifically for that prompt. Otherwise, the global metadata is updated.\n\n### Register new `ModelParser`\n\nUse the `AIConfigRuntime.register_model_parser` if you want to use a different `ModelParser`, or configure AIConfig to work with an additional model.\n\nAIConfig uses the model name string to retrieve the right `ModelParser` for a given Prompt (see `AIConfigRuntime.get_model_parser`), so you can register a different ModelParser for the same ID to override which `ModelParser` handles a Prompt.\n\nFor example, suppose I want to use `MyOpenAIModelParser` to handle `gpt-4` prompts. I can do the following at the start of my application:\n\n```python\nAIConfigRuntime.register_model_parser(myModelParserInstance, ["gpt-4"])\n```\n\n### Callback events\n\nUse callback events to trace and monitor what\'s going on -- helpful for debugging and observability.\n\n```python\nfrom aiconfig import AIConfigRuntime, CallbackEvent, CallbackManager\nconfig = AIConfigRuntime.load(\'aiconfig.json\')\n\nasync def my_custom_callback(event: CallbackEvent) -> None:\n  print(f"Event triggered: {event.name}", event)\n\ncallback_manager = CallbackManager([my_custom_callback])\nconfig.set_callback_manager(callback_manager)\n\nawait config.run("prompt_name")\n```\n\n[**Read more** here](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\n## Extensibility\n\nAIConfig is designed to be customized and extended for your use-case. The [Extensibility](/docs/extensibility) guide goes into more detail.\n\nCurrently, there are 3 core ways to extend AIConfig:\n\n1. [Supporting other models](https://aiconfig.lastmileai.dev/docs/extensibility#1-bring-your-own-model) - define a ModelParser extension\n2. [Callback event handlers](https://aiconfig.lastmileai.dev/docs/extensibility#2-callback-handlers) - tracing and monitoring\n3. [Custom metadata](https://aiconfig.lastmileai.dev/docs/extensibility#3-custom-metadata) - save custom fields in `aiconfig`\n\n## Contributing to `aiconfig`\n\nThis is our first open-source project and we\'d love your help.\n\nSee our [contributing guidelines](https://aiconfig.lastmileai.dev/docs/contributing) -- we would especially love help adding support for additional models that the community wants.\n\n## Cookbooks\n\nWe provide several guides to demonstrate the power of `aiconfig`.\n\n> **See the [`cookbooks`](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks) folder for examples to clone.**\n\n### Chatbot\n\n- [Wizard GPT](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT) - speak to a wizard on your CLI\n\n- [CLI-mate](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Cli-Mate) - help you make code-mods interactively on your codebase.\n\n### Retrieval Augmented Generated (RAG)\n\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n\nAt its core, RAG is about passing data into prompts. Read how to [pass data](/docs/overview/parameters) with AIConfig.\n\n### Function calling\n\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n\n### Prompt routing\n\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n\n### Chain of Thought\n\nA variant of chain-of-thought is Chain of Verification, used to help reduce hallucinations. Check out the aiconfig cookbook for CoVe:\n\n- [Chain of Verification](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### Using local LLaMA2 with `aiconfig`\n\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n\n### Hugging Face text generation\n\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n\n### Google PaLM\n\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n## Roadmap\n\nThis project is under active development.\n\nIf you\'d like to help, please see the [contributing guidelines](#contributing-to-aiconfig).\n\nPlease create issues for additional capabilities you\'d like to see.\n\nHere\'s what\'s already on our roadmap:\n\n- Evaluation interfaces: allow `aiconfig` artifacts to be evaluated with user-defined eval functions.\n  - We are also considering integrating with existing evaluation frameworks.\n- Local editor for `aiconfig`: enable you to interact with aiconfigs more intuitively.\n- OpenAI Assistants API support\n- Multi-modal ModelParsers:\n  - GPT4-V support\n  - DALLE-3\n  - Whisper\n  - HuggingFace image generation\n\n## FAQs\n\n### How should I edit an `aiconfig` file?\n\nEditing a configshould be done either programmatically via SDK or via the UI (workbooks):\n\n- [Programmatic](https://github.com/lastmile-ai/aiconfig/blob/main/cookbooks/Create-AIConfig-Programmatically/create_aiconfig_programmatically.ipynb) editing.\n\n- [Edit with a workbook](#edit-aiconfig-in-a-notebook-editor) editor: this is similar to editing an ipynb file as a notebook (most people never touch the json ipynb directly)\n\nYou should only edit the `aiconfig` by hand for minor modifications, like tweaking a prompt string or updating some metadata.\n\n### Does this support custom endpoints?\n\nOut of the box, AIConfig already supports all OpenAI GPT\\* models, Googles PaLM model and any textgeneration model on Hugging Face (like Mistral). See [Supported Models](#supported-models) for more details.\n\nAdditionally, you can install `aiconfig` [extensions](https://github.com/lastmile-ai/aiconfig/tree/main/extensions) for additional models (see question below).\n\n### Is OpenAI function calling supported?\n\nYes. [This example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI) goes through how to do it.\n\nWe are also working on adding support for the Assistants API.\n\n### How can I use aiconfig with my own model endpoint?\n\nModel support is implemented as ModelParsers in the AIConfig SDK, and the idea is that anyone, including you, can define a ModelParser (and even publish it as an extension package).\n\nAll thats needed to use a model with AIConfig is a ModelParser that knows\n\n- how to serialize data from a model into the aiconfig format\n- how to deserialize data from an aiconfig into the type the model expects\n- how to run inference for model.\n\nFor more details, see [Extensibility](https://aiconfig.lastmileai.dev/docs/extensibility).\n\n### When should I store outputs in an `aiconfig`?\n\nThe `AIConfigRuntime` object is used to interact with an aiconfig programmatically (see [SDK usage guide](#aiconfig-sdk)). As you run prompts, this object keeps track of the outputs returned from the model.\n\nYou can choose to serialize these outputs back into the `aiconfig` by using the `config.save(include_outputs=True)` API. This can be useful for preserving context -- think of it like session state.\n\nFor example, you can use aiconfig to create a chatbot, and use the same format to save the chat history so it can be resumed for the next session.\n\nYou can also choose to save outputs to a _different_ file than the original config -- `config.save("history.aiconfig.json", include_outputs=True)`.\n\n### Why should I use `aiconfig` instead of things like [configurator](https://pypi.org/project/configurator/)?\n\nIt helps to have a [standardized format](http://aiconfig.lastmileai.dev/docs/overview/ai-config-format) specifically for storing generative AI prompts, inference results, model parameters and arbitrary metadata, as opposed to a general-purpose configuration schema.\n\nWith that standardization, you just need a layer that knows how to serialize/deserialize from that format into whatever the inference endpoints require.\n\n### This looks similar to `ipynb` for Jupyter notebooks\n\nWe believe that notebooks are a perfect iteration environment for generative AI -- they are flexible, multi-modal, and collaborative.\n\nThe multi-modality and flexibility offered by notebooks and [`ipynb`](https://ipython.org/ipython-doc/3/notebook/nbformat.html) offers a good interaction model for generative AI. The `aiconfig` file format is extensible like `ipynb`, and AI Workbook editor allows rapid iteration in a notebook-like IDE.\n\n_AI Workbooks are to AIConfig what Jupyter notebooks are to `ipynb`_\n\nThere are 2 areas where we are going beyond what notebooks offer:\n\n1. `aiconfig` is more **source-control friendly** than `ipynb`. `ipynb` stores binary data (images, etc.) by encoding it in the file, while `aiconfig` recommends using file URI references instead.\n2. `aiconfig` can be imported and **connected to application code** using the AIConfig SDK.\n', 'repo_name': 'aiconfig'}, 'options': None, 'kwargs': {}} ts_ns=1702174862235185200
2023-12-09 21:21:26,742 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '<div align="center"><picture>\n  <img alt="aiconfig" src="aiconfig-docs/static/img/readme_logo.png" />\n</picture></div>\n<br/>\n\n![Python](https://github.com/lastmile-ai/aiconfig/actions/workflows/main_python.yml/badge.svg)\n![Node](https://github.com/lastmile-ai/aiconfig/actions/workflows/main-typescript.yml/badge.svg)\n![Docs](https://github.com/lastmile-ai/aiconfig/actions/workflows/test-deploy-docs.yml/badge.svg)\n[![Discord](<https://img.shields.io/badge/Discord-LastMile%20AI-Blue?color=rgb(37%2C%20150%2C%20190)>)](https://discord.gg/qMqgzDae)\n\n> Full documentation: **[aiconfig.lastmileai.dev](https://aiconfig.lastmileai.dev/)**\n\n## Overview\n\nAIConfig saves prompts, models and model parameters as source control friendly configs. This allows you to iterate on prompts and model parameters _separately from your application code_.\n\n1. **Prompts as configs**: a [standardized JSON format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to store generative AI model settings, prompt inputs/outputs, and flexible metadata.\n2. **Model-agnostic SDK**: Python & Node SDKs to use `aiconfig` in your application code. AIConfig is designed to be **model-agnostic** and **multi-modal**, so you can extend it to work with any generative AI model, including text, image and audio.\n3. **AI Workbook editor**: A [notebook-like playground](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to edit `aiconfig` files visually, run prompts, tweak models and model settings, and chain things together.\n\n### What problem it solves\n\nToday, application code is tightly coupled with the gen AI settings for the application -- prompts, parameters, and model-specific logic is all jumbled in with app code.\n\n- results in increased complexity\n- makes it hard to iterate on the prompts or try different models easily\n- makes it hard to evaluate prompt/model performance\n\nAIConfig helps unwind complexity by separating prompts, model parameters, and model-specific logic from your application.\n\n- simplifies application code -- simply call `config.run()`\n- open the `aiconfig` in a playground to iterate quickly\n- version control and evaluate the `aiconfig` - it\'s the AI artifact for your application.\n\n![AIConfig flow](aiconfig-docs/static/img/aiconfig_dataflow.png)\n\n### Quicknav\n\n<ul style="margin-bottom:0; padding-bottom:0;">\n  <li><a href="#install">Getting Started</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig">Create an AIConfig</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig">Run a prompt</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/parameters">Pass data into prompts</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain">Prompt Chains</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig">Callbacks and monitoring</a></li>\n  </ul>\n  <li><a href="#aiconfig-sdk">SDK Cheatsheet</a></li>\n  <li><a href="#cookbooks">Cookbooks and guides</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT">CLI Chatbot</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig">RAG with AIConfig</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing">Prompt routing</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI">OpenAI function calling</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification">Chain of Verification</a></li>\n  </ul>\n  <li><a href="#supported-models">Supported models</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama">LLaMA2 example</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace">Hugging Face (Mistral-7B) example</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency">PaLM</a></li>\n  </ul>\n  <li><a href="#extensibility">Extensibility</a></li>\n  <li><a href="#contributing-to-aiconfig">Contributing</a></li>\n  <li><a href="#roadmap">Roadmap</a></li>\n  <li><a href="#faqs">FAQ</a></li>\n</ul>\n\n## Features\n\n- [x] **Source-control friendly** [`aiconfig` format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to save prompts and model settings, which you can use for evaluation, reproducibility and simplifying your application code.\n- [x] **Multi-modal and model agnostic**. Use with any model, and serialize/deserialize data with the same `aiconfig` format.\n- [x] **Prompt chaining and parameterization** with [{{handlebars}}](https://handlebarsjs.com/) templating syntax, allowing you to pass dynamic data into prompts (as well as between prompts).\n- [x] **Streaming** supported out of the box, allowing you to get playground-like streaming wherever you use `aiconfig`.\n- [x] **Notebook editor**. [AI Workbooks editor](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to visually create your `aiconfig`, and use the SDK to connect it to your application code.\n\n## Install\n\nInstall with your favorite package manager for Node or Python.\n\n### Node.js\n\n#### `npm` or `yarn`\n\n```bash\nnpm install aiconfig\n```\n\n```bash\nyarn add aiconfig\n```\n\n### Python\n\n#### `pip3` or `poetry`\n\n```bash\npip3 install python-aiconfig\n```\n\n```bash\npoetry add python-aiconfig\n```\n\n[Detailed installation instructions](https://aiconfig.lastmileai.dev/docs/getting-started/#installation).\n\n### Set your OpenAI API Key\n\n> **Note**: Make sure to specify the API keys (such as [`OPENAI_API_KEY`](https://platform.openai.com/api-keys)) in your environment before proceeding.\n\nIn your CLI, set the environment variable:\n\n```bash\nexport OPENAI_API_KEY=my_key\n```\n\n## Getting Started\n\n> We cover Python instructions here, for Node.js please see the [detailed Getting Started guide](https://aiconfig.lastmileai.dev/docs/getting-started)\n\nIn this quickstart, you will create a customizable NYC travel itinerary using `aiconfig`.\n\nThis AIConfig contains a prompt chain to get a list of travel activities from an LLM and then generate an itinerary in an order specified by the user.\n\n> **Link to tutorial code: [here](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Getting-Started)**\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/805173d1-0f83-44c5-b570-c776bb7dba66\n\n### Download `travel.aiconfig.json`\n\n> **Note**: Don\'t worry if you don\'t understand all the pieces of this yet, we\'ll go over it step by step.\n\n```json\n{\n  "name": "NYC Trip Planner",\n  "description": "Intrepid explorer with ChatGPT and AIConfig",\n  "schema_version": "latest",\n  "metadata": {\n    "models": {\n      "gpt-3.5-turbo": {\n        "model": "gpt-3.5-turbo",\n        "top_p": 1,\n        "temperature": 1\n      },\n      "gpt-4": {\n        "model": "gpt-4",\n        "max_tokens": 3000,\n        "system_prompt": "You are an expert travel coordinator with exquisite taste."\n      }\n    },\n    "default_model": "gpt-3.5-turbo"\n  },\n  "prompts": [\n    {\n      "name": "get_activities",\n      "input": "Tell me 10 fun attractions to do in NYC."\n    },\n    {\n      "name": "gen_itinerary",\n      "input": "Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}.",\n      "metadata": {\n        "model": "gpt-4",\n        "parameters": {\n          "order_by": "geographic location"\n        }\n      }\n    }\n  ]\n}\n```\n\n### Run the `get_activities` prompt.\n\nYou don\'t need to worry about how to run inference for the model; it\'s all handled by AIConfig. The prompt runs with gpt-3.5-turbo since that is the `default_model` for this AIConfig.\n\nCreate a new file called `app.py` and and enter the following code:\n\n```python\nimport asyncio\nfrom aiconfig import AIConfigRuntime, InferenceOptions\n\nasync def main():\n  # Load the aiconfig\n  config = AIConfigRuntime.load(\'travel.aiconfig.json\')\n\n  # Run a single prompt (with streaming)\n  inference_options = InferenceOptions(stream=True)\n  await config.run("get_activities", options=inference_options)\n\nasyncio.run(main())\n```\n\nNow run this in your terminal with the command:\n\n```bash\npython3 app.py\n```\n\n### Run the `gen_itinerary` prompt.\n\nIn your `app.py` file, change the last line to below:\n\n```python\nawait config.run("gen_itinerary", params=None, options=inference_options)\n```\n\nRe-run the command in your terminal:\n\n```bash\npython3 app.py\n```\n\nThis prompt depends on the output of `get_activities`. It also takes in parameters (user input) to determine the customized itinerary.\n\nLet\'s take a closer look:\n\n**`gen_itinerary` prompt:**\n\n```\n"Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}."\n```\n\n**prompt metadata:**\n\n```json\n{\n  "metadata": {\n    "model": "gpt-4",\n    "parameters": {\n      "order_by": "geographic location"\n    }\n  }\n}\n```\n\nObserve the following:\n\n1. The prompt depends on the output of the `get_activities` prompt.\n2. It also depends on an `order_by` parameter (using {{handlebars}} syntax)\n3. It uses **gpt-4**, whereas the `get_activities` prompt it depends on uses **gpt-3.5-turbo**.\n\n> Effectively, this is a prompt chain between `gen_itinerary` and `get_activities` prompts, _as well as_ as a model chain between **gpt-3.5-turbo** and **gpt-4**.\n\nLet\'s run this with AIConfig:\n\nReplace `config.run` above with this:\n\n```python\nawait config.run("gen_itinerary", params={"order_by": "duration"}, options=inference_options, run_with_dependencies=True)\n```\n\nNotice how simple the syntax is to perform a fairly complex task - running 2 different prompts across 2 different models and chaining one\'s output as part of the input of another.\n\nThe code will just run `get_activities`, then pipe its output as an input to `gen_itinerary`, and finally run `gen_itinerary`.\n\n### Save the AIConfig\n\nLet\'s save the AIConfig back to disk, and serialize the outputs from the latest inference run as well:\n\n```python\n# Save the aiconfig to disk. and serialize outputs from the model run\nconfig.save(\'updated.aiconfig.json\', include_outputs=True)\n```\n\n### Edit `aiconfig` in a notebook editor\n\nWe can iterate on an `aiconfig` using a notebook-like editor called an **AI Workbook**. Now that we have an `aiconfig` file artifact that encapsulates the generative AI part of our application, we can iterate on it separately from the application code that uses it.\n\n1. Go to https://lastmileai.dev.\n2. Go to Workbooks page: https://lastmileai.dev/workbooks\n3. Click dropdown from \'+ New Workbook\' and select \'Create from AIConfig\'\n4. Upload `travel.aiconfig.json`\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/5d901493-bbda-4f8e-93c7-dd9a91bf242e\n\nTry out the workbook playground here: **[NYC Travel Workbook](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9)**\n\n> **We are working on a local editor that you can run yourself. For now, please use the hosted version on https://lastmileai.dev.**\n\n### Additional Guides\n\nThere is a lot you can do with `aiconfig`. We have several other tutorials to help get you started:\n\n- [Create an AIConfig from scratch](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig)\n- [Run a prompt](https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig)\n- [Pass data into prompts](https://aiconfig.lastmileai.dev/docs/overview/parameters)\n- [Prompt chains](https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain)\n- [Callbacks and monitoring](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\nHere are some example uses:\n\n- [CLI Chatbot](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT)\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [Chain of thought](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### OpenAI Introspection API\n\nIf you are already using OpenAI completion API\'s in your application, you can get started very quickly to start saving the messages in an `aiconfig`.\n\nUsage: see openai_wrapper.ipynb.\n\nNow you can continue using `openai` completion API as normal. When you want to save the config, just call `new_config.save()` and all your openai completion calls will get serialized to disk.\n\n> [**Detailed guide here**](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig#openai-api-python-wrapper)\n\n## Supported Models\n\nAIConfig supports the following models out of the box:\n\n- OpenAI chat models (GPT-3, GPT-3.5, GPT-4)\n- LLaMA2 (running locally)\n- Google PaLM models (PaLM chat)\n- Hugging Face text generation models (e.g. Mistral-7B)\n\n### Examples\n\n- [OpenAI](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n> If you need to use a model that isn\'t provided out of the box, you can implement a `ModelParser` for it (see [Extending AIConfig](#extending-aiconfig)). **We welcome [contributions](https://aiconfig.lastmileai.dev/docs/contributing)**\n\n## AIConfig Schema\n\n[AIConfig specification](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format)\n\n## AIConfig SDK\n\n> Read the [Usage Guide](https://aiconfig.lastmileai.dev/docs/usage-guide) for more details.\n\nThe AIConfig SDK supports CRUD operations for prompts, models, parameters and metadata. Here are some common examples.\n\nThe root interface is the `AIConfigRuntime` object. That is the entrypoint for interacting with an AIConfig programmatically.\n\nLet\'s go over a few key CRUD operations to give a glimpse.\n\n### AIConfig `create`\n\n```python\nconfig = AIConfigRuntime.create("aiconfig name", "description")\n```\n\n### Prompt `resolve`\n\n`resolve` deserializes an existing `Prompt` into the data object that its model expects.\n\n```python\nconfig.resolve("prompt_name", params)\n```\n\n`params` are overrides you can specify to resolve any `{{handlebars}}` templates in the prompt. See the `gen_itinerary` prompt in the Getting Started example.\n\n### Prompt `serialize`\n\n`serialize` is the inverse of `resolve` -- it serializes the data object that a model understands into a `Prompt` object that can be serialized into the `aiconfig` format.\n\n```python\nconfig.serialize("model_name", data, "prompt_name")\n```\n\n### Prompt `run`\n\n`run` is used to run inference for the specified `Prompt`.\n\n```python\nconfig.run("prompt_name", params)\n```\n\n### `run_with_dependencies`\n\nThis is a variant of `run` -- this re-runs all prompt dependencies.\nFor example, in [`travel.aiconfig.json`](#download-travelaiconfigjson), the `gen_itinerary` prompt references the output of the `get_activities` prompt using `{{get_activities.output}}`.\n\nRunning this function will first execute `get_activities`, and use its output to resolve the `gen_itinerary` prompt before executing it.\nThis is transitive, so it computes the Directed Acyclic Graph of dependencies to execute. Complex relationships can be modeled this way.\n\n```python\nconfig.run_with_dependencies("gen_itinerary")\n```\n\n### Updating metadata and parameters\n\nUse the `get/set_metadata` and `get/set_parameter` methods to interact with metadata and parameters (`set_parameter` is just syntactic sugar to update `"metadata.parameters"`)\n\n```python\nconfig.set_metadata("key", data, "prompt_name")\n```\n\nNote: if `"prompt_name"` is specified, the metadata is updated specifically for that prompt. Otherwise, the global metadata is updated.\n\n### Register new `ModelParser`\n\nUse the `AIConfigRuntime.register_model_parser` if you want to use a different `ModelParser`, or configure AIConfig to work with an additional model.\n\nAIConfig uses the model name string to retrieve the right `ModelParser` for a given Prompt (see `AIConfigRuntime.get_model_parser`), so you can register a different ModelParser for the same ID to override which `ModelParser` handles a Prompt.\n\nFor example, suppose I want to use `MyOpenAIModelParser` to handle `gpt-4` prompts. I can do the following at the start of my application:\n\n```python\nAIConfigRuntime.register_model_parser(myModelParserInstance, ["gpt-4"])\n```\n\n### Callback events\n\nUse callback events to trace and monitor what\'s going on -- helpful for debugging and observability.\n\n```python\nfrom aiconfig import AIConfigRuntime, CallbackEvent, CallbackManager\nconfig = AIConfigRuntime.load(\'aiconfig.json\')\n\nasync def my_custom_callback(event: CallbackEvent) -> None:\n  print(f"Event triggered: {event.name}", event)\n\ncallback_manager = CallbackManager([my_custom_callback])\nconfig.set_callback_manager(callback_manager)\n\nawait config.run("prompt_name")\n```\n\n[**Read more** here](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\n## Extensibility\n\nAIConfig is designed to be customized and extended for your use-case. The [Extensibility](/docs/extensibility) guide goes into more detail.\n\nCurrently, there are 3 core ways to extend AIConfig:\n\n1. [Supporting other models](https://aiconfig.lastmileai.dev/docs/extensibility#1-bring-your-own-model) - define a ModelParser extension\n2. [Callback event handlers](https://aiconfig.lastmileai.dev/docs/extensibility#2-callback-handlers) - tracing and monitoring\n3. [Custom metadata](https://aiconfig.lastmileai.dev/docs/extensibility#3-custom-metadata) - save custom fields in `aiconfig`\n\n## Contributing to `aiconfig`\n\nThis is our first open-source project and we\'d love your help.\n\nSee our [contributing guidelines](https://aiconfig.lastmileai.dev/docs/contributing) -- we would especially love help adding support for additional models that the community wants.\n\n## Cookbooks\n\nWe provide several guides to demonstrate the power of `aiconfig`.\n\n> **See the [`cookbooks`](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks) folder for examples to clone.**\n\n### Chatbot\n\n- [Wizard GPT](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT) - speak to a wizard on your CLI\n\n- [CLI-mate](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Cli-Mate) - help you make code-mods interactively on your codebase.\n\n### Retrieval Augmented Generated (RAG)\n\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n\nAt its core, RAG is about passing data into prompts. Read how to [pass data](/docs/overview/parameters) with AIConfig.\n\n### Function calling\n\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n\n### Prompt routing\n\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n\n### Chain of Thought\n\nA variant of chain-of-thought is Chain of Verification, used to help reduce hallucinations. Check out the aiconfig cookbook for CoVe:\n\n- [Chain of Verification](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### Using local LLaMA2 with `aiconfig`\n\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n\n### Hugging Face text generation\n\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n\n### Google PaLM\n\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n## Roadmap\n\nThis project is under active development.\n\nIf you\'d like to help, please see the [contributing guidelines](#contributing-to-aiconfig).\n\nPlease create issues for additional capabilities you\'d like to see.\n\nHere\'s what\'s already on our roadmap:\n\n- Evaluation interfaces: allow `aiconfig` artifacts to be evaluated with user-defined eval functions.\n  - We are also considering integrating with existing evaluation frameworks.\n- Local editor for `aiconfig`: enable you to interact with aiconfigs more intuitively.\n- OpenAI Assistants API support\n- Multi-modal ModelParsers:\n  - GPT4-V support\n  - DALLE-3\n  - Whisper\n  - HuggingFace image generation\n\n## FAQs\n\n### How should I edit an `aiconfig` file?\n\nEditing a configshould be done either programmatically via SDK or via the UI (workbooks):\n\n- [Programmatic](https://github.com/lastmile-ai/aiconfig/blob/main/cookbooks/Create-AIConfig-Programmatically/create_aiconfig_programmatically.ipynb) editing.\n\n- [Edit with a workbook](#edit-aiconfig-in-a-notebook-editor) editor: this is similar to editing an ipynb file as a notebook (most people never touch the json ipynb directly)\n\nYou should only edit the `aiconfig` by hand for minor modifications, like tweaking a prompt string or updating some metadata.\n\n### Does this support custom endpoints?\n\nOut of the box, AIConfig already supports all OpenAI GPT\\* models, Googles PaLM model and any textgeneration model on Hugging Face (like Mistral). See [Supported Models](#supported-models) for more details.\n\nAdditionally, you can install `aiconfig` [extensions](https://github.com/lastmile-ai/aiconfig/tree/main/extensions) for additional models (see question below).\n\n### Is OpenAI function calling supported?\n\nYes. [This example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI) goes through how to do it.\n\nWe are also working on adding support for the Assistants API.\n\n### How can I use aiconfig with my own model endpoint?\n\nModel support is implemented as ModelParsers in the AIConfig SDK, and the idea is that anyone, including you, can define a ModelParser (and even publish it as an extension package).\n\nAll thats needed to use a model with AIConfig is a ModelParser that knows\n\n- how to serialize data from a model into the aiconfig format\n- how to deserialize data from an aiconfig into the type the model expects\n- how to run inference for model.\n\nFor more details, see [Extensibility](https://aiconfig.lastmileai.dev/docs/extensibility).\n\n### When should I store outputs in an `aiconfig`?\n\nThe `AIConfigRuntime` object is used to interact with an aiconfig programmatically (see [SDK usage guide](#aiconfig-sdk)). As you run prompts, this object keeps track of the outputs returned from the model.\n\nYou can choose to serialize these outputs back into the `aiconfig` by using the `config.save(include_outputs=True)` API. This can be useful for preserving context -- think of it like session state.\n\nFor example, you can use aiconfig to create a chatbot, and use the same format to save the chat history so it can be resumed for the next session.\n\nYou can also choose to save outputs to a _different_ file than the original config -- `config.save("history.aiconfig.json", include_outputs=True)`.\n\n### Why should I use `aiconfig` instead of things like [configurator](https://pypi.org/project/configurator/)?\n\nIt helps to have a [standardized format](http://aiconfig.lastmileai.dev/docs/overview/ai-config-format) specifically for storing generative AI prompts, inference results, model parameters and arbitrary metadata, as opposed to a general-purpose configuration schema.\n\nWith that standardization, you just need a layer that knows how to serialize/deserialize from that format into whatever the inference endpoints require.\n\n### This looks similar to `ipynb` for Jupyter notebooks\n\nWe believe that notebooks are a perfect iteration environment for generative AI -- they are flexible, multi-modal, and collaborative.\n\nThe multi-modality and flexibility offered by notebooks and [`ipynb`](https://ipython.org/ipython-doc/3/notebook/nbformat.html) offers a good interaction model for generative AI. The `aiconfig` file format is extensible like `ipynb`, and AI Workbook editor allows rapid iteration in a notebook-like IDE.\n\n_AI Workbooks are to AIConfig what Jupyter notebooks are to `ipynb`_\n\nThere are 2 areas where we are going beyond what notebooks offer:\n\n1. `aiconfig` is more **source-control friendly** than `ipynb`. `ipynb` stores binary data (images, etc.) by encoding it in the file, while `aiconfig` recommends using file URI references instead.\n2. `aiconfig` can be imported and **connected to application code** using the AIConfig SDK.\n', 'repo_name': 'aiconfig'}, 'options': None, 'kwargs': {}} ts_ns=1702174862235185200
2023-12-09 21:21:26,742 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '<div align="center"><picture>\n  <img alt="aiconfig" src="aiconfig-docs/static/img/readme_logo.png" />\n</picture></div>\n<br/>\n\n![Python](https://github.com/lastmile-ai/aiconfig/actions/workflows/main_python.yml/badge.svg)\n![Node](https://github.com/lastmile-ai/aiconfig/actions/workflows/main-typescript.yml/badge.svg)\n![Docs](https://github.com/lastmile-ai/aiconfig/actions/workflows/test-deploy-docs.yml/badge.svg)\n[![Discord](<https://img.shields.io/badge/Discord-LastMile%20AI-Blue?color=rgb(37%2C%20150%2C%20190)>)](https://discord.gg/qMqgzDae)\n\n> Full documentation: **[aiconfig.lastmileai.dev](https://aiconfig.lastmileai.dev/)**\n\n## Overview\n\nAIConfig saves prompts, models and model parameters as source control friendly configs. This allows you to iterate on prompts and model parameters _separately from your application code_.\n\n1. **Prompts as configs**: a [standardized JSON format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to store generative AI model settings, prompt inputs/outputs, and flexible metadata.\n2. **Model-agnostic SDK**: Python & Node SDKs to use `aiconfig` in your application code. AIConfig is designed to be **model-agnostic** and **multi-modal**, so you can extend it to work with any generative AI model, including text, image and audio.\n3. **AI Workbook editor**: A [notebook-like playground](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to edit `aiconfig` files visually, run prompts, tweak models and model settings, and chain things together.\n\n### What problem it solves\n\nToday, application code is tightly coupled with the gen AI settings for the application -- prompts, parameters, and model-specific logic is all jumbled in with app code.\n\n- results in increased complexity\n- makes it hard to iterate on the prompts or try different models easily\n- makes it hard to evaluate prompt/model performance\n\nAIConfig helps unwind complexity by separating prompts, model parameters, and model-specific logic from your application.\n\n- simplifies application code -- simply call `config.run()`\n- open the `aiconfig` in a playground to iterate quickly\n- version control and evaluate the `aiconfig` - it\'s the AI artifact for your application.\n\n![AIConfig flow](aiconfig-docs/static/img/aiconfig_dataflow.png)\n\n### Quicknav\n\n<ul style="margin-bottom:0; padding-bottom:0;">\n  <li><a href="#install">Getting Started</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig">Create an AIConfig</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig">Run a prompt</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/parameters">Pass data into prompts</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain">Prompt Chains</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig">Callbacks and monitoring</a></li>\n  </ul>\n  <li><a href="#aiconfig-sdk">SDK Cheatsheet</a></li>\n  <li><a href="#cookbooks">Cookbooks and guides</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT">CLI Chatbot</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig">RAG with AIConfig</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing">Prompt routing</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI">OpenAI function calling</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification">Chain of Verification</a></li>\n  </ul>\n  <li><a href="#supported-models">Supported models</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama">LLaMA2 example</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace">Hugging Face (Mistral-7B) example</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency">PaLM</a></li>\n  </ul>\n  <li><a href="#extensibility">Extensibility</a></li>\n  <li><a href="#contributing-to-aiconfig">Contributing</a></li>\n  <li><a href="#roadmap">Roadmap</a></li>\n  <li><a href="#faqs">FAQ</a></li>\n</ul>\n\n## Features\n\n- [x] **Source-control friendly** [`aiconfig` format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to save prompts and model settings, which you can use for evaluation, reproducibility and simplifying your application code.\n- [x] **Multi-modal and model agnostic**. Use with any model, and serialize/deserialize data with the same `aiconfig` format.\n- [x] **Prompt chaining and parameterization** with [{{handlebars}}](https://handlebarsjs.com/) templating syntax, allowing you to pass dynamic data into prompts (as well as between prompts).\n- [x] **Streaming** supported out of the box, allowing you to get playground-like streaming wherever you use `aiconfig`.\n- [x] **Notebook editor**. [AI Workbooks editor](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to visually create your `aiconfig`, and use the SDK to connect it to your application code.\n\n## Install\n\nInstall with your favorite package manager for Node or Python.\n\n### Node.js\n\n#### `npm` or `yarn`\n\n```bash\nnpm install aiconfig\n```\n\n```bash\nyarn add aiconfig\n```\n\n### Python\n\n#### `pip3` or `poetry`\n\n```bash\npip3 install python-aiconfig\n```\n\n```bash\npoetry add python-aiconfig\n```\n\n[Detailed installation instructions](https://aiconfig.lastmileai.dev/docs/getting-started/#installation).\n\n### Set your OpenAI API Key\n\n> **Note**: Make sure to specify the API keys (such as [`OPENAI_API_KEY`](https://platform.openai.com/api-keys)) in your environment before proceeding.\n\nIn your CLI, set the environment variable:\n\n```bash\nexport OPENAI_API_KEY=my_key\n```\n\n## Getting Started\n\n> We cover Python instructions here, for Node.js please see the [detailed Getting Started guide](https://aiconfig.lastmileai.dev/docs/getting-started)\n\nIn this quickstart, you will create a customizable NYC travel itinerary using `aiconfig`.\n\nThis AIConfig contains a prompt chain to get a list of travel activities from an LLM and then generate an itinerary in an order specified by the user.\n\n> **Link to tutorial code: [here](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Getting-Started)**\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/805173d1-0f83-44c5-b570-c776bb7dba66\n\n### Download `travel.aiconfig.json`\n\n> **Note**: Don\'t worry if you don\'t understand all the pieces of this yet, we\'ll go over it step by step.\n\n```json\n{\n  "name": "NYC Trip Planner",\n  "description": "Intrepid explorer with ChatGPT and AIConfig",\n  "schema_version": "latest",\n  "metadata": {\n    "models": {\n      "gpt-3.5-turbo": {\n        "model": "gpt-3.5-turbo",\n        "top_p": 1,\n        "temperature": 1\n      },\n      "gpt-4": {\n        "model": "gpt-4",\n        "max_tokens": 3000,\n        "system_prompt": "You are an expert travel coordinator with exquisite taste."\n      }\n    },\n    "default_model": "gpt-3.5-turbo"\n  },\n  "prompts": [\n    {\n      "name": "get_activities",\n      "input": "Tell me 10 fun attractions to do in NYC."\n    },\n    {\n      "name": "gen_itinerary",\n      "input": "Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}.",\n      "metadata": {\n        "model": "gpt-4",\n        "parameters": {\n          "order_by": "geographic location"\n        }\n      }\n    }\n  ]\n}\n```\n\n### Run the `get_activities` prompt.\n\nYou don\'t need to worry about how to run inference for the model; it\'s all handled by AIConfig. The prompt runs with gpt-3.5-turbo since that is the `default_model` for this AIConfig.\n\nCreate a new file called `app.py` and and enter the following code:\n\n```python\nimport asyncio\nfrom aiconfig import AIConfigRuntime, InferenceOptions\n\nasync def main():\n  # Load the aiconfig\n  config = AIConfigRuntime.load(\'travel.aiconfig.json\')\n\n  # Run a single prompt (with streaming)\n  inference_options = InferenceOptions(stream=True)\n  await config.run("get_activities", options=inference_options)\n\nasyncio.run(main())\n```\n\nNow run this in your terminal with the command:\n\n```bash\npython3 app.py\n```\n\n### Run the `gen_itinerary` prompt.\n\nIn your `app.py` file, change the last line to below:\n\n```python\nawait config.run("gen_itinerary", params=None, options=inference_options)\n```\n\nRe-run the command in your terminal:\n\n```bash\npython3 app.py\n```\n\nThis prompt depends on the output of `get_activities`. It also takes in parameters (user input) to determine the customized itinerary.\n\nLet\'s take a closer look:\n\n**`gen_itinerary` prompt:**\n\n```\n"Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}."\n```\n\n**prompt metadata:**\n\n```json\n{\n  "metadata": {\n    "model": "gpt-4",\n    "parameters": {\n      "order_by": "geographic location"\n    }\n  }\n}\n```\n\nObserve the following:\n\n1. The prompt depends on the output of the `get_activities` prompt.\n2. It also depends on an `order_by` parameter (using {{handlebars}} syntax)\n3. It uses **gpt-4**, whereas the `get_activities` prompt it depends on uses **gpt-3.5-turbo**.\n\n> Effectively, this is a prompt chain between `gen_itinerary` and `get_activities` prompts, _as well as_ as a model chain between **gpt-3.5-turbo** and **gpt-4**.\n\nLet\'s run this with AIConfig:\n\nReplace `config.run` above with this:\n\n```python\nawait config.run("gen_itinerary", params={"order_by": "duration"}, options=inference_options, run_with_dependencies=True)\n```\n\nNotice how simple the syntax is to perform a fairly complex task - running 2 different prompts across 2 different models and chaining one\'s output as part of the input of another.\n\nThe code will just run `get_activities`, then pipe its output as an input to `gen_itinerary`, and finally run `gen_itinerary`.\n\n### Save the AIConfig\n\nLet\'s save the AIConfig back to disk, and serialize the outputs from the latest inference run as well:\n\n```python\n# Save the aiconfig to disk. and serialize outputs from the model run\nconfig.save(\'updated.aiconfig.json\', include_outputs=True)\n```\n\n### Edit `aiconfig` in a notebook editor\n\nWe can iterate on an `aiconfig` using a notebook-like editor called an **AI Workbook**. Now that we have an `aiconfig` file artifact that encapsulates the generative AI part of our application, we can iterate on it separately from the application code that uses it.\n\n1. Go to https://lastmileai.dev.\n2. Go to Workbooks page: https://lastmileai.dev/workbooks\n3. Click dropdown from \'+ New Workbook\' and select \'Create from AIConfig\'\n4. Upload `travel.aiconfig.json`\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/5d901493-bbda-4f8e-93c7-dd9a91bf242e\n\nTry out the workbook playground here: **[NYC Travel Workbook](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9)**\n\n> **We are working on a local editor that you can run yourself. For now, please use the hosted version on https://lastmileai.dev.**\n\n### Additional Guides\n\nThere is a lot you can do with `aiconfig`. We have several other tutorials to help get you started:\n\n- [Create an AIConfig from scratch](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig)\n- [Run a prompt](https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig)\n- [Pass data into prompts](https://aiconfig.lastmileai.dev/docs/overview/parameters)\n- [Prompt chains](https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain)\n- [Callbacks and monitoring](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\nHere are some example uses:\n\n- [CLI Chatbot](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT)\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [Chain of thought](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### OpenAI Introspection API\n\nIf you are already using OpenAI completion API\'s in your application, you can get started very quickly to start saving the messages in an `aiconfig`.\n\nUsage: see openai_wrapper.ipynb.\n\nNow you can continue using `openai` completion API as normal. When you want to save the config, just call `new_config.save()` and all your openai completion calls will get serialized to disk.\n\n> [**Detailed guide here**](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig#openai-api-python-wrapper)\n\n## Supported Models\n\nAIConfig supports the following models out of the box:\n\n- OpenAI chat models (GPT-3, GPT-3.5, GPT-4)\n- LLaMA2 (running locally)\n- Google PaLM models (PaLM chat)\n- Hugging Face text generation models (e.g. Mistral-7B)\n\n### Examples\n\n- [OpenAI](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n> If you need to use a model that isn\'t provided out of the box, you can implement a `ModelParser` for it (see [Extending AIConfig](#extending-aiconfig)). **We welcome [contributions](https://aiconfig.lastmileai.dev/docs/contributing)**\n\n## AIConfig Schema\n\n[AIConfig specification](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format)\n\n## AIConfig SDK\n\n> Read the [Usage Guide](https://aiconfig.lastmileai.dev/docs/usage-guide) for more details.\n\nThe AIConfig SDK supports CRUD operations for prompts, models, parameters and metadata. Here are some common examples.\n\nThe root interface is the `AIConfigRuntime` object. That is the entrypoint for interacting with an AIConfig programmatically.\n\nLet\'s go over a few key CRUD operations to give a glimpse.\n\n### AIConfig `create`\n\n```python\nconfig = AIConfigRuntime.create("aiconfig name", "description")\n```\n\n### Prompt `resolve`\n\n`resolve` deserializes an existing `Prompt` into the data object that its model expects.\n\n```python\nconfig.resolve("prompt_name", params)\n```\n\n`params` are overrides you can specify to resolve any `{{handlebars}}` templates in the prompt. See the `gen_itinerary` prompt in the Getting Started example.\n\n### Prompt `serialize`\n\n`serialize` is the inverse of `resolve` -- it serializes the data object that a model understands into a `Prompt` object that can be serialized into the `aiconfig` format.\n\n```python\nconfig.serialize("model_name", data, "prompt_name")\n```\n\n### Prompt `run`\n\n`run` is used to run inference for the specified `Prompt`.\n\n```python\nconfig.run("prompt_name", params)\n```\n\n### `run_with_dependencies`\n\nThis is a variant of `run` -- this re-runs all prompt dependencies.\nFor example, in [`travel.aiconfig.json`](#download-travelaiconfigjson), the `gen_itinerary` prompt references the output of the `get_activities` prompt using `{{get_activities.output}}`.\n\nRunning this function will first execute `get_activities`, and use its output to resolve the `gen_itinerary` prompt before executing it.\nThis is transitive, so it computes the Directed Acyclic Graph of dependencies to execute. Complex relationships can be modeled this way.\n\n```python\nconfig.run_with_dependencies("gen_itinerary")\n```\n\n### Updating metadata and parameters\n\nUse the `get/set_metadata` and `get/set_parameter` methods to interact with metadata and parameters (`set_parameter` is just syntactic sugar to update `"metadata.parameters"`)\n\n```python\nconfig.set_metadata("key", data, "prompt_name")\n```\n\nNote: if `"prompt_name"` is specified, the metadata is updated specifically for that prompt. Otherwise, the global metadata is updated.\n\n### Register new `ModelParser`\n\nUse the `AIConfigRuntime.register_model_parser` if you want to use a different `ModelParser`, or configure AIConfig to work with an additional model.\n\nAIConfig uses the model name string to retrieve the right `ModelParser` for a given Prompt (see `AIConfigRuntime.get_model_parser`), so you can register a different ModelParser for the same ID to override which `ModelParser` handles a Prompt.\n\nFor example, suppose I want to use `MyOpenAIModelParser` to handle `gpt-4` prompts. I can do the following at the start of my application:\n\n```python\nAIConfigRuntime.register_model_parser(myModelParserInstance, ["gpt-4"])\n```\n\n### Callback events\n\nUse callback events to trace and monitor what\'s going on -- helpful for debugging and observability.\n\n```python\nfrom aiconfig import AIConfigRuntime, CallbackEvent, CallbackManager\nconfig = AIConfigRuntime.load(\'aiconfig.json\')\n\nasync def my_custom_callback(event: CallbackEvent) -> None:\n  print(f"Event triggered: {event.name}", event)\n\ncallback_manager = CallbackManager([my_custom_callback])\nconfig.set_callback_manager(callback_manager)\n\nawait config.run("prompt_name")\n```\n\n[**Read more** here](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\n## Extensibility\n\nAIConfig is designed to be customized and extended for your use-case. The [Extensibility](/docs/extensibility) guide goes into more detail.\n\nCurrently, there are 3 core ways to extend AIConfig:\n\n1. [Supporting other models](https://aiconfig.lastmileai.dev/docs/extensibility#1-bring-your-own-model) - define a ModelParser extension\n2. [Callback event handlers](https://aiconfig.lastmileai.dev/docs/extensibility#2-callback-handlers) - tracing and monitoring\n3. [Custom metadata](https://aiconfig.lastmileai.dev/docs/extensibility#3-custom-metadata) - save custom fields in `aiconfig`\n\n## Contributing to `aiconfig`\n\nThis is our first open-source project and we\'d love your help.\n\nSee our [contributing guidelines](https://aiconfig.lastmileai.dev/docs/contributing) -- we would especially love help adding support for additional models that the community wants.\n\n## Cookbooks\n\nWe provide several guides to demonstrate the power of `aiconfig`.\n\n> **See the [`cookbooks`](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks) folder for examples to clone.**\n\n### Chatbot\n\n- [Wizard GPT](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT) - speak to a wizard on your CLI\n\n- [CLI-mate](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Cli-Mate) - help you make code-mods interactively on your codebase.\n\n### Retrieval Augmented Generated (RAG)\n\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n\nAt its core, RAG is about passing data into prompts. Read how to [pass data](/docs/overview/parameters) with AIConfig.\n\n### Function calling\n\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n\n### Prompt routing\n\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n\n### Chain of Thought\n\nA variant of chain-of-thought is Chain of Verification, used to help reduce hallucinations. Check out the aiconfig cookbook for CoVe:\n\n- [Chain of Verification](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### Using local LLaMA2 with `aiconfig`\n\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n\n### Hugging Face text generation\n\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n\n### Google PaLM\n\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n## Roadmap\n\nThis project is under active development.\n\nIf you\'d like to help, please see the [contributing guidelines](#contributing-to-aiconfig).\n\nPlease create issues for additional capabilities you\'d like to see.\n\nHere\'s what\'s already on our roadmap:\n\n- Evaluation interfaces: allow `aiconfig` artifacts to be evaluated with user-defined eval functions.\n  - We are also considering integrating with existing evaluation frameworks.\n- Local editor for `aiconfig`: enable you to interact with aiconfigs more intuitively.\n- OpenAI Assistants API support\n- Multi-modal ModelParsers:\n  - GPT4-V support\n  - DALLE-3\n  - Whisper\n  - HuggingFace image generation\n\n## FAQs\n\n### How should I edit an `aiconfig` file?\n\nEditing a configshould be done either programmatically via SDK or via the UI (workbooks):\n\n- [Programmatic](https://github.com/lastmile-ai/aiconfig/blob/main/cookbooks/Create-AIConfig-Programmatically/create_aiconfig_programmatically.ipynb) editing.\n\n- [Edit with a workbook](#edit-aiconfig-in-a-notebook-editor) editor: this is similar to editing an ipynb file as a notebook (most people never touch the json ipynb directly)\n\nYou should only edit the `aiconfig` by hand for minor modifications, like tweaking a prompt string or updating some metadata.\n\n### Does this support custom endpoints?\n\nOut of the box, AIConfig already supports all OpenAI GPT\\* models, Googles PaLM model and any textgeneration model on Hugging Face (like Mistral). See [Supported Models](#supported-models) for more details.\n\nAdditionally, you can install `aiconfig` [extensions](https://github.com/lastmile-ai/aiconfig/tree/main/extensions) for additional models (see question below).\n\n### Is OpenAI function calling supported?\n\nYes. [This example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI) goes through how to do it.\n\nWe are also working on adding support for the Assistants API.\n\n### How can I use aiconfig with my own model endpoint?\n\nModel support is implemented as ModelParsers in the AIConfig SDK, and the idea is that anyone, including you, can define a ModelParser (and even publish it as an extension package).\n\nAll thats needed to use a model with AIConfig is a ModelParser that knows\n\n- how to serialize data from a model into the aiconfig format\n- how to deserialize data from an aiconfig into the type the model expects\n- how to run inference for model.\n\nFor more details, see [Extensibility](https://aiconfig.lastmileai.dev/docs/extensibility).\n\n### When should I store outputs in an `aiconfig`?\n\nThe `AIConfigRuntime` object is used to interact with an aiconfig programmatically (see [SDK usage guide](#aiconfig-sdk)). As you run prompts, this object keeps track of the outputs returned from the model.\n\nYou can choose to serialize these outputs back into the `aiconfig` by using the `config.save(include_outputs=True)` API. This can be useful for preserving context -- think of it like session state.\n\nFor example, you can use aiconfig to create a chatbot, and use the same format to save the chat history so it can be resumed for the next session.\n\nYou can also choose to save outputs to a _different_ file than the original config -- `config.save("history.aiconfig.json", include_outputs=True)`.\n\n### Why should I use `aiconfig` instead of things like [configurator](https://pypi.org/project/configurator/)?\n\nIt helps to have a [standardized format](http://aiconfig.lastmileai.dev/docs/overview/ai-config-format) specifically for storing generative AI prompts, inference results, model parameters and arbitrary metadata, as opposed to a general-purpose configuration schema.\n\nWith that standardization, you just need a layer that knows how to serialize/deserialize from that format into whatever the inference endpoints require.\n\n### This looks similar to `ipynb` for Jupyter notebooks\n\nWe believe that notebooks are a perfect iteration environment for generative AI -- they are flexible, multi-modal, and collaborative.\n\nThe multi-modality and flexibility offered by notebooks and [`ipynb`](https://ipython.org/ipython-doc/3/notebook/nbformat.html) offers a good interaction model for generative AI. The `aiconfig` file format is extensible like `ipynb`, and AI Workbook editor allows rapid iteration in a notebook-like IDE.\n\n_AI Workbooks are to AIConfig what Jupyter notebooks are to `ipynb`_\n\nThere are 2 areas where we are going beyond what notebooks offer:\n\n1. `aiconfig` is more **source-control friendly** than `ipynb`. `ipynb` stores binary data (images, etc.) by encoding it in the file, while `aiconfig` recommends using file URI references instead.\n2. `aiconfig` can be imported and **connected to application code** using the AIConfig SDK.\n', 'repo_name': 'aiconfig'}, 'options': None, 'kwargs': {}} ts_ns=1702174862235185200
2023-12-09 21:21:26,744 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '<div align="center"><picture>\n  <img alt="aiconfig" src="aiconfig-docs/static/img/readme_logo.png" />\n</picture></div>\n<br/>\n\n![Python](https://github.com/lastmile-ai/aiconfig/actions/workflows/main_python.yml/badge.svg)\n![Node](https://github.com/lastmile-ai/aiconfig/actions/workflows/main-typescript.yml/badge.svg)\n![Docs](https://github.com/lastmile-ai/aiconfig/actions/workflows/test-deploy-docs.yml/badge.svg)\n[![Discord](<https://img.shields.io/badge/Discord-LastMile%20AI-Blue?color=rgb(37%2C%20150%2C%20190)>)](https://discord.gg/qMqgzDae)\n\n> Full documentation: **[aiconfig.lastmileai.dev](https://aiconfig.lastmileai.dev/)**\n\n## Overview\n\nAIConfig saves prompts, models and model parameters as source control friendly configs. This allows you to iterate on prompts and model parameters _separately from your application code_.\n\n1. **Prompts as configs**: a [standardized JSON format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to store generative AI model settings, prompt inputs/outputs, and flexible metadata.\n2. **Model-agnostic SDK**: Python & Node SDKs to use `aiconfig` in your application code. AIConfig is designed to be **model-agnostic** and **multi-modal**, so you can extend it to work with any generative AI model, including text, image and audio.\n3. **AI Workbook editor**: A [notebook-like playground](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to edit `aiconfig` files visually, run prompts, tweak models and model settings, and chain things together.\n\n### What problem it solves\n\nToday, application code is tightly coupled with the gen AI settings for the application -- prompts, parameters, and model-specific logic is all jumbled in with app code.\n\n- results in increased complexity\n- makes it hard to iterate on the prompts or try different models easily\n- makes it hard to evaluate prompt/model performance\n\nAIConfig helps unwind complexity by separating prompts, model parameters, and model-specific logic from your application.\n\n- simplifies application code -- simply call `config.run()`\n- open the `aiconfig` in a playground to iterate quickly\n- version control and evaluate the `aiconfig` - it\'s the AI artifact for your application.\n\n![AIConfig flow](aiconfig-docs/static/img/aiconfig_dataflow.png)\n\n### Quicknav\n\n<ul style="margin-bottom:0; padding-bottom:0;">\n  <li><a href="#install">Getting Started</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig">Create an AIConfig</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig">Run a prompt</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/parameters">Pass data into prompts</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain">Prompt Chains</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig">Callbacks and monitoring</a></li>\n  </ul>\n  <li><a href="#aiconfig-sdk">SDK Cheatsheet</a></li>\n  <li><a href="#cookbooks">Cookbooks and guides</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT">CLI Chatbot</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig">RAG with AIConfig</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing">Prompt routing</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI">OpenAI function calling</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification">Chain of Verification</a></li>\n  </ul>\n  <li><a href="#supported-models">Supported models</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama">LLaMA2 example</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace">Hugging Face (Mistral-7B) example</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency">PaLM</a></li>\n  </ul>\n  <li><a href="#extensibility">Extensibility</a></li>\n  <li><a href="#contributing-to-aiconfig">Contributing</a></li>\n  <li><a href="#roadmap">Roadmap</a></li>\n  <li><a href="#faqs">FAQ</a></li>\n</ul>\n\n## Features\n\n- [x] **Source-control friendly** [`aiconfig` format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to save prompts and model settings, which you can use for evaluation, reproducibility and simplifying your application code.\n- [x] **Multi-modal and model agnostic**. Use with any model, and serialize/deserialize data with the same `aiconfig` format.\n- [x] **Prompt chaining and parameterization** with [{{handlebars}}](https://handlebarsjs.com/) templating syntax, allowing you to pass dynamic data into prompts (as well as between prompts).\n- [x] **Streaming** supported out of the box, allowing you to get playground-like streaming wherever you use `aiconfig`.\n- [x] **Notebook editor**. [AI Workbooks editor](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to visually create your `aiconfig`, and use the SDK to connect it to your application code.\n\n## Install\n\nInstall with your favorite package manager for Node or Python.\n\n### Node.js\n\n#### `npm` or `yarn`\n\n```bash\nnpm install aiconfig\n```\n\n```bash\nyarn add aiconfig\n```\n\n### Python\n\n#### `pip3` or `poetry`\n\n```bash\npip3 install python-aiconfig\n```\n\n```bash\npoetry add python-aiconfig\n```\n\n[Detailed installation instructions](https://aiconfig.lastmileai.dev/docs/getting-started/#installation).\n\n### Set your OpenAI API Key\n\n> **Note**: Make sure to specify the API keys (such as [`OPENAI_API_KEY`](https://platform.openai.com/api-keys)) in your environment before proceeding.\n\nIn your CLI, set the environment variable:\n\n```bash\nexport OPENAI_API_KEY=my_key\n```\n\n## Getting Started\n\n> We cover Python instructions here, for Node.js please see the [detailed Getting Started guide](https://aiconfig.lastmileai.dev/docs/getting-started)\n\nIn this quickstart, you will create a customizable NYC travel itinerary using `aiconfig`.\n\nThis AIConfig contains a prompt chain to get a list of travel activities from an LLM and then generate an itinerary in an order specified by the user.\n\n> **Link to tutorial code: [here](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Getting-Started)**\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/805173d1-0f83-44c5-b570-c776bb7dba66\n\n### Download `travel.aiconfig.json`\n\n> **Note**: Don\'t worry if you don\'t understand all the pieces of this yet, we\'ll go over it step by step.\n\n```json\n{\n  "name": "NYC Trip Planner",\n  "description": "Intrepid explorer with ChatGPT and AIConfig",\n  "schema_version": "latest",\n  "metadata": {\n    "models": {\n      "gpt-3.5-turbo": {\n        "model": "gpt-3.5-turbo",\n        "top_p": 1,\n        "temperature": 1\n      },\n      "gpt-4": {\n        "model": "gpt-4",\n        "max_tokens": 3000,\n        "system_prompt": "You are an expert travel coordinator with exquisite taste."\n      }\n    },\n    "default_model": "gpt-3.5-turbo"\n  },\n  "prompts": [\n    {\n      "name": "get_activities",\n      "input": "Tell me 10 fun attractions to do in NYC."\n    },\n    {\n      "name": "gen_itinerary",\n      "input": "Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}.",\n      "metadata": {\n        "model": "gpt-4",\n        "parameters": {\n          "order_by": "geographic location"\n        }\n      }\n    }\n  ]\n}\n```\n\n### Run the `get_activities` prompt.\n\nYou don\'t need to worry about how to run inference for the model; it\'s all handled by AIConfig. The prompt runs with gpt-3.5-turbo since that is the `default_model` for this AIConfig.\n\nCreate a new file called `app.py` and and enter the following code:\n\n```python\nimport asyncio\nfrom aiconfig import AIConfigRuntime, InferenceOptions\n\nasync def main():\n  # Load the aiconfig\n  config = AIConfigRuntime.load(\'travel.aiconfig.json\')\n\n  # Run a single prompt (with streaming)\n  inference_options = InferenceOptions(stream=True)\n  await config.run("get_activities", options=inference_options)\n\nasyncio.run(main())\n```\n\nNow run this in your terminal with the command:\n\n```bash\npython3 app.py\n```\n\n### Run the `gen_itinerary` prompt.\n\nIn your `app.py` file, change the last line to below:\n\n```python\nawait config.run("gen_itinerary", params=None, options=inference_options)\n```\n\nRe-run the command in your terminal:\n\n```bash\npython3 app.py\n```\n\nThis prompt depends on the output of `get_activities`. It also takes in parameters (user input) to determine the customized itinerary.\n\nLet\'s take a closer look:\n\n**`gen_itinerary` prompt:**\n\n```\n"Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}."\n```\n\n**prompt metadata:**\n\n```json\n{\n  "metadata": {\n    "model": "gpt-4",\n    "parameters": {\n      "order_by": "geographic location"\n    }\n  }\n}\n```\n\nObserve the following:\n\n1. The prompt depends on the output of the `get_activities` prompt.\n2. It also depends on an `order_by` parameter (using {{handlebars}} syntax)\n3. It uses **gpt-4**, whereas the `get_activities` prompt it depends on uses **gpt-3.5-turbo**.\n\n> Effectively, this is a prompt chain between `gen_itinerary` and `get_activities` prompts, _as well as_ as a model chain between **gpt-3.5-turbo** and **gpt-4**.\n\nLet\'s run this with AIConfig:\n\nReplace `config.run` above with this:\n\n```python\nawait config.run("gen_itinerary", params={"order_by": "duration"}, options=inference_options, run_with_dependencies=True)\n```\n\nNotice how simple the syntax is to perform a fairly complex task - running 2 different prompts across 2 different models and chaining one\'s output as part of the input of another.\n\nThe code will just run `get_activities`, then pipe its output as an input to `gen_itinerary`, and finally run `gen_itinerary`.\n\n### Save the AIConfig\n\nLet\'s save the AIConfig back to disk, and serialize the outputs from the latest inference run as well:\n\n```python\n# Save the aiconfig to disk. and serialize outputs from the model run\nconfig.save(\'updated.aiconfig.json\', include_outputs=True)\n```\n\n### Edit `aiconfig` in a notebook editor\n\nWe can iterate on an `aiconfig` using a notebook-like editor called an **AI Workbook**. Now that we have an `aiconfig` file artifact that encapsulates the generative AI part of our application, we can iterate on it separately from the application code that uses it.\n\n1. Go to https://lastmileai.dev.\n2. Go to Workbooks page: https://lastmileai.dev/workbooks\n3. Click dropdown from \'+ New Workbook\' and select \'Create from AIConfig\'\n4. Upload `travel.aiconfig.json`\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/5d901493-bbda-4f8e-93c7-dd9a91bf242e\n\nTry out the workbook playground here: **[NYC Travel Workbook](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9)**\n\n> **We are working on a local editor that you can run yourself. For now, please use the hosted version on https://lastmileai.dev.**\n\n### Additional Guides\n\nThere is a lot you can do with `aiconfig`. We have several other tutorials to help get you started:\n\n- [Create an AIConfig from scratch](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig)\n- [Run a prompt](https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig)\n- [Pass data into prompts](https://aiconfig.lastmileai.dev/docs/overview/parameters)\n- [Prompt chains](https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain)\n- [Callbacks and monitoring](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\nHere are some example uses:\n\n- [CLI Chatbot](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT)\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [Chain of thought](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### OpenAI Introspection API\n\nIf you are already using OpenAI completion API\'s in your application, you can get started very quickly to start saving the messages in an `aiconfig`.\n\nUsage: see openai_wrapper.ipynb.\n\nNow you can continue using `openai` completion API as normal. When you want to save the config, just call `new_config.save()` and all your openai completion calls will get serialized to disk.\n\n> [**Detailed guide here**](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig#openai-api-python-wrapper)\n\n## Supported Models\n\nAIConfig supports the following models out of the box:\n\n- OpenAI chat models (GPT-3, GPT-3.5, GPT-4)\n- LLaMA2 (running locally)\n- Google PaLM models (PaLM chat)\n- Hugging Face text generation models (e.g. Mistral-7B)\n\n### Examples\n\n- [OpenAI](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n> If you need to use a model that isn\'t provided out of the box, you can implement a `ModelParser` for it (see [Extending AIConfig](#extending-aiconfig)). **We welcome [contributions](https://aiconfig.lastmileai.dev/docs/contributing)**\n\n## AIConfig Schema\n\n[AIConfig specification](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format)\n\n## AIConfig SDK\n\n> Read the [Usage Guide](https://aiconfig.lastmileai.dev/docs/usage-guide) for more details.\n\nThe AIConfig SDK supports CRUD operations for prompts, models, parameters and metadata. Here are some common examples.\n\nThe root interface is the `AIConfigRuntime` object. That is the entrypoint for interacting with an AIConfig programmatically.\n\nLet\'s go over a few key CRUD operations to give a glimpse.\n\n### AIConfig `create`\n\n```python\nconfig = AIConfigRuntime.create("aiconfig name", "description")\n```\n\n### Prompt `resolve`\n\n`resolve` deserializes an existing `Prompt` into the data object that its model expects.\n\n```python\nconfig.resolve("prompt_name", params)\n```\n\n`params` are overrides you can specify to resolve any `{{handlebars}}` templates in the prompt. See the `gen_itinerary` prompt in the Getting Started example.\n\n### Prompt `serialize`\n\n`serialize` is the inverse of `resolve` -- it serializes the data object that a model understands into a `Prompt` object that can be serialized into the `aiconfig` format.\n\n```python\nconfig.serialize("model_name", data, "prompt_name")\n```\n\n### Prompt `run`\n\n`run` is used to run inference for the specified `Prompt`.\n\n```python\nconfig.run("prompt_name", params)\n```\n\n### `run_with_dependencies`\n\nThis is a variant of `run` -- this re-runs all prompt dependencies.\nFor example, in [`travel.aiconfig.json`](#download-travelaiconfigjson), the `gen_itinerary` prompt references the output of the `get_activities` prompt using `{{get_activities.output}}`.\n\nRunning this function will first execute `get_activities`, and use its output to resolve the `gen_itinerary` prompt before executing it.\nThis is transitive, so it computes the Directed Acyclic Graph of dependencies to execute. Complex relationships can be modeled this way.\n\n```python\nconfig.run_with_dependencies("gen_itinerary")\n```\n\n### Updating metadata and parameters\n\nUse the `get/set_metadata` and `get/set_parameter` methods to interact with metadata and parameters (`set_parameter` is just syntactic sugar to update `"metadata.parameters"`)\n\n```python\nconfig.set_metadata("key", data, "prompt_name")\n```\n\nNote: if `"prompt_name"` is specified, the metadata is updated specifically for that prompt. Otherwise, the global metadata is updated.\n\n### Register new `ModelParser`\n\nUse the `AIConfigRuntime.register_model_parser` if you want to use a different `ModelParser`, or configure AIConfig to work with an additional model.\n\nAIConfig uses the model name string to retrieve the right `ModelParser` for a given Prompt (see `AIConfigRuntime.get_model_parser`), so you can register a different ModelParser for the same ID to override which `ModelParser` handles a Prompt.\n\nFor example, suppose I want to use `MyOpenAIModelParser` to handle `gpt-4` prompts. I can do the following at the start of my application:\n\n```python\nAIConfigRuntime.register_model_parser(myModelParserInstance, ["gpt-4"])\n```\n\n### Callback events\n\nUse callback events to trace and monitor what\'s going on -- helpful for debugging and observability.\n\n```python\nfrom aiconfig import AIConfigRuntime, CallbackEvent, CallbackManager\nconfig = AIConfigRuntime.load(\'aiconfig.json\')\n\nasync def my_custom_callback(event: CallbackEvent) -> None:\n  print(f"Event triggered: {event.name}", event)\n\ncallback_manager = CallbackManager([my_custom_callback])\nconfig.set_callback_manager(callback_manager)\n\nawait config.run("prompt_name")\n```\n\n[**Read more** here](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\n## Extensibility\n\nAIConfig is designed to be customized and extended for your use-case. The [Extensibility](/docs/extensibility) guide goes into more detail.\n\nCurrently, there are 3 core ways to extend AIConfig:\n\n1. [Supporting other models](https://aiconfig.lastmileai.dev/docs/extensibility#1-bring-your-own-model) - define a ModelParser extension\n2. [Callback event handlers](https://aiconfig.lastmileai.dev/docs/extensibility#2-callback-handlers) - tracing and monitoring\n3. [Custom metadata](https://aiconfig.lastmileai.dev/docs/extensibility#3-custom-metadata) - save custom fields in `aiconfig`\n\n## Contributing to `aiconfig`\n\nThis is our first open-source project and we\'d love your help.\n\nSee our [contributing guidelines](https://aiconfig.lastmileai.dev/docs/contributing) -- we would especially love help adding support for additional models that the community wants.\n\n## Cookbooks\n\nWe provide several guides to demonstrate the power of `aiconfig`.\n\n> **See the [`cookbooks`](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks) folder for examples to clone.**\n\n### Chatbot\n\n- [Wizard GPT](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT) - speak to a wizard on your CLI\n\n- [CLI-mate](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Cli-Mate) - help you make code-mods interactively on your codebase.\n\n### Retrieval Augmented Generated (RAG)\n\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n\nAt its core, RAG is about passing data into prompts. Read how to [pass data](/docs/overview/parameters) with AIConfig.\n\n### Function calling\n\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n\n### Prompt routing\n\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n\n### Chain of Thought\n\nA variant of chain-of-thought is Chain of Verification, used to help reduce hallucinations. Check out the aiconfig cookbook for CoVe:\n\n- [Chain of Verification](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### Using local LLaMA2 with `aiconfig`\n\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n\n### Hugging Face text generation\n\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n\n### Google PaLM\n\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n## Roadmap\n\nThis project is under active development.\n\nIf you\'d like to help, please see the [contributing guidelines](#contributing-to-aiconfig).\n\nPlease create issues for additional capabilities you\'d like to see.\n\nHere\'s what\'s already on our roadmap:\n\n- Evaluation interfaces: allow `aiconfig` artifacts to be evaluated with user-defined eval functions.\n  - We are also considering integrating with existing evaluation frameworks.\n- Local editor for `aiconfig`: enable you to interact with aiconfigs more intuitively.\n- OpenAI Assistants API support\n- Multi-modal ModelParsers:\n  - GPT4-V support\n  - DALLE-3\n  - Whisper\n  - HuggingFace image generation\n\n## FAQs\n\n### How should I edit an `aiconfig` file?\n\nEditing a configshould be done either programmatically via SDK or via the UI (workbooks):\n\n- [Programmatic](https://github.com/lastmile-ai/aiconfig/blob/main/cookbooks/Create-AIConfig-Programmatically/create_aiconfig_programmatically.ipynb) editing.\n\n- [Edit with a workbook](#edit-aiconfig-in-a-notebook-editor) editor: this is similar to editing an ipynb file as a notebook (most people never touch the json ipynb directly)\n\nYou should only edit the `aiconfig` by hand for minor modifications, like tweaking a prompt string or updating some metadata.\n\n### Does this support custom endpoints?\n\nOut of the box, AIConfig already supports all OpenAI GPT\\* models, Googles PaLM model and any textgeneration model on Hugging Face (like Mistral). See [Supported Models](#supported-models) for more details.\n\nAdditionally, you can install `aiconfig` [extensions](https://github.com/lastmile-ai/aiconfig/tree/main/extensions) for additional models (see question below).\n\n### Is OpenAI function calling supported?\n\nYes. [This example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI) goes through how to do it.\n\nWe are also working on adding support for the Assistants API.\n\n### How can I use aiconfig with my own model endpoint?\n\nModel support is implemented as ModelParsers in the AIConfig SDK, and the idea is that anyone, including you, can define a ModelParser (and even publish it as an extension package).\n\nAll thats needed to use a model with AIConfig is a ModelParser that knows\n\n- how to serialize data from a model into the aiconfig format\n- how to deserialize data from an aiconfig into the type the model expects\n- how to run inference for model.\n\nFor more details, see [Extensibility](https://aiconfig.lastmileai.dev/docs/extensibility).\n\n### When should I store outputs in an `aiconfig`?\n\nThe `AIConfigRuntime` object is used to interact with an aiconfig programmatically (see [SDK usage guide](#aiconfig-sdk)). As you run prompts, this object keeps track of the outputs returned from the model.\n\nYou can choose to serialize these outputs back into the `aiconfig` by using the `config.save(include_outputs=True)` API. This can be useful for preserving context -- think of it like session state.\n\nFor example, you can use aiconfig to create a chatbot, and use the same format to save the chat history so it can be resumed for the next session.\n\nYou can also choose to save outputs to a _different_ file than the original config -- `config.save("history.aiconfig.json", include_outputs=True)`.\n\n### Why should I use `aiconfig` instead of things like [configurator](https://pypi.org/project/configurator/)?\n\nIt helps to have a [standardized format](http://aiconfig.lastmileai.dev/docs/overview/ai-config-format) specifically for storing generative AI prompts, inference results, model parameters and arbitrary metadata, as opposed to a general-purpose configuration schema.\n\nWith that standardization, you just need a layer that knows how to serialize/deserialize from that format into whatever the inference endpoints require.\n\n### This looks similar to `ipynb` for Jupyter notebooks\n\nWe believe that notebooks are a perfect iteration environment for generative AI -- they are flexible, multi-modal, and collaborative.\n\nThe multi-modality and flexibility offered by notebooks and [`ipynb`](https://ipython.org/ipython-doc/3/notebook/nbformat.html) offers a good interaction model for generative AI. The `aiconfig` file format is extensible like `ipynb`, and AI Workbook editor allows rapid iteration in a notebook-like IDE.\n\n_AI Workbooks are to AIConfig what Jupyter notebooks are to `ipynb`_\n\nThere are 2 areas where we are going beyond what notebooks offer:\n\n1. `aiconfig` is more **source-control friendly** than `ipynb`. `ipynb` stores binary data (images, etc.) by encoding it in the file, while `aiconfig` recommends using file URI references instead.\n2. `aiconfig` can be imported and **connected to application code** using the AIConfig SDK.\n', 'repo_name': 'aiconfig'}} ts_ns=1702174862235185200
2023-12-09 21:21:26,744 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '<div align="center"><picture>\n  <img alt="aiconfig" src="aiconfig-docs/static/img/readme_logo.png" />\n</picture></div>\n<br/>\n\n![Python](https://github.com/lastmile-ai/aiconfig/actions/workflows/main_python.yml/badge.svg)\n![Node](https://github.com/lastmile-ai/aiconfig/actions/workflows/main-typescript.yml/badge.svg)\n![Docs](https://github.com/lastmile-ai/aiconfig/actions/workflows/test-deploy-docs.yml/badge.svg)\n[![Discord](<https://img.shields.io/badge/Discord-LastMile%20AI-Blue?color=rgb(37%2C%20150%2C%20190)>)](https://discord.gg/qMqgzDae)\n\n> Full documentation: **[aiconfig.lastmileai.dev](https://aiconfig.lastmileai.dev/)**\n\n## Overview\n\nAIConfig saves prompts, models and model parameters as source control friendly configs. This allows you to iterate on prompts and model parameters _separately from your application code_.\n\n1. **Prompts as configs**: a [standardized JSON format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to store generative AI model settings, prompt inputs/outputs, and flexible metadata.\n2. **Model-agnostic SDK**: Python & Node SDKs to use `aiconfig` in your application code. AIConfig is designed to be **model-agnostic** and **multi-modal**, so you can extend it to work with any generative AI model, including text, image and audio.\n3. **AI Workbook editor**: A [notebook-like playground](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to edit `aiconfig` files visually, run prompts, tweak models and model settings, and chain things together.\n\n### What problem it solves\n\nToday, application code is tightly coupled with the gen AI settings for the application -- prompts, parameters, and model-specific logic is all jumbled in with app code.\n\n- results in increased complexity\n- makes it hard to iterate on the prompts or try different models easily\n- makes it hard to evaluate prompt/model performance\n\nAIConfig helps unwind complexity by separating prompts, model parameters, and model-specific logic from your application.\n\n- simplifies application code -- simply call `config.run()`\n- open the `aiconfig` in a playground to iterate quickly\n- version control and evaluate the `aiconfig` - it\'s the AI artifact for your application.\n\n![AIConfig flow](aiconfig-docs/static/img/aiconfig_dataflow.png)\n\n### Quicknav\n\n<ul style="margin-bottom:0; padding-bottom:0;">\n  <li><a href="#install">Getting Started</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig">Create an AIConfig</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig">Run a prompt</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/parameters">Pass data into prompts</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain">Prompt Chains</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig">Callbacks and monitoring</a></li>\n  </ul>\n  <li><a href="#aiconfig-sdk">SDK Cheatsheet</a></li>\n  <li><a href="#cookbooks">Cookbooks and guides</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT">CLI Chatbot</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig">RAG with AIConfig</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing">Prompt routing</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI">OpenAI function calling</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification">Chain of Verification</a></li>\n  </ul>\n  <li><a href="#supported-models">Supported models</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama">LLaMA2 example</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace">Hugging Face (Mistral-7B) example</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency">PaLM</a></li>\n  </ul>\n  <li><a href="#extensibility">Extensibility</a></li>\n  <li><a href="#contributing-to-aiconfig">Contributing</a></li>\n  <li><a href="#roadmap">Roadmap</a></li>\n  <li><a href="#faqs">FAQ</a></li>\n</ul>\n\n## Features\n\n- [x] **Source-control friendly** [`aiconfig` format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to save prompts and model settings, which you can use for evaluation, reproducibility and simplifying your application code.\n- [x] **Multi-modal and model agnostic**. Use with any model, and serialize/deserialize data with the same `aiconfig` format.\n- [x] **Prompt chaining and parameterization** with [{{handlebars}}](https://handlebarsjs.com/) templating syntax, allowing you to pass dynamic data into prompts (as well as between prompts).\n- [x] **Streaming** supported out of the box, allowing you to get playground-like streaming wherever you use `aiconfig`.\n- [x] **Notebook editor**. [AI Workbooks editor](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to visually create your `aiconfig`, and use the SDK to connect it to your application code.\n\n## Install\n\nInstall with your favorite package manager for Node or Python.\n\n### Node.js\n\n#### `npm` or `yarn`\n\n```bash\nnpm install aiconfig\n```\n\n```bash\nyarn add aiconfig\n```\n\n### Python\n\n#### `pip3` or `poetry`\n\n```bash\npip3 install python-aiconfig\n```\n\n```bash\npoetry add python-aiconfig\n```\n\n[Detailed installation instructions](https://aiconfig.lastmileai.dev/docs/getting-started/#installation).\n\n### Set your OpenAI API Key\n\n> **Note**: Make sure to specify the API keys (such as [`OPENAI_API_KEY`](https://platform.openai.com/api-keys)) in your environment before proceeding.\n\nIn your CLI, set the environment variable:\n\n```bash\nexport OPENAI_API_KEY=my_key\n```\n\n## Getting Started\n\n> We cover Python instructions here, for Node.js please see the [detailed Getting Started guide](https://aiconfig.lastmileai.dev/docs/getting-started)\n\nIn this quickstart, you will create a customizable NYC travel itinerary using `aiconfig`.\n\nThis AIConfig contains a prompt chain to get a list of travel activities from an LLM and then generate an itinerary in an order specified by the user.\n\n> **Link to tutorial code: [here](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Getting-Started)**\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/805173d1-0f83-44c5-b570-c776bb7dba66\n\n### Download `travel.aiconfig.json`\n\n> **Note**: Don\'t worry if you don\'t understand all the pieces of this yet, we\'ll go over it step by step.\n\n```json\n{\n  "name": "NYC Trip Planner",\n  "description": "Intrepid explorer with ChatGPT and AIConfig",\n  "schema_version": "latest",\n  "metadata": {\n    "models": {\n      "gpt-3.5-turbo": {\n        "model": "gpt-3.5-turbo",\n        "top_p": 1,\n        "temperature": 1\n      },\n      "gpt-4": {\n        "model": "gpt-4",\n        "max_tokens": 3000,\n        "system_prompt": "You are an expert travel coordinator with exquisite taste."\n      }\n    },\n    "default_model": "gpt-3.5-turbo"\n  },\n  "prompts": [\n    {\n      "name": "get_activities",\n      "input": "Tell me 10 fun attractions to do in NYC."\n    },\n    {\n      "name": "gen_itinerary",\n      "input": "Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}.",\n      "metadata": {\n        "model": "gpt-4",\n        "parameters": {\n          "order_by": "geographic location"\n        }\n      }\n    }\n  ]\n}\n```\n\n### Run the `get_activities` prompt.\n\nYou don\'t need to worry about how to run inference for the model; it\'s all handled by AIConfig. The prompt runs with gpt-3.5-turbo since that is the `default_model` for this AIConfig.\n\nCreate a new file called `app.py` and and enter the following code:\n\n```python\nimport asyncio\nfrom aiconfig import AIConfigRuntime, InferenceOptions\n\nasync def main():\n  # Load the aiconfig\n  config = AIConfigRuntime.load(\'travel.aiconfig.json\')\n\n  # Run a single prompt (with streaming)\n  inference_options = InferenceOptions(stream=True)\n  await config.run("get_activities", options=inference_options)\n\nasyncio.run(main())\n```\n\nNow run this in your terminal with the command:\n\n```bash\npython3 app.py\n```\n\n### Run the `gen_itinerary` prompt.\n\nIn your `app.py` file, change the last line to below:\n\n```python\nawait config.run("gen_itinerary", params=None, options=inference_options)\n```\n\nRe-run the command in your terminal:\n\n```bash\npython3 app.py\n```\n\nThis prompt depends on the output of `get_activities`. It also takes in parameters (user input) to determine the customized itinerary.\n\nLet\'s take a closer look:\n\n**`gen_itinerary` prompt:**\n\n```\n"Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}."\n```\n\n**prompt metadata:**\n\n```json\n{\n  "metadata": {\n    "model": "gpt-4",\n    "parameters": {\n      "order_by": "geographic location"\n    }\n  }\n}\n```\n\nObserve the following:\n\n1. The prompt depends on the output of the `get_activities` prompt.\n2. It also depends on an `order_by` parameter (using {{handlebars}} syntax)\n3. It uses **gpt-4**, whereas the `get_activities` prompt it depends on uses **gpt-3.5-turbo**.\n\n> Effectively, this is a prompt chain between `gen_itinerary` and `get_activities` prompts, _as well as_ as a model chain between **gpt-3.5-turbo** and **gpt-4**.\n\nLet\'s run this with AIConfig:\n\nReplace `config.run` above with this:\n\n```python\nawait config.run("gen_itinerary", params={"order_by": "duration"}, options=inference_options, run_with_dependencies=True)\n```\n\nNotice how simple the syntax is to perform a fairly complex task - running 2 different prompts across 2 different models and chaining one\'s output as part of the input of another.\n\nThe code will just run `get_activities`, then pipe its output as an input to `gen_itinerary`, and finally run `gen_itinerary`.\n\n### Save the AIConfig\n\nLet\'s save the AIConfig back to disk, and serialize the outputs from the latest inference run as well:\n\n```python\n# Save the aiconfig to disk. and serialize outputs from the model run\nconfig.save(\'updated.aiconfig.json\', include_outputs=True)\n```\n\n### Edit `aiconfig` in a notebook editor\n\nWe can iterate on an `aiconfig` using a notebook-like editor called an **AI Workbook**. Now that we have an `aiconfig` file artifact that encapsulates the generative AI part of our application, we can iterate on it separately from the application code that uses it.\n\n1. Go to https://lastmileai.dev.\n2. Go to Workbooks page: https://lastmileai.dev/workbooks\n3. Click dropdown from \'+ New Workbook\' and select \'Create from AIConfig\'\n4. Upload `travel.aiconfig.json`\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/5d901493-bbda-4f8e-93c7-dd9a91bf242e\n\nTry out the workbook playground here: **[NYC Travel Workbook](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9)**\n\n> **We are working on a local editor that you can run yourself. For now, please use the hosted version on https://lastmileai.dev.**\n\n### Additional Guides\n\nThere is a lot you can do with `aiconfig`. We have several other tutorials to help get you started:\n\n- [Create an AIConfig from scratch](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig)\n- [Run a prompt](https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig)\n- [Pass data into prompts](https://aiconfig.lastmileai.dev/docs/overview/parameters)\n- [Prompt chains](https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain)\n- [Callbacks and monitoring](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\nHere are some example uses:\n\n- [CLI Chatbot](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT)\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [Chain of thought](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### OpenAI Introspection API\n\nIf you are already using OpenAI completion API\'s in your application, you can get started very quickly to start saving the messages in an `aiconfig`.\n\nUsage: see openai_wrapper.ipynb.\n\nNow you can continue using `openai` completion API as normal. When you want to save the config, just call `new_config.save()` and all your openai completion calls will get serialized to disk.\n\n> [**Detailed guide here**](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig#openai-api-python-wrapper)\n\n## Supported Models\n\nAIConfig supports the following models out of the box:\n\n- OpenAI chat models (GPT-3, GPT-3.5, GPT-4)\n- LLaMA2 (running locally)\n- Google PaLM models (PaLM chat)\n- Hugging Face text generation models (e.g. Mistral-7B)\n\n### Examples\n\n- [OpenAI](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n> If you need to use a model that isn\'t provided out of the box, you can implement a `ModelParser` for it (see [Extending AIConfig](#extending-aiconfig)). **We welcome [contributions](https://aiconfig.lastmileai.dev/docs/contributing)**\n\n## AIConfig Schema\n\n[AIConfig specification](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format)\n\n## AIConfig SDK\n\n> Read the [Usage Guide](https://aiconfig.lastmileai.dev/docs/usage-guide) for more details.\n\nThe AIConfig SDK supports CRUD operations for prompts, models, parameters and metadata. Here are some common examples.\n\nThe root interface is the `AIConfigRuntime` object. That is the entrypoint for interacting with an AIConfig programmatically.\n\nLet\'s go over a few key CRUD operations to give a glimpse.\n\n### AIConfig `create`\n\n```python\nconfig = AIConfigRuntime.create("aiconfig name", "description")\n```\n\n### Prompt `resolve`\n\n`resolve` deserializes an existing `Prompt` into the data object that its model expects.\n\n```python\nconfig.resolve("prompt_name", params)\n```\n\n`params` are overrides you can specify to resolve any `{{handlebars}}` templates in the prompt. See the `gen_itinerary` prompt in the Getting Started example.\n\n### Prompt `serialize`\n\n`serialize` is the inverse of `resolve` -- it serializes the data object that a model understands into a `Prompt` object that can be serialized into the `aiconfig` format.\n\n```python\nconfig.serialize("model_name", data, "prompt_name")\n```\n\n### Prompt `run`\n\n`run` is used to run inference for the specified `Prompt`.\n\n```python\nconfig.run("prompt_name", params)\n```\n\n### `run_with_dependencies`\n\nThis is a variant of `run` -- this re-runs all prompt dependencies.\nFor example, in [`travel.aiconfig.json`](#download-travelaiconfigjson), the `gen_itinerary` prompt references the output of the `get_activities` prompt using `{{get_activities.output}}`.\n\nRunning this function will first execute `get_activities`, and use its output to resolve the `gen_itinerary` prompt before executing it.\nThis is transitive, so it computes the Directed Acyclic Graph of dependencies to execute. Complex relationships can be modeled this way.\n\n```python\nconfig.run_with_dependencies("gen_itinerary")\n```\n\n### Updating metadata and parameters\n\nUse the `get/set_metadata` and `get/set_parameter` methods to interact with metadata and parameters (`set_parameter` is just syntactic sugar to update `"metadata.parameters"`)\n\n```python\nconfig.set_metadata("key", data, "prompt_name")\n```\n\nNote: if `"prompt_name"` is specified, the metadata is updated specifically for that prompt. Otherwise, the global metadata is updated.\n\n### Register new `ModelParser`\n\nUse the `AIConfigRuntime.register_model_parser` if you want to use a different `ModelParser`, or configure AIConfig to work with an additional model.\n\nAIConfig uses the model name string to retrieve the right `ModelParser` for a given Prompt (see `AIConfigRuntime.get_model_parser`), so you can register a different ModelParser for the same ID to override which `ModelParser` handles a Prompt.\n\nFor example, suppose I want to use `MyOpenAIModelParser` to handle `gpt-4` prompts. I can do the following at the start of my application:\n\n```python\nAIConfigRuntime.register_model_parser(myModelParserInstance, ["gpt-4"])\n```\n\n### Callback events\n\nUse callback events to trace and monitor what\'s going on -- helpful for debugging and observability.\n\n```python\nfrom aiconfig import AIConfigRuntime, CallbackEvent, CallbackManager\nconfig = AIConfigRuntime.load(\'aiconfig.json\')\n\nasync def my_custom_callback(event: CallbackEvent) -> None:\n  print(f"Event triggered: {event.name}", event)\n\ncallback_manager = CallbackManager([my_custom_callback])\nconfig.set_callback_manager(callback_manager)\n\nawait config.run("prompt_name")\n```\n\n[**Read more** here](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\n## Extensibility\n\nAIConfig is designed to be customized and extended for your use-case. The [Extensibility](/docs/extensibility) guide goes into more detail.\n\nCurrently, there are 3 core ways to extend AIConfig:\n\n1. [Supporting other models](https://aiconfig.lastmileai.dev/docs/extensibility#1-bring-your-own-model) - define a ModelParser extension\n2. [Callback event handlers](https://aiconfig.lastmileai.dev/docs/extensibility#2-callback-handlers) - tracing and monitoring\n3. [Custom metadata](https://aiconfig.lastmileai.dev/docs/extensibility#3-custom-metadata) - save custom fields in `aiconfig`\n\n## Contributing to `aiconfig`\n\nThis is our first open-source project and we\'d love your help.\n\nSee our [contributing guidelines](https://aiconfig.lastmileai.dev/docs/contributing) -- we would especially love help adding support for additional models that the community wants.\n\n## Cookbooks\n\nWe provide several guides to demonstrate the power of `aiconfig`.\n\n> **See the [`cookbooks`](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks) folder for examples to clone.**\n\n### Chatbot\n\n- [Wizard GPT](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT) - speak to a wizard on your CLI\n\n- [CLI-mate](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Cli-Mate) - help you make code-mods interactively on your codebase.\n\n### Retrieval Augmented Generated (RAG)\n\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n\nAt its core, RAG is about passing data into prompts. Read how to [pass data](/docs/overview/parameters) with AIConfig.\n\n### Function calling\n\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n\n### Prompt routing\n\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n\n### Chain of Thought\n\nA variant of chain-of-thought is Chain of Verification, used to help reduce hallucinations. Check out the aiconfig cookbook for CoVe:\n\n- [Chain of Verification](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### Using local LLaMA2 with `aiconfig`\n\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n\n### Hugging Face text generation\n\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n\n### Google PaLM\n\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n## Roadmap\n\nThis project is under active development.\n\nIf you\'d like to help, please see the [contributing guidelines](#contributing-to-aiconfig).\n\nPlease create issues for additional capabilities you\'d like to see.\n\nHere\'s what\'s already on our roadmap:\n\n- Evaluation interfaces: allow `aiconfig` artifacts to be evaluated with user-defined eval functions.\n  - We are also considering integrating with existing evaluation frameworks.\n- Local editor for `aiconfig`: enable you to interact with aiconfigs more intuitively.\n- OpenAI Assistants API support\n- Multi-modal ModelParsers:\n  - GPT4-V support\n  - DALLE-3\n  - Whisper\n  - HuggingFace image generation\n\n## FAQs\n\n### How should I edit an `aiconfig` file?\n\nEditing a configshould be done either programmatically via SDK or via the UI (workbooks):\n\n- [Programmatic](https://github.com/lastmile-ai/aiconfig/blob/main/cookbooks/Create-AIConfig-Programmatically/create_aiconfig_programmatically.ipynb) editing.\n\n- [Edit with a workbook](#edit-aiconfig-in-a-notebook-editor) editor: this is similar to editing an ipynb file as a notebook (most people never touch the json ipynb directly)\n\nYou should only edit the `aiconfig` by hand for minor modifications, like tweaking a prompt string or updating some metadata.\n\n### Does this support custom endpoints?\n\nOut of the box, AIConfig already supports all OpenAI GPT\\* models, Googles PaLM model and any textgeneration model on Hugging Face (like Mistral). See [Supported Models](#supported-models) for more details.\n\nAdditionally, you can install `aiconfig` [extensions](https://github.com/lastmile-ai/aiconfig/tree/main/extensions) for additional models (see question below).\n\n### Is OpenAI function calling supported?\n\nYes. [This example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI) goes through how to do it.\n\nWe are also working on adding support for the Assistants API.\n\n### How can I use aiconfig with my own model endpoint?\n\nModel support is implemented as ModelParsers in the AIConfig SDK, and the idea is that anyone, including you, can define a ModelParser (and even publish it as an extension package).\n\nAll thats needed to use a model with AIConfig is a ModelParser that knows\n\n- how to serialize data from a model into the aiconfig format\n- how to deserialize data from an aiconfig into the type the model expects\n- how to run inference for model.\n\nFor more details, see [Extensibility](https://aiconfig.lastmileai.dev/docs/extensibility).\n\n### When should I store outputs in an `aiconfig`?\n\nThe `AIConfigRuntime` object is used to interact with an aiconfig programmatically (see [SDK usage guide](#aiconfig-sdk)). As you run prompts, this object keeps track of the outputs returned from the model.\n\nYou can choose to serialize these outputs back into the `aiconfig` by using the `config.save(include_outputs=True)` API. This can be useful for preserving context -- think of it like session state.\n\nFor example, you can use aiconfig to create a chatbot, and use the same format to save the chat history so it can be resumed for the next session.\n\nYou can also choose to save outputs to a _different_ file than the original config -- `config.save("history.aiconfig.json", include_outputs=True)`.\n\n### Why should I use `aiconfig` instead of things like [configurator](https://pypi.org/project/configurator/)?\n\nIt helps to have a [standardized format](http://aiconfig.lastmileai.dev/docs/overview/ai-config-format) specifically for storing generative AI prompts, inference results, model parameters and arbitrary metadata, as opposed to a general-purpose configuration schema.\n\nWith that standardization, you just need a layer that knows how to serialize/deserialize from that format into whatever the inference endpoints require.\n\n### This looks similar to `ipynb` for Jupyter notebooks\n\nWe believe that notebooks are a perfect iteration environment for generative AI -- they are flexible, multi-modal, and collaborative.\n\nThe multi-modality and flexibility offered by notebooks and [`ipynb`](https://ipython.org/ipython-doc/3/notebook/nbformat.html) offers a good interaction model for generative AI. The `aiconfig` file format is extensible like `ipynb`, and AI Workbook editor allows rapid iteration in a notebook-like IDE.\n\n_AI Workbooks are to AIConfig what Jupyter notebooks are to `ipynb`_\n\nThere are 2 areas where we are going beyond what notebooks offer:\n\n1. `aiconfig` is more **source-control friendly** than `ipynb`. `ipynb` stores binary data (images, etc.) by encoding it in the file, while `aiconfig` recommends using file URI references instead.\n2. `aiconfig` can be imported and **connected to application code** using the AIConfig SDK.\n', 'repo_name': 'aiconfig'}} ts_ns=1702174862235185200
2023-12-09 21:21:26,744 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '<div align="center"><picture>\n  <img alt="aiconfig" src="aiconfig-docs/static/img/readme_logo.png" />\n</picture></div>\n<br/>\n\n![Python](https://github.com/lastmile-ai/aiconfig/actions/workflows/main_python.yml/badge.svg)\n![Node](https://github.com/lastmile-ai/aiconfig/actions/workflows/main-typescript.yml/badge.svg)\n![Docs](https://github.com/lastmile-ai/aiconfig/actions/workflows/test-deploy-docs.yml/badge.svg)\n[![Discord](<https://img.shields.io/badge/Discord-LastMile%20AI-Blue?color=rgb(37%2C%20150%2C%20190)>)](https://discord.gg/qMqgzDae)\n\n> Full documentation: **[aiconfig.lastmileai.dev](https://aiconfig.lastmileai.dev/)**\n\n## Overview\n\nAIConfig saves prompts, models and model parameters as source control friendly configs. This allows you to iterate on prompts and model parameters _separately from your application code_.\n\n1. **Prompts as configs**: a [standardized JSON format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to store generative AI model settings, prompt inputs/outputs, and flexible metadata.\n2. **Model-agnostic SDK**: Python & Node SDKs to use `aiconfig` in your application code. AIConfig is designed to be **model-agnostic** and **multi-modal**, so you can extend it to work with any generative AI model, including text, image and audio.\n3. **AI Workbook editor**: A [notebook-like playground](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to edit `aiconfig` files visually, run prompts, tweak models and model settings, and chain things together.\n\n### What problem it solves\n\nToday, application code is tightly coupled with the gen AI settings for the application -- prompts, parameters, and model-specific logic is all jumbled in with app code.\n\n- results in increased complexity\n- makes it hard to iterate on the prompts or try different models easily\n- makes it hard to evaluate prompt/model performance\n\nAIConfig helps unwind complexity by separating prompts, model parameters, and model-specific logic from your application.\n\n- simplifies application code -- simply call `config.run()`\n- open the `aiconfig` in a playground to iterate quickly\n- version control and evaluate the `aiconfig` - it\'s the AI artifact for your application.\n\n![AIConfig flow](aiconfig-docs/static/img/aiconfig_dataflow.png)\n\n### Quicknav\n\n<ul style="margin-bottom:0; padding-bottom:0;">\n  <li><a href="#install">Getting Started</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig">Create an AIConfig</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig">Run a prompt</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/parameters">Pass data into prompts</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain">Prompt Chains</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig">Callbacks and monitoring</a></li>\n  </ul>\n  <li><a href="#aiconfig-sdk">SDK Cheatsheet</a></li>\n  <li><a href="#cookbooks">Cookbooks and guides</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT">CLI Chatbot</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig">RAG with AIConfig</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing">Prompt routing</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI">OpenAI function calling</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification">Chain of Verification</a></li>\n  </ul>\n  <li><a href="#supported-models">Supported models</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama">LLaMA2 example</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace">Hugging Face (Mistral-7B) example</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency">PaLM</a></li>\n  </ul>\n  <li><a href="#extensibility">Extensibility</a></li>\n  <li><a href="#contributing-to-aiconfig">Contributing</a></li>\n  <li><a href="#roadmap">Roadmap</a></li>\n  <li><a href="#faqs">FAQ</a></li>\n</ul>\n\n## Features\n\n- [x] **Source-control friendly** [`aiconfig` format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to save prompts and model settings, which you can use for evaluation, reproducibility and simplifying your application code.\n- [x] **Multi-modal and model agnostic**. Use with any model, and serialize/deserialize data with the same `aiconfig` format.\n- [x] **Prompt chaining and parameterization** with [{{handlebars}}](https://handlebarsjs.com/) templating syntax, allowing you to pass dynamic data into prompts (as well as between prompts).\n- [x] **Streaming** supported out of the box, allowing you to get playground-like streaming wherever you use `aiconfig`.\n- [x] **Notebook editor**. [AI Workbooks editor](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to visually create your `aiconfig`, and use the SDK to connect it to your application code.\n\n## Install\n\nInstall with your favorite package manager for Node or Python.\n\n### Node.js\n\n#### `npm` or `yarn`\n\n```bash\nnpm install aiconfig\n```\n\n```bash\nyarn add aiconfig\n```\n\n### Python\n\n#### `pip3` or `poetry`\n\n```bash\npip3 install python-aiconfig\n```\n\n```bash\npoetry add python-aiconfig\n```\n\n[Detailed installation instructions](https://aiconfig.lastmileai.dev/docs/getting-started/#installation).\n\n### Set your OpenAI API Key\n\n> **Note**: Make sure to specify the API keys (such as [`OPENAI_API_KEY`](https://platform.openai.com/api-keys)) in your environment before proceeding.\n\nIn your CLI, set the environment variable:\n\n```bash\nexport OPENAI_API_KEY=my_key\n```\n\n## Getting Started\n\n> We cover Python instructions here, for Node.js please see the [detailed Getting Started guide](https://aiconfig.lastmileai.dev/docs/getting-started)\n\nIn this quickstart, you will create a customizable NYC travel itinerary using `aiconfig`.\n\nThis AIConfig contains a prompt chain to get a list of travel activities from an LLM and then generate an itinerary in an order specified by the user.\n\n> **Link to tutorial code: [here](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Getting-Started)**\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/805173d1-0f83-44c5-b570-c776bb7dba66\n\n### Download `travel.aiconfig.json`\n\n> **Note**: Don\'t worry if you don\'t understand all the pieces of this yet, we\'ll go over it step by step.\n\n```json\n{\n  "name": "NYC Trip Planner",\n  "description": "Intrepid explorer with ChatGPT and AIConfig",\n  "schema_version": "latest",\n  "metadata": {\n    "models": {\n      "gpt-3.5-turbo": {\n        "model": "gpt-3.5-turbo",\n        "top_p": 1,\n        "temperature": 1\n      },\n      "gpt-4": {\n        "model": "gpt-4",\n        "max_tokens": 3000,\n        "system_prompt": "You are an expert travel coordinator with exquisite taste."\n      }\n    },\n    "default_model": "gpt-3.5-turbo"\n  },\n  "prompts": [\n    {\n      "name": "get_activities",\n      "input": "Tell me 10 fun attractions to do in NYC."\n    },\n    {\n      "name": "gen_itinerary",\n      "input": "Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}.",\n      "metadata": {\n        "model": "gpt-4",\n        "parameters": {\n          "order_by": "geographic location"\n        }\n      }\n    }\n  ]\n}\n```\n\n### Run the `get_activities` prompt.\n\nYou don\'t need to worry about how to run inference for the model; it\'s all handled by AIConfig. The prompt runs with gpt-3.5-turbo since that is the `default_model` for this AIConfig.\n\nCreate a new file called `app.py` and and enter the following code:\n\n```python\nimport asyncio\nfrom aiconfig import AIConfigRuntime, InferenceOptions\n\nasync def main():\n  # Load the aiconfig\n  config = AIConfigRuntime.load(\'travel.aiconfig.json\')\n\n  # Run a single prompt (with streaming)\n  inference_options = InferenceOptions(stream=True)\n  await config.run("get_activities", options=inference_options)\n\nasyncio.run(main())\n```\n\nNow run this in your terminal with the command:\n\n```bash\npython3 app.py\n```\n\n### Run the `gen_itinerary` prompt.\n\nIn your `app.py` file, change the last line to below:\n\n```python\nawait config.run("gen_itinerary", params=None, options=inference_options)\n```\n\nRe-run the command in your terminal:\n\n```bash\npython3 app.py\n```\n\nThis prompt depends on the output of `get_activities`. It also takes in parameters (user input) to determine the customized itinerary.\n\nLet\'s take a closer look:\n\n**`gen_itinerary` prompt:**\n\n```\n"Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}."\n```\n\n**prompt metadata:**\n\n```json\n{\n  "metadata": {\n    "model": "gpt-4",\n    "parameters": {\n      "order_by": "geographic location"\n    }\n  }\n}\n```\n\nObserve the following:\n\n1. The prompt depends on the output of the `get_activities` prompt.\n2. It also depends on an `order_by` parameter (using {{handlebars}} syntax)\n3. It uses **gpt-4**, whereas the `get_activities` prompt it depends on uses **gpt-3.5-turbo**.\n\n> Effectively, this is a prompt chain between `gen_itinerary` and `get_activities` prompts, _as well as_ as a model chain between **gpt-3.5-turbo** and **gpt-4**.\n\nLet\'s run this with AIConfig:\n\nReplace `config.run` above with this:\n\n```python\nawait config.run("gen_itinerary", params={"order_by": "duration"}, options=inference_options, run_with_dependencies=True)\n```\n\nNotice how simple the syntax is to perform a fairly complex task - running 2 different prompts across 2 different models and chaining one\'s output as part of the input of another.\n\nThe code will just run `get_activities`, then pipe its output as an input to `gen_itinerary`, and finally run `gen_itinerary`.\n\n### Save the AIConfig\n\nLet\'s save the AIConfig back to disk, and serialize the outputs from the latest inference run as well:\n\n```python\n# Save the aiconfig to disk. and serialize outputs from the model run\nconfig.save(\'updated.aiconfig.json\', include_outputs=True)\n```\n\n### Edit `aiconfig` in a notebook editor\n\nWe can iterate on an `aiconfig` using a notebook-like editor called an **AI Workbook**. Now that we have an `aiconfig` file artifact that encapsulates the generative AI part of our application, we can iterate on it separately from the application code that uses it.\n\n1. Go to https://lastmileai.dev.\n2. Go to Workbooks page: https://lastmileai.dev/workbooks\n3. Click dropdown from \'+ New Workbook\' and select \'Create from AIConfig\'\n4. Upload `travel.aiconfig.json`\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/5d901493-bbda-4f8e-93c7-dd9a91bf242e\n\nTry out the workbook playground here: **[NYC Travel Workbook](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9)**\n\n> **We are working on a local editor that you can run yourself. For now, please use the hosted version on https://lastmileai.dev.**\n\n### Additional Guides\n\nThere is a lot you can do with `aiconfig`. We have several other tutorials to help get you started:\n\n- [Create an AIConfig from scratch](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig)\n- [Run a prompt](https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig)\n- [Pass data into prompts](https://aiconfig.lastmileai.dev/docs/overview/parameters)\n- [Prompt chains](https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain)\n- [Callbacks and monitoring](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\nHere are some example uses:\n\n- [CLI Chatbot](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT)\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [Chain of thought](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### OpenAI Introspection API\n\nIf you are already using OpenAI completion API\'s in your application, you can get started very quickly to start saving the messages in an `aiconfig`.\n\nUsage: see openai_wrapper.ipynb.\n\nNow you can continue using `openai` completion API as normal. When you want to save the config, just call `new_config.save()` and all your openai completion calls will get serialized to disk.\n\n> [**Detailed guide here**](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig#openai-api-python-wrapper)\n\n## Supported Models\n\nAIConfig supports the following models out of the box:\n\n- OpenAI chat models (GPT-3, GPT-3.5, GPT-4)\n- LLaMA2 (running locally)\n- Google PaLM models (PaLM chat)\n- Hugging Face text generation models (e.g. Mistral-7B)\n\n### Examples\n\n- [OpenAI](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n> If you need to use a model that isn\'t provided out of the box, you can implement a `ModelParser` for it (see [Extending AIConfig](#extending-aiconfig)). **We welcome [contributions](https://aiconfig.lastmileai.dev/docs/contributing)**\n\n## AIConfig Schema\n\n[AIConfig specification](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format)\n\n## AIConfig SDK\n\n> Read the [Usage Guide](https://aiconfig.lastmileai.dev/docs/usage-guide) for more details.\n\nThe AIConfig SDK supports CRUD operations for prompts, models, parameters and metadata. Here are some common examples.\n\nThe root interface is the `AIConfigRuntime` object. That is the entrypoint for interacting with an AIConfig programmatically.\n\nLet\'s go over a few key CRUD operations to give a glimpse.\n\n### AIConfig `create`\n\n```python\nconfig = AIConfigRuntime.create("aiconfig name", "description")\n```\n\n### Prompt `resolve`\n\n`resolve` deserializes an existing `Prompt` into the data object that its model expects.\n\n```python\nconfig.resolve("prompt_name", params)\n```\n\n`params` are overrides you can specify to resolve any `{{handlebars}}` templates in the prompt. See the `gen_itinerary` prompt in the Getting Started example.\n\n### Prompt `serialize`\n\n`serialize` is the inverse of `resolve` -- it serializes the data object that a model understands into a `Prompt` object that can be serialized into the `aiconfig` format.\n\n```python\nconfig.serialize("model_name", data, "prompt_name")\n```\n\n### Prompt `run`\n\n`run` is used to run inference for the specified `Prompt`.\n\n```python\nconfig.run("prompt_name", params)\n```\n\n### `run_with_dependencies`\n\nThis is a variant of `run` -- this re-runs all prompt dependencies.\nFor example, in [`travel.aiconfig.json`](#download-travelaiconfigjson), the `gen_itinerary` prompt references the output of the `get_activities` prompt using `{{get_activities.output}}`.\n\nRunning this function will first execute `get_activities`, and use its output to resolve the `gen_itinerary` prompt before executing it.\nThis is transitive, so it computes the Directed Acyclic Graph of dependencies to execute. Complex relationships can be modeled this way.\n\n```python\nconfig.run_with_dependencies("gen_itinerary")\n```\n\n### Updating metadata and parameters\n\nUse the `get/set_metadata` and `get/set_parameter` methods to interact with metadata and parameters (`set_parameter` is just syntactic sugar to update `"metadata.parameters"`)\n\n```python\nconfig.set_metadata("key", data, "prompt_name")\n```\n\nNote: if `"prompt_name"` is specified, the metadata is updated specifically for that prompt. Otherwise, the global metadata is updated.\n\n### Register new `ModelParser`\n\nUse the `AIConfigRuntime.register_model_parser` if you want to use a different `ModelParser`, or configure AIConfig to work with an additional model.\n\nAIConfig uses the model name string to retrieve the right `ModelParser` for a given Prompt (see `AIConfigRuntime.get_model_parser`), so you can register a different ModelParser for the same ID to override which `ModelParser` handles a Prompt.\n\nFor example, suppose I want to use `MyOpenAIModelParser` to handle `gpt-4` prompts. I can do the following at the start of my application:\n\n```python\nAIConfigRuntime.register_model_parser(myModelParserInstance, ["gpt-4"])\n```\n\n### Callback events\n\nUse callback events to trace and monitor what\'s going on -- helpful for debugging and observability.\n\n```python\nfrom aiconfig import AIConfigRuntime, CallbackEvent, CallbackManager\nconfig = AIConfigRuntime.load(\'aiconfig.json\')\n\nasync def my_custom_callback(event: CallbackEvent) -> None:\n  print(f"Event triggered: {event.name}", event)\n\ncallback_manager = CallbackManager([my_custom_callback])\nconfig.set_callback_manager(callback_manager)\n\nawait config.run("prompt_name")\n```\n\n[**Read more** here](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\n## Extensibility\n\nAIConfig is designed to be customized and extended for your use-case. The [Extensibility](/docs/extensibility) guide goes into more detail.\n\nCurrently, there are 3 core ways to extend AIConfig:\n\n1. [Supporting other models](https://aiconfig.lastmileai.dev/docs/extensibility#1-bring-your-own-model) - define a ModelParser extension\n2. [Callback event handlers](https://aiconfig.lastmileai.dev/docs/extensibility#2-callback-handlers) - tracing and monitoring\n3. [Custom metadata](https://aiconfig.lastmileai.dev/docs/extensibility#3-custom-metadata) - save custom fields in `aiconfig`\n\n## Contributing to `aiconfig`\n\nThis is our first open-source project and we\'d love your help.\n\nSee our [contributing guidelines](https://aiconfig.lastmileai.dev/docs/contributing) -- we would especially love help adding support for additional models that the community wants.\n\n## Cookbooks\n\nWe provide several guides to demonstrate the power of `aiconfig`.\n\n> **See the [`cookbooks`](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks) folder for examples to clone.**\n\n### Chatbot\n\n- [Wizard GPT](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT) - speak to a wizard on your CLI\n\n- [CLI-mate](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Cli-Mate) - help you make code-mods interactively on your codebase.\n\n### Retrieval Augmented Generated (RAG)\n\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n\nAt its core, RAG is about passing data into prompts. Read how to [pass data](/docs/overview/parameters) with AIConfig.\n\n### Function calling\n\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n\n### Prompt routing\n\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n\n### Chain of Thought\n\nA variant of chain-of-thought is Chain of Verification, used to help reduce hallucinations. Check out the aiconfig cookbook for CoVe:\n\n- [Chain of Verification](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### Using local LLaMA2 with `aiconfig`\n\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n\n### Hugging Face text generation\n\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n\n### Google PaLM\n\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n## Roadmap\n\nThis project is under active development.\n\nIf you\'d like to help, please see the [contributing guidelines](#contributing-to-aiconfig).\n\nPlease create issues for additional capabilities you\'d like to see.\n\nHere\'s what\'s already on our roadmap:\n\n- Evaluation interfaces: allow `aiconfig` artifacts to be evaluated with user-defined eval functions.\n  - We are also considering integrating with existing evaluation frameworks.\n- Local editor for `aiconfig`: enable you to interact with aiconfigs more intuitively.\n- OpenAI Assistants API support\n- Multi-modal ModelParsers:\n  - GPT4-V support\n  - DALLE-3\n  - Whisper\n  - HuggingFace image generation\n\n## FAQs\n\n### How should I edit an `aiconfig` file?\n\nEditing a configshould be done either programmatically via SDK or via the UI (workbooks):\n\n- [Programmatic](https://github.com/lastmile-ai/aiconfig/blob/main/cookbooks/Create-AIConfig-Programmatically/create_aiconfig_programmatically.ipynb) editing.\n\n- [Edit with a workbook](#edit-aiconfig-in-a-notebook-editor) editor: this is similar to editing an ipynb file as a notebook (most people never touch the json ipynb directly)\n\nYou should only edit the `aiconfig` by hand for minor modifications, like tweaking a prompt string or updating some metadata.\n\n### Does this support custom endpoints?\n\nOut of the box, AIConfig already supports all OpenAI GPT\\* models, Googles PaLM model and any textgeneration model on Hugging Face (like Mistral). See [Supported Models](#supported-models) for more details.\n\nAdditionally, you can install `aiconfig` [extensions](https://github.com/lastmile-ai/aiconfig/tree/main/extensions) for additional models (see question below).\n\n### Is OpenAI function calling supported?\n\nYes. [This example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI) goes through how to do it.\n\nWe are also working on adding support for the Assistants API.\n\n### How can I use aiconfig with my own model endpoint?\n\nModel support is implemented as ModelParsers in the AIConfig SDK, and the idea is that anyone, including you, can define a ModelParser (and even publish it as an extension package).\n\nAll thats needed to use a model with AIConfig is a ModelParser that knows\n\n- how to serialize data from a model into the aiconfig format\n- how to deserialize data from an aiconfig into the type the model expects\n- how to run inference for model.\n\nFor more details, see [Extensibility](https://aiconfig.lastmileai.dev/docs/extensibility).\n\n### When should I store outputs in an `aiconfig`?\n\nThe `AIConfigRuntime` object is used to interact with an aiconfig programmatically (see [SDK usage guide](#aiconfig-sdk)). As you run prompts, this object keeps track of the outputs returned from the model.\n\nYou can choose to serialize these outputs back into the `aiconfig` by using the `config.save(include_outputs=True)` API. This can be useful for preserving context -- think of it like session state.\n\nFor example, you can use aiconfig to create a chatbot, and use the same format to save the chat history so it can be resumed for the next session.\n\nYou can also choose to save outputs to a _different_ file than the original config -- `config.save("history.aiconfig.json", include_outputs=True)`.\n\n### Why should I use `aiconfig` instead of things like [configurator](https://pypi.org/project/configurator/)?\n\nIt helps to have a [standardized format](http://aiconfig.lastmileai.dev/docs/overview/ai-config-format) specifically for storing generative AI prompts, inference results, model parameters and arbitrary metadata, as opposed to a general-purpose configuration schema.\n\nWith that standardization, you just need a layer that knows how to serialize/deserialize from that format into whatever the inference endpoints require.\n\n### This looks similar to `ipynb` for Jupyter notebooks\n\nWe believe that notebooks are a perfect iteration environment for generative AI -- they are flexible, multi-modal, and collaborative.\n\nThe multi-modality and flexibility offered by notebooks and [`ipynb`](https://ipython.org/ipython-doc/3/notebook/nbformat.html) offers a good interaction model for generative AI. The `aiconfig` file format is extensible like `ipynb`, and AI Workbook editor allows rapid iteration in a notebook-like IDE.\n\n_AI Workbooks are to AIConfig what Jupyter notebooks are to `ipynb`_\n\nThere are 2 areas where we are going beyond what notebooks offer:\n\n1. `aiconfig` is more **source-control friendly** than `ipynb`. `ipynb` stores binary data (images, etc.) by encoding it in the file, while `aiconfig` recommends using file URI references instead.\n2. `aiconfig` can be imported and **connected to application code** using the AIConfig SDK.\n', 'repo_name': 'aiconfig'}} ts_ns=1702174862235185200
2023-12-09 21:25:40,003 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# RepoRover\n\nA developer tool that allows users to quickly understand GitHub repositories that they are not familiar with utilizing config.ai and OpenAi.\n', 'repo_name': 'RepoRover'}, 'options': None, 'kwargs': {}} ts_ns=1702175112083635600
2023-12-09 21:25:40,003 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# RepoRover\n\nA developer tool that allows users to quickly understand GitHub repositories that they are not familiar with utilizing config.ai and OpenAi.\n', 'repo_name': 'RepoRover'}, 'options': None, 'kwargs': {}} ts_ns=1702175112083635600
2023-12-09 21:25:40,003 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# RepoRover\n\nA developer tool that allows users to quickly understand GitHub repositories that they are not familiar with utilizing config.ai and OpenAi.\n', 'repo_name': 'RepoRover'}, 'options': None, 'kwargs': {}} ts_ns=1702175112083635600
2023-12-09 21:25:40,004 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# RepoRover\n\nA developer tool that allows users to quickly understand GitHub repositories that they are not familiar with utilizing config.ai and OpenAi.\n', 'repo_name': 'RepoRover'}} ts_ns=1702175112083635600
2023-12-09 21:25:40,004 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# RepoRover\n\nA developer tool that allows users to quickly understand GitHub repositories that they are not familiar with utilizing config.ai and OpenAi.\n', 'repo_name': 'RepoRover'}} ts_ns=1702175112083635600
2023-12-09 21:25:40,004 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# RepoRover\n\nA developer tool that allows users to quickly understand GitHub repositories that they are not familiar with utilizing config.ai and OpenAi.\n', 'repo_name': 'RepoRover'}} ts_ns=1702175112083635600
2023-12-09 21:25:40,004 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# RepoRover\n\nA developer tool that allows users to quickly understand GitHub repositories that they are not familiar with utilizing config.ai and OpenAi.\n', 'repo_name': 'RepoRover'}} ts_ns=1702175112083635600
2023-12-09 21:25:40,004 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# RepoRover\n\nA developer tool that allows users to quickly understand GitHub repositories that they are not familiar with utilizing config.ai and OpenAi.\n', 'repo_name': 'RepoRover'}} ts_ns=1702175112083635600
2023-12-09 21:25:40,004 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# RepoRover\n\nA developer tool that allows users to quickly understand GitHub repositories that they are not familiar with utilizing config.ai and OpenAi.\n', 'repo_name': 'RepoRover'}} ts_ns=1702175112083635600
2023-12-09 21:25:40,014 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'model': 'gpt-4', 'temperature': 0.75, 'frequency_penalty': 0, 'top_p': 1, 'presence_penalty': 0, 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is RepoRover. The READMEfile is:\n# RepoRover\n\nA developer tool that allows users to quickly understand GitHub repositories that they are not familiar with utilizing config.ai and OpenAi.\n', 'role': 'user'}]}} ts_ns=1702175112083635600
2023-12-09 21:25:40,014 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'model': 'gpt-4', 'temperature': 0.75, 'frequency_penalty': 0, 'top_p': 1, 'presence_penalty': 0, 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is RepoRover. The READMEfile is:\n# RepoRover\n\nA developer tool that allows users to quickly understand GitHub repositories that they are not familiar with utilizing config.ai and OpenAi.\n', 'role': 'user'}]}} ts_ns=1702175112083635600
2023-12-09 21:25:40,014 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'model': 'gpt-4', 'temperature': 0.75, 'frequency_penalty': 0, 'top_p': 1, 'presence_penalty': 0, 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is RepoRover. The READMEfile is:\n# RepoRover\n\nA developer tool that allows users to quickly understand GitHub repositories that they are not familiar with utilizing config.ai and OpenAi.\n', 'role': 'user'}]}} ts_ns=1702175112083635600
2023-12-09 21:25:43,189 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The RepoRover repository houses a developer tool designed to help users quickly understand GitHub repositories that they may not be familiar with. This tool leverages the capabilities of config.ai and OpenAI to facilitate this understanding.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175112083635600
2023-12-09 21:25:43,189 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The RepoRover repository houses a developer tool designed to help users quickly understand GitHub repositories that they may not be familiar with. This tool leverages the capabilities of config.ai and OpenAI to facilitate this understanding.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175112083635600
2023-12-09 21:25:43,189 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The RepoRover repository houses a developer tool designed to help users quickly understand GitHub repositories that they may not be familiar with. This tool leverages the capabilities of config.ai and OpenAI to facilitate this understanding.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175112083635600
2023-12-09 21:25:43,189 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The RepoRover repository houses a developer tool designed to help users quickly understand GitHub repositories that they may not be familiar with. This tool leverages the capabilities of config.ai and OpenAI to facilitate this understanding.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175112083635600
2023-12-09 21:25:43,189 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The RepoRover repository houses a developer tool designed to help users quickly understand GitHub repositories that they may not be familiar with. This tool leverages the capabilities of config.ai and OpenAI to facilitate this understanding.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175112083635600
2023-12-09 21:25:43,189 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The RepoRover repository houses a developer tool designed to help users quickly understand GitHub repositories that they may not be familiar with. This tool leverages the capabilities of config.ai and OpenAI to facilitate this understanding.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175112083635600
2023-12-09 21:29:22,761 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# RepoRover\n\nA developer tool that allows users to quickly understand GitHub repositories that they are not familiar with utilizing config.ai and OpenAi.\n', 'repo_name': 'RepoRover'}, 'options': None, 'kwargs': {}} ts_ns=1702175341454770100
2023-12-09 21:29:22,761 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# RepoRover\n\nA developer tool that allows users to quickly understand GitHub repositories that they are not familiar with utilizing config.ai and OpenAi.\n', 'repo_name': 'RepoRover'}, 'options': None, 'kwargs': {}} ts_ns=1702175341454770100
2023-12-09 21:29:22,761 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# RepoRover\n\nA developer tool that allows users to quickly understand GitHub repositories that they are not familiar with utilizing config.ai and OpenAi.\n', 'repo_name': 'RepoRover'}, 'options': None, 'kwargs': {}} ts_ns=1702175341454770100
2023-12-09 21:29:22,761 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# RepoRover\n\nA developer tool that allows users to quickly understand GitHub repositories that they are not familiar with utilizing config.ai and OpenAi.\n', 'repo_name': 'RepoRover'}} ts_ns=1702175341454770100
2023-12-09 21:29:22,761 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# RepoRover\n\nA developer tool that allows users to quickly understand GitHub repositories that they are not familiar with utilizing config.ai and OpenAi.\n', 'repo_name': 'RepoRover'}} ts_ns=1702175341454770100
2023-12-09 21:29:22,761 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# RepoRover\n\nA developer tool that allows users to quickly understand GitHub repositories that they are not familiar with utilizing config.ai and OpenAi.\n', 'repo_name': 'RepoRover'}} ts_ns=1702175341454770100
2023-12-09 21:29:22,761 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# RepoRover\n\nA developer tool that allows users to quickly understand GitHub repositories that they are not familiar with utilizing config.ai and OpenAi.\n', 'repo_name': 'RepoRover'}} ts_ns=1702175341454770100
2023-12-09 21:29:22,761 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# RepoRover\n\nA developer tool that allows users to quickly understand GitHub repositories that they are not familiar with utilizing config.ai and OpenAi.\n', 'repo_name': 'RepoRover'}} ts_ns=1702175341454770100
2023-12-09 21:29:22,761 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# RepoRover\n\nA developer tool that allows users to quickly understand GitHub repositories that they are not familiar with utilizing config.ai and OpenAi.\n', 'repo_name': 'RepoRover'}} ts_ns=1702175341454770100
2023-12-09 21:29:22,769 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'model': 'gpt-4', 'frequency_penalty': 0, 'top_p': 1, 'temperature': 0.75, 'presence_penalty': 0, 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is RepoRover. The READMEfile is:\n# RepoRover\n\nA developer tool that allows users to quickly understand GitHub repositories that they are not familiar with utilizing config.ai and OpenAi.\n', 'role': 'user'}]}} ts_ns=1702175341454770100
2023-12-09 21:29:22,769 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'model': 'gpt-4', 'frequency_penalty': 0, 'top_p': 1, 'temperature': 0.75, 'presence_penalty': 0, 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is RepoRover. The READMEfile is:\n# RepoRover\n\nA developer tool that allows users to quickly understand GitHub repositories that they are not familiar with utilizing config.ai and OpenAi.\n', 'role': 'user'}]}} ts_ns=1702175341454770100
2023-12-09 21:29:22,769 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'model': 'gpt-4', 'frequency_penalty': 0, 'top_p': 1, 'temperature': 0.75, 'presence_penalty': 0, 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is RepoRover. The READMEfile is:\n# RepoRover\n\nA developer tool that allows users to quickly understand GitHub repositories that they are not familiar with utilizing config.ai and OpenAi.\n', 'role': 'user'}]}} ts_ns=1702175341454770100
2023-12-09 21:29:25,216 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The RepoRover repository is a tool designed for developers. Its primary function is to aid users in quickly understanding GitHub repositories with which they are not familiar. To achieve this, RepoRover utilizes config.ai and OpenAi technologies.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175341454770100
2023-12-09 21:29:25,216 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The RepoRover repository is a tool designed for developers. Its primary function is to aid users in quickly understanding GitHub repositories with which they are not familiar. To achieve this, RepoRover utilizes config.ai and OpenAi technologies.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175341454770100
2023-12-09 21:29:25,216 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The RepoRover repository is a tool designed for developers. Its primary function is to aid users in quickly understanding GitHub repositories with which they are not familiar. To achieve this, RepoRover utilizes config.ai and OpenAi technologies.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175341454770100
2023-12-09 21:29:25,216 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The RepoRover repository is a tool designed for developers. Its primary function is to aid users in quickly understanding GitHub repositories with which they are not familiar. To achieve this, RepoRover utilizes config.ai and OpenAi technologies.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175341454770100
2023-12-09 21:29:25,216 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The RepoRover repository is a tool designed for developers. Its primary function is to aid users in quickly understanding GitHub repositories with which they are not familiar. To achieve this, RepoRover utilizes config.ai and OpenAi technologies.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175341454770100
2023-12-09 21:29:25,216 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The RepoRover repository is a tool designed for developers. Its primary function is to aid users in quickly understanding GitHub repositories with which they are not familiar. To achieve this, RepoRover utilizes config.ai and OpenAi technologies.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175341454770100
2023-12-09 21:31:40,214 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '<div align="center"><picture>\n  <img alt="aiconfig" src="aiconfig-docs/static/img/readme_logo.png" />\n</picture></div>\n<br/>\n\n![Python](https://github.com/lastmile-ai/aiconfig/actions/workflows/main_python.yml/badge.svg)\n![Node](https://github.com/lastmile-ai/aiconfig/actions/workflows/main-typescript.yml/badge.svg)\n![Docs](https://github.com/lastmile-ai/aiconfig/actions/workflows/test-deploy-docs.yml/badge.svg)\n[![Discord](<https://img.shields.io/badge/Discord-LastMile%20AI-Blue?color=rgb(37%2C%20150%2C%20190)>)](https://discord.gg/qMqgzDae)\n\n> Full documentation: **[aiconfig.lastmileai.dev](https://aiconfig.lastmileai.dev/)**\n\n## Overview\n\nAIConfig saves prompts, models and model parameters as source control friendly configs. This allows you to iterate on prompts and model parameters _separately from your application code_.\n\n1. **Prompts as configs**: a [standardized JSON format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to store generative AI model settings, prompt inputs/outputs, and flexible metadata.\n2. **Model-agnostic SDK**: Python & Node SDKs to use `aiconfig` in your application code. AIConfig is designed to be **model-agnostic** and **multi-modal**, so you can extend it to work with any generative AI model, including text, image and audio.\n3. **AI Workbook editor**: A [notebook-like playground](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to edit `aiconfig` files visually, run prompts, tweak models and model settings, and chain things together.\n\n### What problem it solves\n\nToday, application code is tightly coupled with the gen AI settings for the application -- prompts, parameters, and model-specific logic is all jumbled in with app code.\n\n- results in increased complexity\n- makes it hard to iterate on the prompts or try different models easily\n- makes it hard to evaluate prompt/model performance\n\nAIConfig helps unwind complexity by separating prompts, model parameters, and model-specific logic from your application.\n\n- simplifies application code -- simply call `config.run()`\n- open the `aiconfig` in a playground to iterate quickly\n- version control and evaluate the `aiconfig` - it\'s the AI artifact for your application.\n\n![AIConfig flow](aiconfig-docs/static/img/aiconfig_dataflow.png)\n\n### Quicknav\n\n<ul style="margin-bottom:0; padding-bottom:0;">\n  <li><a href="#install">Getting Started</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig">Create an AIConfig</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig">Run a prompt</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/parameters">Pass data into prompts</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain">Prompt Chains</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig">Callbacks and monitoring</a></li>\n  </ul>\n  <li><a href="#aiconfig-sdk">SDK Cheatsheet</a></li>\n  <li><a href="#cookbooks">Cookbooks and guides</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT">CLI Chatbot</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig">RAG with AIConfig</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing">Prompt routing</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI">OpenAI function calling</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification">Chain of Verification</a></li>\n  </ul>\n  <li><a href="#supported-models">Supported models</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama">LLaMA2 example</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace">Hugging Face (Mistral-7B) example</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency">PaLM</a></li>\n  </ul>\n  <li><a href="#extensibility">Extensibility</a></li>\n  <li><a href="#contributing-to-aiconfig">Contributing</a></li>\n  <li><a href="#roadmap">Roadmap</a></li>\n  <li><a href="#faqs">FAQ</a></li>\n</ul>\n\n## Features\n\n- [x] **Source-control friendly** [`aiconfig` format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to save prompts and model settings, which you can use for evaluation, reproducibility and simplifying your application code.\n- [x] **Multi-modal and model agnostic**. Use with any model, and serialize/deserialize data with the same `aiconfig` format.\n- [x] **Prompt chaining and parameterization** with [{{handlebars}}](https://handlebarsjs.com/) templating syntax, allowing you to pass dynamic data into prompts (as well as between prompts).\n- [x] **Streaming** supported out of the box, allowing you to get playground-like streaming wherever you use `aiconfig`.\n- [x] **Notebook editor**. [AI Workbooks editor](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to visually create your `aiconfig`, and use the SDK to connect it to your application code.\n\n## Install\n\nInstall with your favorite package manager for Node or Python.\n\n### Node.js\n\n#### `npm` or `yarn`\n\n```bash\nnpm install aiconfig\n```\n\n```bash\nyarn add aiconfig\n```\n\n### Python\n\n#### `pip3` or `poetry`\n\n```bash\npip3 install python-aiconfig\n```\n\n```bash\npoetry add python-aiconfig\n```\n\n[Detailed installation instructions](https://aiconfig.lastmileai.dev/docs/getting-started/#installation).\n\n### Set your OpenAI API Key\n\n> **Note**: Make sure to specify the API keys (such as [`OPENAI_API_KEY`](https://platform.openai.com/api-keys)) in your environment before proceeding.\n\nIn your CLI, set the environment variable:\n\n```bash\nexport OPENAI_API_KEY=my_key\n```\n\n## Getting Started\n\n> We cover Python instructions here, for Node.js please see the [detailed Getting Started guide](https://aiconfig.lastmileai.dev/docs/getting-started)\n\nIn this quickstart, you will create a customizable NYC travel itinerary using `aiconfig`.\n\nThis AIConfig contains a prompt chain to get a list of travel activities from an LLM and then generate an itinerary in an order specified by the user.\n\n> **Link to tutorial code: [here](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Getting-Started)**\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/805173d1-0f83-44c5-b570-c776bb7dba66\n\n### Download `travel.aiconfig.json`\n\n> **Note**: Don\'t worry if you don\'t understand all the pieces of this yet, we\'ll go over it step by step.\n\n```json\n{\n  "name": "NYC Trip Planner",\n  "description": "Intrepid explorer with ChatGPT and AIConfig",\n  "schema_version": "latest",\n  "metadata": {\n    "models": {\n      "gpt-3.5-turbo": {\n        "model": "gpt-3.5-turbo",\n        "top_p": 1,\n        "temperature": 1\n      },\n      "gpt-4": {\n        "model": "gpt-4",\n        "max_tokens": 3000,\n        "system_prompt": "You are an expert travel coordinator with exquisite taste."\n      }\n    },\n    "default_model": "gpt-3.5-turbo"\n  },\n  "prompts": [\n    {\n      "name": "get_activities",\n      "input": "Tell me 10 fun attractions to do in NYC."\n    },\n    {\n      "name": "gen_itinerary",\n      "input": "Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}.",\n      "metadata": {\n        "model": "gpt-4",\n        "parameters": {\n          "order_by": "geographic location"\n        }\n      }\n    }\n  ]\n}\n```\n\n### Run the `get_activities` prompt.\n\nYou don\'t need to worry about how to run inference for the model; it\'s all handled by AIConfig. The prompt runs with gpt-3.5-turbo since that is the `default_model` for this AIConfig.\n\nCreate a new file called `app.py` and and enter the following code:\n\n```python\nimport asyncio\nfrom aiconfig import AIConfigRuntime, InferenceOptions\n\nasync def main():\n  # Load the aiconfig\n  config = AIConfigRuntime.load(\'travel.aiconfig.json\')\n\n  # Run a single prompt (with streaming)\n  inference_options = InferenceOptions(stream=True)\n  await config.run("get_activities", options=inference_options)\n\nasyncio.run(main())\n```\n\nNow run this in your terminal with the command:\n\n```bash\npython3 app.py\n```\n\n### Run the `gen_itinerary` prompt.\n\nIn your `app.py` file, change the last line to below:\n\n```python\nawait config.run("gen_itinerary", params=None, options=inference_options)\n```\n\nRe-run the command in your terminal:\n\n```bash\npython3 app.py\n```\n\nThis prompt depends on the output of `get_activities`. It also takes in parameters (user input) to determine the customized itinerary.\n\nLet\'s take a closer look:\n\n**`gen_itinerary` prompt:**\n\n```\n"Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}."\n```\n\n**prompt metadata:**\n\n```json\n{\n  "metadata": {\n    "model": "gpt-4",\n    "parameters": {\n      "order_by": "geographic location"\n    }\n  }\n}\n```\n\nObserve the following:\n\n1. The prompt depends on the output of the `get_activities` prompt.\n2. It also depends on an `order_by` parameter (using {{handlebars}} syntax)\n3. It uses **gpt-4**, whereas the `get_activities` prompt it depends on uses **gpt-3.5-turbo**.\n\n> Effectively, this is a prompt chain between `gen_itinerary` and `get_activities` prompts, _as well as_ as a model chain between **gpt-3.5-turbo** and **gpt-4**.\n\nLet\'s run this with AIConfig:\n\nReplace `config.run` above with this:\n\n```python\nawait config.run("gen_itinerary", params={"order_by": "duration"}, options=inference_options, run_with_dependencies=True)\n```\n\nNotice how simple the syntax is to perform a fairly complex task - running 2 different prompts across 2 different models and chaining one\'s output as part of the input of another.\n\nThe code will just run `get_activities`, then pipe its output as an input to `gen_itinerary`, and finally run `gen_itinerary`.\n\n### Save the AIConfig\n\nLet\'s save the AIConfig back to disk, and serialize the outputs from the latest inference run as well:\n\n```python\n# Save the aiconfig to disk. and serialize outputs from the model run\nconfig.save(\'updated.aiconfig.json\', include_outputs=True)\n```\n\n### Edit `aiconfig` in a notebook editor\n\nWe can iterate on an `aiconfig` using a notebook-like editor called an **AI Workbook**. Now that we have an `aiconfig` file artifact that encapsulates the generative AI part of our application, we can iterate on it separately from the application code that uses it.\n\n1. Go to https://lastmileai.dev.\n2. Go to Workbooks page: https://lastmileai.dev/workbooks\n3. Click dropdown from \'+ New Workbook\' and select \'Create from AIConfig\'\n4. Upload `travel.aiconfig.json`\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/5d901493-bbda-4f8e-93c7-dd9a91bf242e\n\nTry out the workbook playground here: **[NYC Travel Workbook](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9)**\n\n> **We are working on a local editor that you can run yourself. For now, please use the hosted version on https://lastmileai.dev.**\n\n### Additional Guides\n\nThere is a lot you can do with `aiconfig`. We have several other tutorials to help get you started:\n\n- [Create an AIConfig from scratch](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig)\n- [Run a prompt](https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig)\n- [Pass data into prompts](https://aiconfig.lastmileai.dev/docs/overview/parameters)\n- [Prompt chains](https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain)\n- [Callbacks and monitoring](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\nHere are some example uses:\n\n- [CLI Chatbot](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT)\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [Chain of thought](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### OpenAI Introspection API\n\nIf you are already using OpenAI completion API\'s in your application, you can get started very quickly to start saving the messages in an `aiconfig`.\n\nUsage: see openai_wrapper.ipynb.\n\nNow you can continue using `openai` completion API as normal. When you want to save the config, just call `new_config.save()` and all your openai completion calls will get serialized to disk.\n\n> [**Detailed guide here**](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig#openai-api-python-wrapper)\n\n## Supported Models\n\nAIConfig supports the following models out of the box:\n\n- OpenAI chat models (GPT-3, GPT-3.5, GPT-4)\n- LLaMA2 (running locally)\n- Google PaLM models (PaLM chat)\n- Hugging Face text generation models (e.g. Mistral-7B)\n\n### Examples\n\n- [OpenAI](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n> If you need to use a model that isn\'t provided out of the box, you can implement a `ModelParser` for it (see [Extending AIConfig](#extending-aiconfig)). **We welcome [contributions](https://aiconfig.lastmileai.dev/docs/contributing)**\n\n## AIConfig Schema\n\n[AIConfig specification](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format)\n\n## AIConfig SDK\n\n> Read the [Usage Guide](https://aiconfig.lastmileai.dev/docs/usage-guide) for more details.\n\nThe AIConfig SDK supports CRUD operations for prompts, models, parameters and metadata. Here are some common examples.\n\nThe root interface is the `AIConfigRuntime` object. That is the entrypoint for interacting with an AIConfig programmatically.\n\nLet\'s go over a few key CRUD operations to give a glimpse.\n\n### AIConfig `create`\n\n```python\nconfig = AIConfigRuntime.create("aiconfig name", "description")\n```\n\n### Prompt `resolve`\n\n`resolve` deserializes an existing `Prompt` into the data object that its model expects.\n\n```python\nconfig.resolve("prompt_name", params)\n```\n\n`params` are overrides you can specify to resolve any `{{handlebars}}` templates in the prompt. See the `gen_itinerary` prompt in the Getting Started example.\n\n### Prompt `serialize`\n\n`serialize` is the inverse of `resolve` -- it serializes the data object that a model understands into a `Prompt` object that can be serialized into the `aiconfig` format.\n\n```python\nconfig.serialize("model_name", data, "prompt_name")\n```\n\n### Prompt `run`\n\n`run` is used to run inference for the specified `Prompt`.\n\n```python\nconfig.run("prompt_name", params)\n```\n\n### `run_with_dependencies`\n\nThis is a variant of `run` -- this re-runs all prompt dependencies.\nFor example, in [`travel.aiconfig.json`](#download-travelaiconfigjson), the `gen_itinerary` prompt references the output of the `get_activities` prompt using `{{get_activities.output}}`.\n\nRunning this function will first execute `get_activities`, and use its output to resolve the `gen_itinerary` prompt before executing it.\nThis is transitive, so it computes the Directed Acyclic Graph of dependencies to execute. Complex relationships can be modeled this way.\n\n```python\nconfig.run_with_dependencies("gen_itinerary")\n```\n\n### Updating metadata and parameters\n\nUse the `get/set_metadata` and `get/set_parameter` methods to interact with metadata and parameters (`set_parameter` is just syntactic sugar to update `"metadata.parameters"`)\n\n```python\nconfig.set_metadata("key", data, "prompt_name")\n```\n\nNote: if `"prompt_name"` is specified, the metadata is updated specifically for that prompt. Otherwise, the global metadata is updated.\n\n### Register new `ModelParser`\n\nUse the `AIConfigRuntime.register_model_parser` if you want to use a different `ModelParser`, or configure AIConfig to work with an additional model.\n\nAIConfig uses the model name string to retrieve the right `ModelParser` for a given Prompt (see `AIConfigRuntime.get_model_parser`), so you can register a different ModelParser for the same ID to override which `ModelParser` handles a Prompt.\n\nFor example, suppose I want to use `MyOpenAIModelParser` to handle `gpt-4` prompts. I can do the following at the start of my application:\n\n```python\nAIConfigRuntime.register_model_parser(myModelParserInstance, ["gpt-4"])\n```\n\n### Callback events\n\nUse callback events to trace and monitor what\'s going on -- helpful for debugging and observability.\n\n```python\nfrom aiconfig import AIConfigRuntime, CallbackEvent, CallbackManager\nconfig = AIConfigRuntime.load(\'aiconfig.json\')\n\nasync def my_custom_callback(event: CallbackEvent) -> None:\n  print(f"Event triggered: {event.name}", event)\n\ncallback_manager = CallbackManager([my_custom_callback])\nconfig.set_callback_manager(callback_manager)\n\nawait config.run("prompt_name")\n```\n\n[**Read more** here](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\n## Extensibility\n\nAIConfig is designed to be customized and extended for your use-case. The [Extensibility](/docs/extensibility) guide goes into more detail.\n\nCurrently, there are 3 core ways to extend AIConfig:\n\n1. [Supporting other models](https://aiconfig.lastmileai.dev/docs/extensibility#1-bring-your-own-model) - define a ModelParser extension\n2. [Callback event handlers](https://aiconfig.lastmileai.dev/docs/extensibility#2-callback-handlers) - tracing and monitoring\n3. [Custom metadata](https://aiconfig.lastmileai.dev/docs/extensibility#3-custom-metadata) - save custom fields in `aiconfig`\n\n## Contributing to `aiconfig`\n\nThis is our first open-source project and we\'d love your help.\n\nSee our [contributing guidelines](https://aiconfig.lastmileai.dev/docs/contributing) -- we would especially love help adding support for additional models that the community wants.\n\n## Cookbooks\n\nWe provide several guides to demonstrate the power of `aiconfig`.\n\n> **See the [`cookbooks`](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks) folder for examples to clone.**\n\n### Chatbot\n\n- [Wizard GPT](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT) - speak to a wizard on your CLI\n\n- [CLI-mate](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Cli-Mate) - help you make code-mods interactively on your codebase.\n\n### Retrieval Augmented Generated (RAG)\n\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n\nAt its core, RAG is about passing data into prompts. Read how to [pass data](/docs/overview/parameters) with AIConfig.\n\n### Function calling\n\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n\n### Prompt routing\n\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n\n### Chain of Thought\n\nA variant of chain-of-thought is Chain of Verification, used to help reduce hallucinations. Check out the aiconfig cookbook for CoVe:\n\n- [Chain of Verification](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### Using local LLaMA2 with `aiconfig`\n\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n\n### Hugging Face text generation\n\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n\n### Google PaLM\n\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n## Roadmap\n\nThis project is under active development.\n\nIf you\'d like to help, please see the [contributing guidelines](#contributing-to-aiconfig).\n\nPlease create issues for additional capabilities you\'d like to see.\n\nHere\'s what\'s already on our roadmap:\n\n- Evaluation interfaces: allow `aiconfig` artifacts to be evaluated with user-defined eval functions.\n  - We are also considering integrating with existing evaluation frameworks.\n- Local editor for `aiconfig`: enable you to interact with aiconfigs more intuitively.\n- OpenAI Assistants API support\n- Multi-modal ModelParsers:\n  - GPT4-V support\n  - DALLE-3\n  - Whisper\n  - HuggingFace image generation\n\n## FAQs\n\n### How should I edit an `aiconfig` file?\n\nEditing a configshould be done either programmatically via SDK or via the UI (workbooks):\n\n- [Programmatic](https://github.com/lastmile-ai/aiconfig/blob/main/cookbooks/Create-AIConfig-Programmatically/create_aiconfig_programmatically.ipynb) editing.\n\n- [Edit with a workbook](#edit-aiconfig-in-a-notebook-editor) editor: this is similar to editing an ipynb file as a notebook (most people never touch the json ipynb directly)\n\nYou should only edit the `aiconfig` by hand for minor modifications, like tweaking a prompt string or updating some metadata.\n\n### Does this support custom endpoints?\n\nOut of the box, AIConfig already supports all OpenAI GPT\\* models, Googles PaLM model and any textgeneration model on Hugging Face (like Mistral). See [Supported Models](#supported-models) for more details.\n\nAdditionally, you can install `aiconfig` [extensions](https://github.com/lastmile-ai/aiconfig/tree/main/extensions) for additional models (see question below).\n\n### Is OpenAI function calling supported?\n\nYes. [This example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI) goes through how to do it.\n\nWe are also working on adding support for the Assistants API.\n\n### How can I use aiconfig with my own model endpoint?\n\nModel support is implemented as ModelParsers in the AIConfig SDK, and the idea is that anyone, including you, can define a ModelParser (and even publish it as an extension package).\n\nAll thats needed to use a model with AIConfig is a ModelParser that knows\n\n- how to serialize data from a model into the aiconfig format\n- how to deserialize data from an aiconfig into the type the model expects\n- how to run inference for model.\n\nFor more details, see [Extensibility](https://aiconfig.lastmileai.dev/docs/extensibility).\n\n### When should I store outputs in an `aiconfig`?\n\nThe `AIConfigRuntime` object is used to interact with an aiconfig programmatically (see [SDK usage guide](#aiconfig-sdk)). As you run prompts, this object keeps track of the outputs returned from the model.\n\nYou can choose to serialize these outputs back into the `aiconfig` by using the `config.save(include_outputs=True)` API. This can be useful for preserving context -- think of it like session state.\n\nFor example, you can use aiconfig to create a chatbot, and use the same format to save the chat history so it can be resumed for the next session.\n\nYou can also choose to save outputs to a _different_ file than the original config -- `config.save("history.aiconfig.json", include_outputs=True)`.\n\n### Why should I use `aiconfig` instead of things like [configurator](https://pypi.org/project/configurator/)?\n\nIt helps to have a [standardized format](http://aiconfig.lastmileai.dev/docs/overview/ai-config-format) specifically for storing generative AI prompts, inference results, model parameters and arbitrary metadata, as opposed to a general-purpose configuration schema.\n\nWith that standardization, you just need a layer that knows how to serialize/deserialize from that format into whatever the inference endpoints require.\n\n### This looks similar to `ipynb` for Jupyter notebooks\n\nWe believe that notebooks are a perfect iteration environment for generative AI -- they are flexible, multi-modal, and collaborative.\n\nThe multi-modality and flexibility offered by notebooks and [`ipynb`](https://ipython.org/ipython-doc/3/notebook/nbformat.html) offers a good interaction model for generative AI. The `aiconfig` file format is extensible like `ipynb`, and AI Workbook editor allows rapid iteration in a notebook-like IDE.\n\n_AI Workbooks are to AIConfig what Jupyter notebooks are to `ipynb`_\n\nThere are 2 areas where we are going beyond what notebooks offer:\n\n1. `aiconfig` is more **source-control friendly** than `ipynb`. `ipynb` stores binary data (images, etc.) by encoding it in the file, while `aiconfig` recommends using file URI references instead.\n2. `aiconfig` can be imported and **connected to application code** using the AIConfig SDK.\n', 'repo_name': 'aiconfig'}, 'options': None, 'kwargs': {}} ts_ns=1702175485063260400
2023-12-09 21:31:40,214 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '<div align="center"><picture>\n  <img alt="aiconfig" src="aiconfig-docs/static/img/readme_logo.png" />\n</picture></div>\n<br/>\n\n![Python](https://github.com/lastmile-ai/aiconfig/actions/workflows/main_python.yml/badge.svg)\n![Node](https://github.com/lastmile-ai/aiconfig/actions/workflows/main-typescript.yml/badge.svg)\n![Docs](https://github.com/lastmile-ai/aiconfig/actions/workflows/test-deploy-docs.yml/badge.svg)\n[![Discord](<https://img.shields.io/badge/Discord-LastMile%20AI-Blue?color=rgb(37%2C%20150%2C%20190)>)](https://discord.gg/qMqgzDae)\n\n> Full documentation: **[aiconfig.lastmileai.dev](https://aiconfig.lastmileai.dev/)**\n\n## Overview\n\nAIConfig saves prompts, models and model parameters as source control friendly configs. This allows you to iterate on prompts and model parameters _separately from your application code_.\n\n1. **Prompts as configs**: a [standardized JSON format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to store generative AI model settings, prompt inputs/outputs, and flexible metadata.\n2. **Model-agnostic SDK**: Python & Node SDKs to use `aiconfig` in your application code. AIConfig is designed to be **model-agnostic** and **multi-modal**, so you can extend it to work with any generative AI model, including text, image and audio.\n3. **AI Workbook editor**: A [notebook-like playground](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to edit `aiconfig` files visually, run prompts, tweak models and model settings, and chain things together.\n\n### What problem it solves\n\nToday, application code is tightly coupled with the gen AI settings for the application -- prompts, parameters, and model-specific logic is all jumbled in with app code.\n\n- results in increased complexity\n- makes it hard to iterate on the prompts or try different models easily\n- makes it hard to evaluate prompt/model performance\n\nAIConfig helps unwind complexity by separating prompts, model parameters, and model-specific logic from your application.\n\n- simplifies application code -- simply call `config.run()`\n- open the `aiconfig` in a playground to iterate quickly\n- version control and evaluate the `aiconfig` - it\'s the AI artifact for your application.\n\n![AIConfig flow](aiconfig-docs/static/img/aiconfig_dataflow.png)\n\n### Quicknav\n\n<ul style="margin-bottom:0; padding-bottom:0;">\n  <li><a href="#install">Getting Started</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig">Create an AIConfig</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig">Run a prompt</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/parameters">Pass data into prompts</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain">Prompt Chains</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig">Callbacks and monitoring</a></li>\n  </ul>\n  <li><a href="#aiconfig-sdk">SDK Cheatsheet</a></li>\n  <li><a href="#cookbooks">Cookbooks and guides</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT">CLI Chatbot</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig">RAG with AIConfig</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing">Prompt routing</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI">OpenAI function calling</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification">Chain of Verification</a></li>\n  </ul>\n  <li><a href="#supported-models">Supported models</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama">LLaMA2 example</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace">Hugging Face (Mistral-7B) example</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency">PaLM</a></li>\n  </ul>\n  <li><a href="#extensibility">Extensibility</a></li>\n  <li><a href="#contributing-to-aiconfig">Contributing</a></li>\n  <li><a href="#roadmap">Roadmap</a></li>\n  <li><a href="#faqs">FAQ</a></li>\n</ul>\n\n## Features\n\n- [x] **Source-control friendly** [`aiconfig` format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to save prompts and model settings, which you can use for evaluation, reproducibility and simplifying your application code.\n- [x] **Multi-modal and model agnostic**. Use with any model, and serialize/deserialize data with the same `aiconfig` format.\n- [x] **Prompt chaining and parameterization** with [{{handlebars}}](https://handlebarsjs.com/) templating syntax, allowing you to pass dynamic data into prompts (as well as between prompts).\n- [x] **Streaming** supported out of the box, allowing you to get playground-like streaming wherever you use `aiconfig`.\n- [x] **Notebook editor**. [AI Workbooks editor](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to visually create your `aiconfig`, and use the SDK to connect it to your application code.\n\n## Install\n\nInstall with your favorite package manager for Node or Python.\n\n### Node.js\n\n#### `npm` or `yarn`\n\n```bash\nnpm install aiconfig\n```\n\n```bash\nyarn add aiconfig\n```\n\n### Python\n\n#### `pip3` or `poetry`\n\n```bash\npip3 install python-aiconfig\n```\n\n```bash\npoetry add python-aiconfig\n```\n\n[Detailed installation instructions](https://aiconfig.lastmileai.dev/docs/getting-started/#installation).\n\n### Set your OpenAI API Key\n\n> **Note**: Make sure to specify the API keys (such as [`OPENAI_API_KEY`](https://platform.openai.com/api-keys)) in your environment before proceeding.\n\nIn your CLI, set the environment variable:\n\n```bash\nexport OPENAI_API_KEY=my_key\n```\n\n## Getting Started\n\n> We cover Python instructions here, for Node.js please see the [detailed Getting Started guide](https://aiconfig.lastmileai.dev/docs/getting-started)\n\nIn this quickstart, you will create a customizable NYC travel itinerary using `aiconfig`.\n\nThis AIConfig contains a prompt chain to get a list of travel activities from an LLM and then generate an itinerary in an order specified by the user.\n\n> **Link to tutorial code: [here](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Getting-Started)**\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/805173d1-0f83-44c5-b570-c776bb7dba66\n\n### Download `travel.aiconfig.json`\n\n> **Note**: Don\'t worry if you don\'t understand all the pieces of this yet, we\'ll go over it step by step.\n\n```json\n{\n  "name": "NYC Trip Planner",\n  "description": "Intrepid explorer with ChatGPT and AIConfig",\n  "schema_version": "latest",\n  "metadata": {\n    "models": {\n      "gpt-3.5-turbo": {\n        "model": "gpt-3.5-turbo",\n        "top_p": 1,\n        "temperature": 1\n      },\n      "gpt-4": {\n        "model": "gpt-4",\n        "max_tokens": 3000,\n        "system_prompt": "You are an expert travel coordinator with exquisite taste."\n      }\n    },\n    "default_model": "gpt-3.5-turbo"\n  },\n  "prompts": [\n    {\n      "name": "get_activities",\n      "input": "Tell me 10 fun attractions to do in NYC."\n    },\n    {\n      "name": "gen_itinerary",\n      "input": "Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}.",\n      "metadata": {\n        "model": "gpt-4",\n        "parameters": {\n          "order_by": "geographic location"\n        }\n      }\n    }\n  ]\n}\n```\n\n### Run the `get_activities` prompt.\n\nYou don\'t need to worry about how to run inference for the model; it\'s all handled by AIConfig. The prompt runs with gpt-3.5-turbo since that is the `default_model` for this AIConfig.\n\nCreate a new file called `app.py` and and enter the following code:\n\n```python\nimport asyncio\nfrom aiconfig import AIConfigRuntime, InferenceOptions\n\nasync def main():\n  # Load the aiconfig\n  config = AIConfigRuntime.load(\'travel.aiconfig.json\')\n\n  # Run a single prompt (with streaming)\n  inference_options = InferenceOptions(stream=True)\n  await config.run("get_activities", options=inference_options)\n\nasyncio.run(main())\n```\n\nNow run this in your terminal with the command:\n\n```bash\npython3 app.py\n```\n\n### Run the `gen_itinerary` prompt.\n\nIn your `app.py` file, change the last line to below:\n\n```python\nawait config.run("gen_itinerary", params=None, options=inference_options)\n```\n\nRe-run the command in your terminal:\n\n```bash\npython3 app.py\n```\n\nThis prompt depends on the output of `get_activities`. It also takes in parameters (user input) to determine the customized itinerary.\n\nLet\'s take a closer look:\n\n**`gen_itinerary` prompt:**\n\n```\n"Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}."\n```\n\n**prompt metadata:**\n\n```json\n{\n  "metadata": {\n    "model": "gpt-4",\n    "parameters": {\n      "order_by": "geographic location"\n    }\n  }\n}\n```\n\nObserve the following:\n\n1. The prompt depends on the output of the `get_activities` prompt.\n2. It also depends on an `order_by` parameter (using {{handlebars}} syntax)\n3. It uses **gpt-4**, whereas the `get_activities` prompt it depends on uses **gpt-3.5-turbo**.\n\n> Effectively, this is a prompt chain between `gen_itinerary` and `get_activities` prompts, _as well as_ as a model chain between **gpt-3.5-turbo** and **gpt-4**.\n\nLet\'s run this with AIConfig:\n\nReplace `config.run` above with this:\n\n```python\nawait config.run("gen_itinerary", params={"order_by": "duration"}, options=inference_options, run_with_dependencies=True)\n```\n\nNotice how simple the syntax is to perform a fairly complex task - running 2 different prompts across 2 different models and chaining one\'s output as part of the input of another.\n\nThe code will just run `get_activities`, then pipe its output as an input to `gen_itinerary`, and finally run `gen_itinerary`.\n\n### Save the AIConfig\n\nLet\'s save the AIConfig back to disk, and serialize the outputs from the latest inference run as well:\n\n```python\n# Save the aiconfig to disk. and serialize outputs from the model run\nconfig.save(\'updated.aiconfig.json\', include_outputs=True)\n```\n\n### Edit `aiconfig` in a notebook editor\n\nWe can iterate on an `aiconfig` using a notebook-like editor called an **AI Workbook**. Now that we have an `aiconfig` file artifact that encapsulates the generative AI part of our application, we can iterate on it separately from the application code that uses it.\n\n1. Go to https://lastmileai.dev.\n2. Go to Workbooks page: https://lastmileai.dev/workbooks\n3. Click dropdown from \'+ New Workbook\' and select \'Create from AIConfig\'\n4. Upload `travel.aiconfig.json`\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/5d901493-bbda-4f8e-93c7-dd9a91bf242e\n\nTry out the workbook playground here: **[NYC Travel Workbook](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9)**\n\n> **We are working on a local editor that you can run yourself. For now, please use the hosted version on https://lastmileai.dev.**\n\n### Additional Guides\n\nThere is a lot you can do with `aiconfig`. We have several other tutorials to help get you started:\n\n- [Create an AIConfig from scratch](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig)\n- [Run a prompt](https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig)\n- [Pass data into prompts](https://aiconfig.lastmileai.dev/docs/overview/parameters)\n- [Prompt chains](https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain)\n- [Callbacks and monitoring](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\nHere are some example uses:\n\n- [CLI Chatbot](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT)\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [Chain of thought](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### OpenAI Introspection API\n\nIf you are already using OpenAI completion API\'s in your application, you can get started very quickly to start saving the messages in an `aiconfig`.\n\nUsage: see openai_wrapper.ipynb.\n\nNow you can continue using `openai` completion API as normal. When you want to save the config, just call `new_config.save()` and all your openai completion calls will get serialized to disk.\n\n> [**Detailed guide here**](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig#openai-api-python-wrapper)\n\n## Supported Models\n\nAIConfig supports the following models out of the box:\n\n- OpenAI chat models (GPT-3, GPT-3.5, GPT-4)\n- LLaMA2 (running locally)\n- Google PaLM models (PaLM chat)\n- Hugging Face text generation models (e.g. Mistral-7B)\n\n### Examples\n\n- [OpenAI](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n> If you need to use a model that isn\'t provided out of the box, you can implement a `ModelParser` for it (see [Extending AIConfig](#extending-aiconfig)). **We welcome [contributions](https://aiconfig.lastmileai.dev/docs/contributing)**\n\n## AIConfig Schema\n\n[AIConfig specification](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format)\n\n## AIConfig SDK\n\n> Read the [Usage Guide](https://aiconfig.lastmileai.dev/docs/usage-guide) for more details.\n\nThe AIConfig SDK supports CRUD operations for prompts, models, parameters and metadata. Here are some common examples.\n\nThe root interface is the `AIConfigRuntime` object. That is the entrypoint for interacting with an AIConfig programmatically.\n\nLet\'s go over a few key CRUD operations to give a glimpse.\n\n### AIConfig `create`\n\n```python\nconfig = AIConfigRuntime.create("aiconfig name", "description")\n```\n\n### Prompt `resolve`\n\n`resolve` deserializes an existing `Prompt` into the data object that its model expects.\n\n```python\nconfig.resolve("prompt_name", params)\n```\n\n`params` are overrides you can specify to resolve any `{{handlebars}}` templates in the prompt. See the `gen_itinerary` prompt in the Getting Started example.\n\n### Prompt `serialize`\n\n`serialize` is the inverse of `resolve` -- it serializes the data object that a model understands into a `Prompt` object that can be serialized into the `aiconfig` format.\n\n```python\nconfig.serialize("model_name", data, "prompt_name")\n```\n\n### Prompt `run`\n\n`run` is used to run inference for the specified `Prompt`.\n\n```python\nconfig.run("prompt_name", params)\n```\n\n### `run_with_dependencies`\n\nThis is a variant of `run` -- this re-runs all prompt dependencies.\nFor example, in [`travel.aiconfig.json`](#download-travelaiconfigjson), the `gen_itinerary` prompt references the output of the `get_activities` prompt using `{{get_activities.output}}`.\n\nRunning this function will first execute `get_activities`, and use its output to resolve the `gen_itinerary` prompt before executing it.\nThis is transitive, so it computes the Directed Acyclic Graph of dependencies to execute. Complex relationships can be modeled this way.\n\n```python\nconfig.run_with_dependencies("gen_itinerary")\n```\n\n### Updating metadata and parameters\n\nUse the `get/set_metadata` and `get/set_parameter` methods to interact with metadata and parameters (`set_parameter` is just syntactic sugar to update `"metadata.parameters"`)\n\n```python\nconfig.set_metadata("key", data, "prompt_name")\n```\n\nNote: if `"prompt_name"` is specified, the metadata is updated specifically for that prompt. Otherwise, the global metadata is updated.\n\n### Register new `ModelParser`\n\nUse the `AIConfigRuntime.register_model_parser` if you want to use a different `ModelParser`, or configure AIConfig to work with an additional model.\n\nAIConfig uses the model name string to retrieve the right `ModelParser` for a given Prompt (see `AIConfigRuntime.get_model_parser`), so you can register a different ModelParser for the same ID to override which `ModelParser` handles a Prompt.\n\nFor example, suppose I want to use `MyOpenAIModelParser` to handle `gpt-4` prompts. I can do the following at the start of my application:\n\n```python\nAIConfigRuntime.register_model_parser(myModelParserInstance, ["gpt-4"])\n```\n\n### Callback events\n\nUse callback events to trace and monitor what\'s going on -- helpful for debugging and observability.\n\n```python\nfrom aiconfig import AIConfigRuntime, CallbackEvent, CallbackManager\nconfig = AIConfigRuntime.load(\'aiconfig.json\')\n\nasync def my_custom_callback(event: CallbackEvent) -> None:\n  print(f"Event triggered: {event.name}", event)\n\ncallback_manager = CallbackManager([my_custom_callback])\nconfig.set_callback_manager(callback_manager)\n\nawait config.run("prompt_name")\n```\n\n[**Read more** here](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\n## Extensibility\n\nAIConfig is designed to be customized and extended for your use-case. The [Extensibility](/docs/extensibility) guide goes into more detail.\n\nCurrently, there are 3 core ways to extend AIConfig:\n\n1. [Supporting other models](https://aiconfig.lastmileai.dev/docs/extensibility#1-bring-your-own-model) - define a ModelParser extension\n2. [Callback event handlers](https://aiconfig.lastmileai.dev/docs/extensibility#2-callback-handlers) - tracing and monitoring\n3. [Custom metadata](https://aiconfig.lastmileai.dev/docs/extensibility#3-custom-metadata) - save custom fields in `aiconfig`\n\n## Contributing to `aiconfig`\n\nThis is our first open-source project and we\'d love your help.\n\nSee our [contributing guidelines](https://aiconfig.lastmileai.dev/docs/contributing) -- we would especially love help adding support for additional models that the community wants.\n\n## Cookbooks\n\nWe provide several guides to demonstrate the power of `aiconfig`.\n\n> **See the [`cookbooks`](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks) folder for examples to clone.**\n\n### Chatbot\n\n- [Wizard GPT](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT) - speak to a wizard on your CLI\n\n- [CLI-mate](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Cli-Mate) - help you make code-mods interactively on your codebase.\n\n### Retrieval Augmented Generated (RAG)\n\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n\nAt its core, RAG is about passing data into prompts. Read how to [pass data](/docs/overview/parameters) with AIConfig.\n\n### Function calling\n\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n\n### Prompt routing\n\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n\n### Chain of Thought\n\nA variant of chain-of-thought is Chain of Verification, used to help reduce hallucinations. Check out the aiconfig cookbook for CoVe:\n\n- [Chain of Verification](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### Using local LLaMA2 with `aiconfig`\n\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n\n### Hugging Face text generation\n\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n\n### Google PaLM\n\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n## Roadmap\n\nThis project is under active development.\n\nIf you\'d like to help, please see the [contributing guidelines](#contributing-to-aiconfig).\n\nPlease create issues for additional capabilities you\'d like to see.\n\nHere\'s what\'s already on our roadmap:\n\n- Evaluation interfaces: allow `aiconfig` artifacts to be evaluated with user-defined eval functions.\n  - We are also considering integrating with existing evaluation frameworks.\n- Local editor for `aiconfig`: enable you to interact with aiconfigs more intuitively.\n- OpenAI Assistants API support\n- Multi-modal ModelParsers:\n  - GPT4-V support\n  - DALLE-3\n  - Whisper\n  - HuggingFace image generation\n\n## FAQs\n\n### How should I edit an `aiconfig` file?\n\nEditing a configshould be done either programmatically via SDK or via the UI (workbooks):\n\n- [Programmatic](https://github.com/lastmile-ai/aiconfig/blob/main/cookbooks/Create-AIConfig-Programmatically/create_aiconfig_programmatically.ipynb) editing.\n\n- [Edit with a workbook](#edit-aiconfig-in-a-notebook-editor) editor: this is similar to editing an ipynb file as a notebook (most people never touch the json ipynb directly)\n\nYou should only edit the `aiconfig` by hand for minor modifications, like tweaking a prompt string or updating some metadata.\n\n### Does this support custom endpoints?\n\nOut of the box, AIConfig already supports all OpenAI GPT\\* models, Googles PaLM model and any textgeneration model on Hugging Face (like Mistral). See [Supported Models](#supported-models) for more details.\n\nAdditionally, you can install `aiconfig` [extensions](https://github.com/lastmile-ai/aiconfig/tree/main/extensions) for additional models (see question below).\n\n### Is OpenAI function calling supported?\n\nYes. [This example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI) goes through how to do it.\n\nWe are also working on adding support for the Assistants API.\n\n### How can I use aiconfig with my own model endpoint?\n\nModel support is implemented as ModelParsers in the AIConfig SDK, and the idea is that anyone, including you, can define a ModelParser (and even publish it as an extension package).\n\nAll thats needed to use a model with AIConfig is a ModelParser that knows\n\n- how to serialize data from a model into the aiconfig format\n- how to deserialize data from an aiconfig into the type the model expects\n- how to run inference for model.\n\nFor more details, see [Extensibility](https://aiconfig.lastmileai.dev/docs/extensibility).\n\n### When should I store outputs in an `aiconfig`?\n\nThe `AIConfigRuntime` object is used to interact with an aiconfig programmatically (see [SDK usage guide](#aiconfig-sdk)). As you run prompts, this object keeps track of the outputs returned from the model.\n\nYou can choose to serialize these outputs back into the `aiconfig` by using the `config.save(include_outputs=True)` API. This can be useful for preserving context -- think of it like session state.\n\nFor example, you can use aiconfig to create a chatbot, and use the same format to save the chat history so it can be resumed for the next session.\n\nYou can also choose to save outputs to a _different_ file than the original config -- `config.save("history.aiconfig.json", include_outputs=True)`.\n\n### Why should I use `aiconfig` instead of things like [configurator](https://pypi.org/project/configurator/)?\n\nIt helps to have a [standardized format](http://aiconfig.lastmileai.dev/docs/overview/ai-config-format) specifically for storing generative AI prompts, inference results, model parameters and arbitrary metadata, as opposed to a general-purpose configuration schema.\n\nWith that standardization, you just need a layer that knows how to serialize/deserialize from that format into whatever the inference endpoints require.\n\n### This looks similar to `ipynb` for Jupyter notebooks\n\nWe believe that notebooks are a perfect iteration environment for generative AI -- they are flexible, multi-modal, and collaborative.\n\nThe multi-modality and flexibility offered by notebooks and [`ipynb`](https://ipython.org/ipython-doc/3/notebook/nbformat.html) offers a good interaction model for generative AI. The `aiconfig` file format is extensible like `ipynb`, and AI Workbook editor allows rapid iteration in a notebook-like IDE.\n\n_AI Workbooks are to AIConfig what Jupyter notebooks are to `ipynb`_\n\nThere are 2 areas where we are going beyond what notebooks offer:\n\n1. `aiconfig` is more **source-control friendly** than `ipynb`. `ipynb` stores binary data (images, etc.) by encoding it in the file, while `aiconfig` recommends using file URI references instead.\n2. `aiconfig` can be imported and **connected to application code** using the AIConfig SDK.\n', 'repo_name': 'aiconfig'}, 'options': None, 'kwargs': {}} ts_ns=1702175485063260400
2023-12-09 21:31:40,214 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '<div align="center"><picture>\n  <img alt="aiconfig" src="aiconfig-docs/static/img/readme_logo.png" />\n</picture></div>\n<br/>\n\n![Python](https://github.com/lastmile-ai/aiconfig/actions/workflows/main_python.yml/badge.svg)\n![Node](https://github.com/lastmile-ai/aiconfig/actions/workflows/main-typescript.yml/badge.svg)\n![Docs](https://github.com/lastmile-ai/aiconfig/actions/workflows/test-deploy-docs.yml/badge.svg)\n[![Discord](<https://img.shields.io/badge/Discord-LastMile%20AI-Blue?color=rgb(37%2C%20150%2C%20190)>)](https://discord.gg/qMqgzDae)\n\n> Full documentation: **[aiconfig.lastmileai.dev](https://aiconfig.lastmileai.dev/)**\n\n## Overview\n\nAIConfig saves prompts, models and model parameters as source control friendly configs. This allows you to iterate on prompts and model parameters _separately from your application code_.\n\n1. **Prompts as configs**: a [standardized JSON format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to store generative AI model settings, prompt inputs/outputs, and flexible metadata.\n2. **Model-agnostic SDK**: Python & Node SDKs to use `aiconfig` in your application code. AIConfig is designed to be **model-agnostic** and **multi-modal**, so you can extend it to work with any generative AI model, including text, image and audio.\n3. **AI Workbook editor**: A [notebook-like playground](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to edit `aiconfig` files visually, run prompts, tweak models and model settings, and chain things together.\n\n### What problem it solves\n\nToday, application code is tightly coupled with the gen AI settings for the application -- prompts, parameters, and model-specific logic is all jumbled in with app code.\n\n- results in increased complexity\n- makes it hard to iterate on the prompts or try different models easily\n- makes it hard to evaluate prompt/model performance\n\nAIConfig helps unwind complexity by separating prompts, model parameters, and model-specific logic from your application.\n\n- simplifies application code -- simply call `config.run()`\n- open the `aiconfig` in a playground to iterate quickly\n- version control and evaluate the `aiconfig` - it\'s the AI artifact for your application.\n\n![AIConfig flow](aiconfig-docs/static/img/aiconfig_dataflow.png)\n\n### Quicknav\n\n<ul style="margin-bottom:0; padding-bottom:0;">\n  <li><a href="#install">Getting Started</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig">Create an AIConfig</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig">Run a prompt</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/parameters">Pass data into prompts</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain">Prompt Chains</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig">Callbacks and monitoring</a></li>\n  </ul>\n  <li><a href="#aiconfig-sdk">SDK Cheatsheet</a></li>\n  <li><a href="#cookbooks">Cookbooks and guides</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT">CLI Chatbot</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig">RAG with AIConfig</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing">Prompt routing</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI">OpenAI function calling</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification">Chain of Verification</a></li>\n  </ul>\n  <li><a href="#supported-models">Supported models</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama">LLaMA2 example</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace">Hugging Face (Mistral-7B) example</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency">PaLM</a></li>\n  </ul>\n  <li><a href="#extensibility">Extensibility</a></li>\n  <li><a href="#contributing-to-aiconfig">Contributing</a></li>\n  <li><a href="#roadmap">Roadmap</a></li>\n  <li><a href="#faqs">FAQ</a></li>\n</ul>\n\n## Features\n\n- [x] **Source-control friendly** [`aiconfig` format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to save prompts and model settings, which you can use for evaluation, reproducibility and simplifying your application code.\n- [x] **Multi-modal and model agnostic**. Use with any model, and serialize/deserialize data with the same `aiconfig` format.\n- [x] **Prompt chaining and parameterization** with [{{handlebars}}](https://handlebarsjs.com/) templating syntax, allowing you to pass dynamic data into prompts (as well as between prompts).\n- [x] **Streaming** supported out of the box, allowing you to get playground-like streaming wherever you use `aiconfig`.\n- [x] **Notebook editor**. [AI Workbooks editor](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to visually create your `aiconfig`, and use the SDK to connect it to your application code.\n\n## Install\n\nInstall with your favorite package manager for Node or Python.\n\n### Node.js\n\n#### `npm` or `yarn`\n\n```bash\nnpm install aiconfig\n```\n\n```bash\nyarn add aiconfig\n```\n\n### Python\n\n#### `pip3` or `poetry`\n\n```bash\npip3 install python-aiconfig\n```\n\n```bash\npoetry add python-aiconfig\n```\n\n[Detailed installation instructions](https://aiconfig.lastmileai.dev/docs/getting-started/#installation).\n\n### Set your OpenAI API Key\n\n> **Note**: Make sure to specify the API keys (such as [`OPENAI_API_KEY`](https://platform.openai.com/api-keys)) in your environment before proceeding.\n\nIn your CLI, set the environment variable:\n\n```bash\nexport OPENAI_API_KEY=my_key\n```\n\n## Getting Started\n\n> We cover Python instructions here, for Node.js please see the [detailed Getting Started guide](https://aiconfig.lastmileai.dev/docs/getting-started)\n\nIn this quickstart, you will create a customizable NYC travel itinerary using `aiconfig`.\n\nThis AIConfig contains a prompt chain to get a list of travel activities from an LLM and then generate an itinerary in an order specified by the user.\n\n> **Link to tutorial code: [here](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Getting-Started)**\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/805173d1-0f83-44c5-b570-c776bb7dba66\n\n### Download `travel.aiconfig.json`\n\n> **Note**: Don\'t worry if you don\'t understand all the pieces of this yet, we\'ll go over it step by step.\n\n```json\n{\n  "name": "NYC Trip Planner",\n  "description": "Intrepid explorer with ChatGPT and AIConfig",\n  "schema_version": "latest",\n  "metadata": {\n    "models": {\n      "gpt-3.5-turbo": {\n        "model": "gpt-3.5-turbo",\n        "top_p": 1,\n        "temperature": 1\n      },\n      "gpt-4": {\n        "model": "gpt-4",\n        "max_tokens": 3000,\n        "system_prompt": "You are an expert travel coordinator with exquisite taste."\n      }\n    },\n    "default_model": "gpt-3.5-turbo"\n  },\n  "prompts": [\n    {\n      "name": "get_activities",\n      "input": "Tell me 10 fun attractions to do in NYC."\n    },\n    {\n      "name": "gen_itinerary",\n      "input": "Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}.",\n      "metadata": {\n        "model": "gpt-4",\n        "parameters": {\n          "order_by": "geographic location"\n        }\n      }\n    }\n  ]\n}\n```\n\n### Run the `get_activities` prompt.\n\nYou don\'t need to worry about how to run inference for the model; it\'s all handled by AIConfig. The prompt runs with gpt-3.5-turbo since that is the `default_model` for this AIConfig.\n\nCreate a new file called `app.py` and and enter the following code:\n\n```python\nimport asyncio\nfrom aiconfig import AIConfigRuntime, InferenceOptions\n\nasync def main():\n  # Load the aiconfig\n  config = AIConfigRuntime.load(\'travel.aiconfig.json\')\n\n  # Run a single prompt (with streaming)\n  inference_options = InferenceOptions(stream=True)\n  await config.run("get_activities", options=inference_options)\n\nasyncio.run(main())\n```\n\nNow run this in your terminal with the command:\n\n```bash\npython3 app.py\n```\n\n### Run the `gen_itinerary` prompt.\n\nIn your `app.py` file, change the last line to below:\n\n```python\nawait config.run("gen_itinerary", params=None, options=inference_options)\n```\n\nRe-run the command in your terminal:\n\n```bash\npython3 app.py\n```\n\nThis prompt depends on the output of `get_activities`. It also takes in parameters (user input) to determine the customized itinerary.\n\nLet\'s take a closer look:\n\n**`gen_itinerary` prompt:**\n\n```\n"Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}."\n```\n\n**prompt metadata:**\n\n```json\n{\n  "metadata": {\n    "model": "gpt-4",\n    "parameters": {\n      "order_by": "geographic location"\n    }\n  }\n}\n```\n\nObserve the following:\n\n1. The prompt depends on the output of the `get_activities` prompt.\n2. It also depends on an `order_by` parameter (using {{handlebars}} syntax)\n3. It uses **gpt-4**, whereas the `get_activities` prompt it depends on uses **gpt-3.5-turbo**.\n\n> Effectively, this is a prompt chain between `gen_itinerary` and `get_activities` prompts, _as well as_ as a model chain between **gpt-3.5-turbo** and **gpt-4**.\n\nLet\'s run this with AIConfig:\n\nReplace `config.run` above with this:\n\n```python\nawait config.run("gen_itinerary", params={"order_by": "duration"}, options=inference_options, run_with_dependencies=True)\n```\n\nNotice how simple the syntax is to perform a fairly complex task - running 2 different prompts across 2 different models and chaining one\'s output as part of the input of another.\n\nThe code will just run `get_activities`, then pipe its output as an input to `gen_itinerary`, and finally run `gen_itinerary`.\n\n### Save the AIConfig\n\nLet\'s save the AIConfig back to disk, and serialize the outputs from the latest inference run as well:\n\n```python\n# Save the aiconfig to disk. and serialize outputs from the model run\nconfig.save(\'updated.aiconfig.json\', include_outputs=True)\n```\n\n### Edit `aiconfig` in a notebook editor\n\nWe can iterate on an `aiconfig` using a notebook-like editor called an **AI Workbook**. Now that we have an `aiconfig` file artifact that encapsulates the generative AI part of our application, we can iterate on it separately from the application code that uses it.\n\n1. Go to https://lastmileai.dev.\n2. Go to Workbooks page: https://lastmileai.dev/workbooks\n3. Click dropdown from \'+ New Workbook\' and select \'Create from AIConfig\'\n4. Upload `travel.aiconfig.json`\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/5d901493-bbda-4f8e-93c7-dd9a91bf242e\n\nTry out the workbook playground here: **[NYC Travel Workbook](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9)**\n\n> **We are working on a local editor that you can run yourself. For now, please use the hosted version on https://lastmileai.dev.**\n\n### Additional Guides\n\nThere is a lot you can do with `aiconfig`. We have several other tutorials to help get you started:\n\n- [Create an AIConfig from scratch](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig)\n- [Run a prompt](https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig)\n- [Pass data into prompts](https://aiconfig.lastmileai.dev/docs/overview/parameters)\n- [Prompt chains](https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain)\n- [Callbacks and monitoring](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\nHere are some example uses:\n\n- [CLI Chatbot](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT)\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [Chain of thought](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### OpenAI Introspection API\n\nIf you are already using OpenAI completion API\'s in your application, you can get started very quickly to start saving the messages in an `aiconfig`.\n\nUsage: see openai_wrapper.ipynb.\n\nNow you can continue using `openai` completion API as normal. When you want to save the config, just call `new_config.save()` and all your openai completion calls will get serialized to disk.\n\n> [**Detailed guide here**](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig#openai-api-python-wrapper)\n\n## Supported Models\n\nAIConfig supports the following models out of the box:\n\n- OpenAI chat models (GPT-3, GPT-3.5, GPT-4)\n- LLaMA2 (running locally)\n- Google PaLM models (PaLM chat)\n- Hugging Face text generation models (e.g. Mistral-7B)\n\n### Examples\n\n- [OpenAI](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n> If you need to use a model that isn\'t provided out of the box, you can implement a `ModelParser` for it (see [Extending AIConfig](#extending-aiconfig)). **We welcome [contributions](https://aiconfig.lastmileai.dev/docs/contributing)**\n\n## AIConfig Schema\n\n[AIConfig specification](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format)\n\n## AIConfig SDK\n\n> Read the [Usage Guide](https://aiconfig.lastmileai.dev/docs/usage-guide) for more details.\n\nThe AIConfig SDK supports CRUD operations for prompts, models, parameters and metadata. Here are some common examples.\n\nThe root interface is the `AIConfigRuntime` object. That is the entrypoint for interacting with an AIConfig programmatically.\n\nLet\'s go over a few key CRUD operations to give a glimpse.\n\n### AIConfig `create`\n\n```python\nconfig = AIConfigRuntime.create("aiconfig name", "description")\n```\n\n### Prompt `resolve`\n\n`resolve` deserializes an existing `Prompt` into the data object that its model expects.\n\n```python\nconfig.resolve("prompt_name", params)\n```\n\n`params` are overrides you can specify to resolve any `{{handlebars}}` templates in the prompt. See the `gen_itinerary` prompt in the Getting Started example.\n\n### Prompt `serialize`\n\n`serialize` is the inverse of `resolve` -- it serializes the data object that a model understands into a `Prompt` object that can be serialized into the `aiconfig` format.\n\n```python\nconfig.serialize("model_name", data, "prompt_name")\n```\n\n### Prompt `run`\n\n`run` is used to run inference for the specified `Prompt`.\n\n```python\nconfig.run("prompt_name", params)\n```\n\n### `run_with_dependencies`\n\nThis is a variant of `run` -- this re-runs all prompt dependencies.\nFor example, in [`travel.aiconfig.json`](#download-travelaiconfigjson), the `gen_itinerary` prompt references the output of the `get_activities` prompt using `{{get_activities.output}}`.\n\nRunning this function will first execute `get_activities`, and use its output to resolve the `gen_itinerary` prompt before executing it.\nThis is transitive, so it computes the Directed Acyclic Graph of dependencies to execute. Complex relationships can be modeled this way.\n\n```python\nconfig.run_with_dependencies("gen_itinerary")\n```\n\n### Updating metadata and parameters\n\nUse the `get/set_metadata` and `get/set_parameter` methods to interact with metadata and parameters (`set_parameter` is just syntactic sugar to update `"metadata.parameters"`)\n\n```python\nconfig.set_metadata("key", data, "prompt_name")\n```\n\nNote: if `"prompt_name"` is specified, the metadata is updated specifically for that prompt. Otherwise, the global metadata is updated.\n\n### Register new `ModelParser`\n\nUse the `AIConfigRuntime.register_model_parser` if you want to use a different `ModelParser`, or configure AIConfig to work with an additional model.\n\nAIConfig uses the model name string to retrieve the right `ModelParser` for a given Prompt (see `AIConfigRuntime.get_model_parser`), so you can register a different ModelParser for the same ID to override which `ModelParser` handles a Prompt.\n\nFor example, suppose I want to use `MyOpenAIModelParser` to handle `gpt-4` prompts. I can do the following at the start of my application:\n\n```python\nAIConfigRuntime.register_model_parser(myModelParserInstance, ["gpt-4"])\n```\n\n### Callback events\n\nUse callback events to trace and monitor what\'s going on -- helpful for debugging and observability.\n\n```python\nfrom aiconfig import AIConfigRuntime, CallbackEvent, CallbackManager\nconfig = AIConfigRuntime.load(\'aiconfig.json\')\n\nasync def my_custom_callback(event: CallbackEvent) -> None:\n  print(f"Event triggered: {event.name}", event)\n\ncallback_manager = CallbackManager([my_custom_callback])\nconfig.set_callback_manager(callback_manager)\n\nawait config.run("prompt_name")\n```\n\n[**Read more** here](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\n## Extensibility\n\nAIConfig is designed to be customized and extended for your use-case. The [Extensibility](/docs/extensibility) guide goes into more detail.\n\nCurrently, there are 3 core ways to extend AIConfig:\n\n1. [Supporting other models](https://aiconfig.lastmileai.dev/docs/extensibility#1-bring-your-own-model) - define a ModelParser extension\n2. [Callback event handlers](https://aiconfig.lastmileai.dev/docs/extensibility#2-callback-handlers) - tracing and monitoring\n3. [Custom metadata](https://aiconfig.lastmileai.dev/docs/extensibility#3-custom-metadata) - save custom fields in `aiconfig`\n\n## Contributing to `aiconfig`\n\nThis is our first open-source project and we\'d love your help.\n\nSee our [contributing guidelines](https://aiconfig.lastmileai.dev/docs/contributing) -- we would especially love help adding support for additional models that the community wants.\n\n## Cookbooks\n\nWe provide several guides to demonstrate the power of `aiconfig`.\n\n> **See the [`cookbooks`](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks) folder for examples to clone.**\n\n### Chatbot\n\n- [Wizard GPT](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT) - speak to a wizard on your CLI\n\n- [CLI-mate](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Cli-Mate) - help you make code-mods interactively on your codebase.\n\n### Retrieval Augmented Generated (RAG)\n\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n\nAt its core, RAG is about passing data into prompts. Read how to [pass data](/docs/overview/parameters) with AIConfig.\n\n### Function calling\n\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n\n### Prompt routing\n\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n\n### Chain of Thought\n\nA variant of chain-of-thought is Chain of Verification, used to help reduce hallucinations. Check out the aiconfig cookbook for CoVe:\n\n- [Chain of Verification](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### Using local LLaMA2 with `aiconfig`\n\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n\n### Hugging Face text generation\n\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n\n### Google PaLM\n\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n## Roadmap\n\nThis project is under active development.\n\nIf you\'d like to help, please see the [contributing guidelines](#contributing-to-aiconfig).\n\nPlease create issues for additional capabilities you\'d like to see.\n\nHere\'s what\'s already on our roadmap:\n\n- Evaluation interfaces: allow `aiconfig` artifacts to be evaluated with user-defined eval functions.\n  - We are also considering integrating with existing evaluation frameworks.\n- Local editor for `aiconfig`: enable you to interact with aiconfigs more intuitively.\n- OpenAI Assistants API support\n- Multi-modal ModelParsers:\n  - GPT4-V support\n  - DALLE-3\n  - Whisper\n  - HuggingFace image generation\n\n## FAQs\n\n### How should I edit an `aiconfig` file?\n\nEditing a configshould be done either programmatically via SDK or via the UI (workbooks):\n\n- [Programmatic](https://github.com/lastmile-ai/aiconfig/blob/main/cookbooks/Create-AIConfig-Programmatically/create_aiconfig_programmatically.ipynb) editing.\n\n- [Edit with a workbook](#edit-aiconfig-in-a-notebook-editor) editor: this is similar to editing an ipynb file as a notebook (most people never touch the json ipynb directly)\n\nYou should only edit the `aiconfig` by hand for minor modifications, like tweaking a prompt string or updating some metadata.\n\n### Does this support custom endpoints?\n\nOut of the box, AIConfig already supports all OpenAI GPT\\* models, Googles PaLM model and any textgeneration model on Hugging Face (like Mistral). See [Supported Models](#supported-models) for more details.\n\nAdditionally, you can install `aiconfig` [extensions](https://github.com/lastmile-ai/aiconfig/tree/main/extensions) for additional models (see question below).\n\n### Is OpenAI function calling supported?\n\nYes. [This example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI) goes through how to do it.\n\nWe are also working on adding support for the Assistants API.\n\n### How can I use aiconfig with my own model endpoint?\n\nModel support is implemented as ModelParsers in the AIConfig SDK, and the idea is that anyone, including you, can define a ModelParser (and even publish it as an extension package).\n\nAll thats needed to use a model with AIConfig is a ModelParser that knows\n\n- how to serialize data from a model into the aiconfig format\n- how to deserialize data from an aiconfig into the type the model expects\n- how to run inference for model.\n\nFor more details, see [Extensibility](https://aiconfig.lastmileai.dev/docs/extensibility).\n\n### When should I store outputs in an `aiconfig`?\n\nThe `AIConfigRuntime` object is used to interact with an aiconfig programmatically (see [SDK usage guide](#aiconfig-sdk)). As you run prompts, this object keeps track of the outputs returned from the model.\n\nYou can choose to serialize these outputs back into the `aiconfig` by using the `config.save(include_outputs=True)` API. This can be useful for preserving context -- think of it like session state.\n\nFor example, you can use aiconfig to create a chatbot, and use the same format to save the chat history so it can be resumed for the next session.\n\nYou can also choose to save outputs to a _different_ file than the original config -- `config.save("history.aiconfig.json", include_outputs=True)`.\n\n### Why should I use `aiconfig` instead of things like [configurator](https://pypi.org/project/configurator/)?\n\nIt helps to have a [standardized format](http://aiconfig.lastmileai.dev/docs/overview/ai-config-format) specifically for storing generative AI prompts, inference results, model parameters and arbitrary metadata, as opposed to a general-purpose configuration schema.\n\nWith that standardization, you just need a layer that knows how to serialize/deserialize from that format into whatever the inference endpoints require.\n\n### This looks similar to `ipynb` for Jupyter notebooks\n\nWe believe that notebooks are a perfect iteration environment for generative AI -- they are flexible, multi-modal, and collaborative.\n\nThe multi-modality and flexibility offered by notebooks and [`ipynb`](https://ipython.org/ipython-doc/3/notebook/nbformat.html) offers a good interaction model for generative AI. The `aiconfig` file format is extensible like `ipynb`, and AI Workbook editor allows rapid iteration in a notebook-like IDE.\n\n_AI Workbooks are to AIConfig what Jupyter notebooks are to `ipynb`_\n\nThere are 2 areas where we are going beyond what notebooks offer:\n\n1. `aiconfig` is more **source-control friendly** than `ipynb`. `ipynb` stores binary data (images, etc.) by encoding it in the file, while `aiconfig` recommends using file URI references instead.\n2. `aiconfig` can be imported and **connected to application code** using the AIConfig SDK.\n', 'repo_name': 'aiconfig'}, 'options': None, 'kwargs': {}} ts_ns=1702175485063260400
2023-12-09 21:31:40,215 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '<div align="center"><picture>\n  <img alt="aiconfig" src="aiconfig-docs/static/img/readme_logo.png" />\n</picture></div>\n<br/>\n\n![Python](https://github.com/lastmile-ai/aiconfig/actions/workflows/main_python.yml/badge.svg)\n![Node](https://github.com/lastmile-ai/aiconfig/actions/workflows/main-typescript.yml/badge.svg)\n![Docs](https://github.com/lastmile-ai/aiconfig/actions/workflows/test-deploy-docs.yml/badge.svg)\n[![Discord](<https://img.shields.io/badge/Discord-LastMile%20AI-Blue?color=rgb(37%2C%20150%2C%20190)>)](https://discord.gg/qMqgzDae)\n\n> Full documentation: **[aiconfig.lastmileai.dev](https://aiconfig.lastmileai.dev/)**\n\n## Overview\n\nAIConfig saves prompts, models and model parameters as source control friendly configs. This allows you to iterate on prompts and model parameters _separately from your application code_.\n\n1. **Prompts as configs**: a [standardized JSON format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to store generative AI model settings, prompt inputs/outputs, and flexible metadata.\n2. **Model-agnostic SDK**: Python & Node SDKs to use `aiconfig` in your application code. AIConfig is designed to be **model-agnostic** and **multi-modal**, so you can extend it to work with any generative AI model, including text, image and audio.\n3. **AI Workbook editor**: A [notebook-like playground](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to edit `aiconfig` files visually, run prompts, tweak models and model settings, and chain things together.\n\n### What problem it solves\n\nToday, application code is tightly coupled with the gen AI settings for the application -- prompts, parameters, and model-specific logic is all jumbled in with app code.\n\n- results in increased complexity\n- makes it hard to iterate on the prompts or try different models easily\n- makes it hard to evaluate prompt/model performance\n\nAIConfig helps unwind complexity by separating prompts, model parameters, and model-specific logic from your application.\n\n- simplifies application code -- simply call `config.run()`\n- open the `aiconfig` in a playground to iterate quickly\n- version control and evaluate the `aiconfig` - it\'s the AI artifact for your application.\n\n![AIConfig flow](aiconfig-docs/static/img/aiconfig_dataflow.png)\n\n### Quicknav\n\n<ul style="margin-bottom:0; padding-bottom:0;">\n  <li><a href="#install">Getting Started</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig">Create an AIConfig</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig">Run a prompt</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/parameters">Pass data into prompts</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain">Prompt Chains</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig">Callbacks and monitoring</a></li>\n  </ul>\n  <li><a href="#aiconfig-sdk">SDK Cheatsheet</a></li>\n  <li><a href="#cookbooks">Cookbooks and guides</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT">CLI Chatbot</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig">RAG with AIConfig</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing">Prompt routing</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI">OpenAI function calling</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification">Chain of Verification</a></li>\n  </ul>\n  <li><a href="#supported-models">Supported models</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama">LLaMA2 example</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace">Hugging Face (Mistral-7B) example</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency">PaLM</a></li>\n  </ul>\n  <li><a href="#extensibility">Extensibility</a></li>\n  <li><a href="#contributing-to-aiconfig">Contributing</a></li>\n  <li><a href="#roadmap">Roadmap</a></li>\n  <li><a href="#faqs">FAQ</a></li>\n</ul>\n\n## Features\n\n- [x] **Source-control friendly** [`aiconfig` format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to save prompts and model settings, which you can use for evaluation, reproducibility and simplifying your application code.\n- [x] **Multi-modal and model agnostic**. Use with any model, and serialize/deserialize data with the same `aiconfig` format.\n- [x] **Prompt chaining and parameterization** with [{{handlebars}}](https://handlebarsjs.com/) templating syntax, allowing you to pass dynamic data into prompts (as well as between prompts).\n- [x] **Streaming** supported out of the box, allowing you to get playground-like streaming wherever you use `aiconfig`.\n- [x] **Notebook editor**. [AI Workbooks editor](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to visually create your `aiconfig`, and use the SDK to connect it to your application code.\n\n## Install\n\nInstall with your favorite package manager for Node or Python.\n\n### Node.js\n\n#### `npm` or `yarn`\n\n```bash\nnpm install aiconfig\n```\n\n```bash\nyarn add aiconfig\n```\n\n### Python\n\n#### `pip3` or `poetry`\n\n```bash\npip3 install python-aiconfig\n```\n\n```bash\npoetry add python-aiconfig\n```\n\n[Detailed installation instructions](https://aiconfig.lastmileai.dev/docs/getting-started/#installation).\n\n### Set your OpenAI API Key\n\n> **Note**: Make sure to specify the API keys (such as [`OPENAI_API_KEY`](https://platform.openai.com/api-keys)) in your environment before proceeding.\n\nIn your CLI, set the environment variable:\n\n```bash\nexport OPENAI_API_KEY=my_key\n```\n\n## Getting Started\n\n> We cover Python instructions here, for Node.js please see the [detailed Getting Started guide](https://aiconfig.lastmileai.dev/docs/getting-started)\n\nIn this quickstart, you will create a customizable NYC travel itinerary using `aiconfig`.\n\nThis AIConfig contains a prompt chain to get a list of travel activities from an LLM and then generate an itinerary in an order specified by the user.\n\n> **Link to tutorial code: [here](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Getting-Started)**\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/805173d1-0f83-44c5-b570-c776bb7dba66\n\n### Download `travel.aiconfig.json`\n\n> **Note**: Don\'t worry if you don\'t understand all the pieces of this yet, we\'ll go over it step by step.\n\n```json\n{\n  "name": "NYC Trip Planner",\n  "description": "Intrepid explorer with ChatGPT and AIConfig",\n  "schema_version": "latest",\n  "metadata": {\n    "models": {\n      "gpt-3.5-turbo": {\n        "model": "gpt-3.5-turbo",\n        "top_p": 1,\n        "temperature": 1\n      },\n      "gpt-4": {\n        "model": "gpt-4",\n        "max_tokens": 3000,\n        "system_prompt": "You are an expert travel coordinator with exquisite taste."\n      }\n    },\n    "default_model": "gpt-3.5-turbo"\n  },\n  "prompts": [\n    {\n      "name": "get_activities",\n      "input": "Tell me 10 fun attractions to do in NYC."\n    },\n    {\n      "name": "gen_itinerary",\n      "input": "Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}.",\n      "metadata": {\n        "model": "gpt-4",\n        "parameters": {\n          "order_by": "geographic location"\n        }\n      }\n    }\n  ]\n}\n```\n\n### Run the `get_activities` prompt.\n\nYou don\'t need to worry about how to run inference for the model; it\'s all handled by AIConfig. The prompt runs with gpt-3.5-turbo since that is the `default_model` for this AIConfig.\n\nCreate a new file called `app.py` and and enter the following code:\n\n```python\nimport asyncio\nfrom aiconfig import AIConfigRuntime, InferenceOptions\n\nasync def main():\n  # Load the aiconfig\n  config = AIConfigRuntime.load(\'travel.aiconfig.json\')\n\n  # Run a single prompt (with streaming)\n  inference_options = InferenceOptions(stream=True)\n  await config.run("get_activities", options=inference_options)\n\nasyncio.run(main())\n```\n\nNow run this in your terminal with the command:\n\n```bash\npython3 app.py\n```\n\n### Run the `gen_itinerary` prompt.\n\nIn your `app.py` file, change the last line to below:\n\n```python\nawait config.run("gen_itinerary", params=None, options=inference_options)\n```\n\nRe-run the command in your terminal:\n\n```bash\npython3 app.py\n```\n\nThis prompt depends on the output of `get_activities`. It also takes in parameters (user input) to determine the customized itinerary.\n\nLet\'s take a closer look:\n\n**`gen_itinerary` prompt:**\n\n```\n"Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}."\n```\n\n**prompt metadata:**\n\n```json\n{\n  "metadata": {\n    "model": "gpt-4",\n    "parameters": {\n      "order_by": "geographic location"\n    }\n  }\n}\n```\n\nObserve the following:\n\n1. The prompt depends on the output of the `get_activities` prompt.\n2. It also depends on an `order_by` parameter (using {{handlebars}} syntax)\n3. It uses **gpt-4**, whereas the `get_activities` prompt it depends on uses **gpt-3.5-turbo**.\n\n> Effectively, this is a prompt chain between `gen_itinerary` and `get_activities` prompts, _as well as_ as a model chain between **gpt-3.5-turbo** and **gpt-4**.\n\nLet\'s run this with AIConfig:\n\nReplace `config.run` above with this:\n\n```python\nawait config.run("gen_itinerary", params={"order_by": "duration"}, options=inference_options, run_with_dependencies=True)\n```\n\nNotice how simple the syntax is to perform a fairly complex task - running 2 different prompts across 2 different models and chaining one\'s output as part of the input of another.\n\nThe code will just run `get_activities`, then pipe its output as an input to `gen_itinerary`, and finally run `gen_itinerary`.\n\n### Save the AIConfig\n\nLet\'s save the AIConfig back to disk, and serialize the outputs from the latest inference run as well:\n\n```python\n# Save the aiconfig to disk. and serialize outputs from the model run\nconfig.save(\'updated.aiconfig.json\', include_outputs=True)\n```\n\n### Edit `aiconfig` in a notebook editor\n\nWe can iterate on an `aiconfig` using a notebook-like editor called an **AI Workbook**. Now that we have an `aiconfig` file artifact that encapsulates the generative AI part of our application, we can iterate on it separately from the application code that uses it.\n\n1. Go to https://lastmileai.dev.\n2. Go to Workbooks page: https://lastmileai.dev/workbooks\n3. Click dropdown from \'+ New Workbook\' and select \'Create from AIConfig\'\n4. Upload `travel.aiconfig.json`\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/5d901493-bbda-4f8e-93c7-dd9a91bf242e\n\nTry out the workbook playground here: **[NYC Travel Workbook](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9)**\n\n> **We are working on a local editor that you can run yourself. For now, please use the hosted version on https://lastmileai.dev.**\n\n### Additional Guides\n\nThere is a lot you can do with `aiconfig`. We have several other tutorials to help get you started:\n\n- [Create an AIConfig from scratch](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig)\n- [Run a prompt](https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig)\n- [Pass data into prompts](https://aiconfig.lastmileai.dev/docs/overview/parameters)\n- [Prompt chains](https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain)\n- [Callbacks and monitoring](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\nHere are some example uses:\n\n- [CLI Chatbot](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT)\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [Chain of thought](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### OpenAI Introspection API\n\nIf you are already using OpenAI completion API\'s in your application, you can get started very quickly to start saving the messages in an `aiconfig`.\n\nUsage: see openai_wrapper.ipynb.\n\nNow you can continue using `openai` completion API as normal. When you want to save the config, just call `new_config.save()` and all your openai completion calls will get serialized to disk.\n\n> [**Detailed guide here**](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig#openai-api-python-wrapper)\n\n## Supported Models\n\nAIConfig supports the following models out of the box:\n\n- OpenAI chat models (GPT-3, GPT-3.5, GPT-4)\n- LLaMA2 (running locally)\n- Google PaLM models (PaLM chat)\n- Hugging Face text generation models (e.g. Mistral-7B)\n\n### Examples\n\n- [OpenAI](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n> If you need to use a model that isn\'t provided out of the box, you can implement a `ModelParser` for it (see [Extending AIConfig](#extending-aiconfig)). **We welcome [contributions](https://aiconfig.lastmileai.dev/docs/contributing)**\n\n## AIConfig Schema\n\n[AIConfig specification](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format)\n\n## AIConfig SDK\n\n> Read the [Usage Guide](https://aiconfig.lastmileai.dev/docs/usage-guide) for more details.\n\nThe AIConfig SDK supports CRUD operations for prompts, models, parameters and metadata. Here are some common examples.\n\nThe root interface is the `AIConfigRuntime` object. That is the entrypoint for interacting with an AIConfig programmatically.\n\nLet\'s go over a few key CRUD operations to give a glimpse.\n\n### AIConfig `create`\n\n```python\nconfig = AIConfigRuntime.create("aiconfig name", "description")\n```\n\n### Prompt `resolve`\n\n`resolve` deserializes an existing `Prompt` into the data object that its model expects.\n\n```python\nconfig.resolve("prompt_name", params)\n```\n\n`params` are overrides you can specify to resolve any `{{handlebars}}` templates in the prompt. See the `gen_itinerary` prompt in the Getting Started example.\n\n### Prompt `serialize`\n\n`serialize` is the inverse of `resolve` -- it serializes the data object that a model understands into a `Prompt` object that can be serialized into the `aiconfig` format.\n\n```python\nconfig.serialize("model_name", data, "prompt_name")\n```\n\n### Prompt `run`\n\n`run` is used to run inference for the specified `Prompt`.\n\n```python\nconfig.run("prompt_name", params)\n```\n\n### `run_with_dependencies`\n\nThis is a variant of `run` -- this re-runs all prompt dependencies.\nFor example, in [`travel.aiconfig.json`](#download-travelaiconfigjson), the `gen_itinerary` prompt references the output of the `get_activities` prompt using `{{get_activities.output}}`.\n\nRunning this function will first execute `get_activities`, and use its output to resolve the `gen_itinerary` prompt before executing it.\nThis is transitive, so it computes the Directed Acyclic Graph of dependencies to execute. Complex relationships can be modeled this way.\n\n```python\nconfig.run_with_dependencies("gen_itinerary")\n```\n\n### Updating metadata and parameters\n\nUse the `get/set_metadata` and `get/set_parameter` methods to interact with metadata and parameters (`set_parameter` is just syntactic sugar to update `"metadata.parameters"`)\n\n```python\nconfig.set_metadata("key", data, "prompt_name")\n```\n\nNote: if `"prompt_name"` is specified, the metadata is updated specifically for that prompt. Otherwise, the global metadata is updated.\n\n### Register new `ModelParser`\n\nUse the `AIConfigRuntime.register_model_parser` if you want to use a different `ModelParser`, or configure AIConfig to work with an additional model.\n\nAIConfig uses the model name string to retrieve the right `ModelParser` for a given Prompt (see `AIConfigRuntime.get_model_parser`), so you can register a different ModelParser for the same ID to override which `ModelParser` handles a Prompt.\n\nFor example, suppose I want to use `MyOpenAIModelParser` to handle `gpt-4` prompts. I can do the following at the start of my application:\n\n```python\nAIConfigRuntime.register_model_parser(myModelParserInstance, ["gpt-4"])\n```\n\n### Callback events\n\nUse callback events to trace and monitor what\'s going on -- helpful for debugging and observability.\n\n```python\nfrom aiconfig import AIConfigRuntime, CallbackEvent, CallbackManager\nconfig = AIConfigRuntime.load(\'aiconfig.json\')\n\nasync def my_custom_callback(event: CallbackEvent) -> None:\n  print(f"Event triggered: {event.name}", event)\n\ncallback_manager = CallbackManager([my_custom_callback])\nconfig.set_callback_manager(callback_manager)\n\nawait config.run("prompt_name")\n```\n\n[**Read more** here](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\n## Extensibility\n\nAIConfig is designed to be customized and extended for your use-case. The [Extensibility](/docs/extensibility) guide goes into more detail.\n\nCurrently, there are 3 core ways to extend AIConfig:\n\n1. [Supporting other models](https://aiconfig.lastmileai.dev/docs/extensibility#1-bring-your-own-model) - define a ModelParser extension\n2. [Callback event handlers](https://aiconfig.lastmileai.dev/docs/extensibility#2-callback-handlers) - tracing and monitoring\n3. [Custom metadata](https://aiconfig.lastmileai.dev/docs/extensibility#3-custom-metadata) - save custom fields in `aiconfig`\n\n## Contributing to `aiconfig`\n\nThis is our first open-source project and we\'d love your help.\n\nSee our [contributing guidelines](https://aiconfig.lastmileai.dev/docs/contributing) -- we would especially love help adding support for additional models that the community wants.\n\n## Cookbooks\n\nWe provide several guides to demonstrate the power of `aiconfig`.\n\n> **See the [`cookbooks`](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks) folder for examples to clone.**\n\n### Chatbot\n\n- [Wizard GPT](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT) - speak to a wizard on your CLI\n\n- [CLI-mate](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Cli-Mate) - help you make code-mods interactively on your codebase.\n\n### Retrieval Augmented Generated (RAG)\n\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n\nAt its core, RAG is about passing data into prompts. Read how to [pass data](/docs/overview/parameters) with AIConfig.\n\n### Function calling\n\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n\n### Prompt routing\n\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n\n### Chain of Thought\n\nA variant of chain-of-thought is Chain of Verification, used to help reduce hallucinations. Check out the aiconfig cookbook for CoVe:\n\n- [Chain of Verification](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### Using local LLaMA2 with `aiconfig`\n\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n\n### Hugging Face text generation\n\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n\n### Google PaLM\n\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n## Roadmap\n\nThis project is under active development.\n\nIf you\'d like to help, please see the [contributing guidelines](#contributing-to-aiconfig).\n\nPlease create issues for additional capabilities you\'d like to see.\n\nHere\'s what\'s already on our roadmap:\n\n- Evaluation interfaces: allow `aiconfig` artifacts to be evaluated with user-defined eval functions.\n  - We are also considering integrating with existing evaluation frameworks.\n- Local editor for `aiconfig`: enable you to interact with aiconfigs more intuitively.\n- OpenAI Assistants API support\n- Multi-modal ModelParsers:\n  - GPT4-V support\n  - DALLE-3\n  - Whisper\n  - HuggingFace image generation\n\n## FAQs\n\n### How should I edit an `aiconfig` file?\n\nEditing a configshould be done either programmatically via SDK or via the UI (workbooks):\n\n- [Programmatic](https://github.com/lastmile-ai/aiconfig/blob/main/cookbooks/Create-AIConfig-Programmatically/create_aiconfig_programmatically.ipynb) editing.\n\n- [Edit with a workbook](#edit-aiconfig-in-a-notebook-editor) editor: this is similar to editing an ipynb file as a notebook (most people never touch the json ipynb directly)\n\nYou should only edit the `aiconfig` by hand for minor modifications, like tweaking a prompt string or updating some metadata.\n\n### Does this support custom endpoints?\n\nOut of the box, AIConfig already supports all OpenAI GPT\\* models, Googles PaLM model and any textgeneration model on Hugging Face (like Mistral). See [Supported Models](#supported-models) for more details.\n\nAdditionally, you can install `aiconfig` [extensions](https://github.com/lastmile-ai/aiconfig/tree/main/extensions) for additional models (see question below).\n\n### Is OpenAI function calling supported?\n\nYes. [This example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI) goes through how to do it.\n\nWe are also working on adding support for the Assistants API.\n\n### How can I use aiconfig with my own model endpoint?\n\nModel support is implemented as ModelParsers in the AIConfig SDK, and the idea is that anyone, including you, can define a ModelParser (and even publish it as an extension package).\n\nAll thats needed to use a model with AIConfig is a ModelParser that knows\n\n- how to serialize data from a model into the aiconfig format\n- how to deserialize data from an aiconfig into the type the model expects\n- how to run inference for model.\n\nFor more details, see [Extensibility](https://aiconfig.lastmileai.dev/docs/extensibility).\n\n### When should I store outputs in an `aiconfig`?\n\nThe `AIConfigRuntime` object is used to interact with an aiconfig programmatically (see [SDK usage guide](#aiconfig-sdk)). As you run prompts, this object keeps track of the outputs returned from the model.\n\nYou can choose to serialize these outputs back into the `aiconfig` by using the `config.save(include_outputs=True)` API. This can be useful for preserving context -- think of it like session state.\n\nFor example, you can use aiconfig to create a chatbot, and use the same format to save the chat history so it can be resumed for the next session.\n\nYou can also choose to save outputs to a _different_ file than the original config -- `config.save("history.aiconfig.json", include_outputs=True)`.\n\n### Why should I use `aiconfig` instead of things like [configurator](https://pypi.org/project/configurator/)?\n\nIt helps to have a [standardized format](http://aiconfig.lastmileai.dev/docs/overview/ai-config-format) specifically for storing generative AI prompts, inference results, model parameters and arbitrary metadata, as opposed to a general-purpose configuration schema.\n\nWith that standardization, you just need a layer that knows how to serialize/deserialize from that format into whatever the inference endpoints require.\n\n### This looks similar to `ipynb` for Jupyter notebooks\n\nWe believe that notebooks are a perfect iteration environment for generative AI -- they are flexible, multi-modal, and collaborative.\n\nThe multi-modality and flexibility offered by notebooks and [`ipynb`](https://ipython.org/ipython-doc/3/notebook/nbformat.html) offers a good interaction model for generative AI. The `aiconfig` file format is extensible like `ipynb`, and AI Workbook editor allows rapid iteration in a notebook-like IDE.\n\n_AI Workbooks are to AIConfig what Jupyter notebooks are to `ipynb`_\n\nThere are 2 areas where we are going beyond what notebooks offer:\n\n1. `aiconfig` is more **source-control friendly** than `ipynb`. `ipynb` stores binary data (images, etc.) by encoding it in the file, while `aiconfig` recommends using file URI references instead.\n2. `aiconfig` can be imported and **connected to application code** using the AIConfig SDK.\n', 'repo_name': 'aiconfig'}} ts_ns=1702175485063260400
2023-12-09 21:31:40,215 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '<div align="center"><picture>\n  <img alt="aiconfig" src="aiconfig-docs/static/img/readme_logo.png" />\n</picture></div>\n<br/>\n\n![Python](https://github.com/lastmile-ai/aiconfig/actions/workflows/main_python.yml/badge.svg)\n![Node](https://github.com/lastmile-ai/aiconfig/actions/workflows/main-typescript.yml/badge.svg)\n![Docs](https://github.com/lastmile-ai/aiconfig/actions/workflows/test-deploy-docs.yml/badge.svg)\n[![Discord](<https://img.shields.io/badge/Discord-LastMile%20AI-Blue?color=rgb(37%2C%20150%2C%20190)>)](https://discord.gg/qMqgzDae)\n\n> Full documentation: **[aiconfig.lastmileai.dev](https://aiconfig.lastmileai.dev/)**\n\n## Overview\n\nAIConfig saves prompts, models and model parameters as source control friendly configs. This allows you to iterate on prompts and model parameters _separately from your application code_.\n\n1. **Prompts as configs**: a [standardized JSON format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to store generative AI model settings, prompt inputs/outputs, and flexible metadata.\n2. **Model-agnostic SDK**: Python & Node SDKs to use `aiconfig` in your application code. AIConfig is designed to be **model-agnostic** and **multi-modal**, so you can extend it to work with any generative AI model, including text, image and audio.\n3. **AI Workbook editor**: A [notebook-like playground](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to edit `aiconfig` files visually, run prompts, tweak models and model settings, and chain things together.\n\n### What problem it solves\n\nToday, application code is tightly coupled with the gen AI settings for the application -- prompts, parameters, and model-specific logic is all jumbled in with app code.\n\n- results in increased complexity\n- makes it hard to iterate on the prompts or try different models easily\n- makes it hard to evaluate prompt/model performance\n\nAIConfig helps unwind complexity by separating prompts, model parameters, and model-specific logic from your application.\n\n- simplifies application code -- simply call `config.run()`\n- open the `aiconfig` in a playground to iterate quickly\n- version control and evaluate the `aiconfig` - it\'s the AI artifact for your application.\n\n![AIConfig flow](aiconfig-docs/static/img/aiconfig_dataflow.png)\n\n### Quicknav\n\n<ul style="margin-bottom:0; padding-bottom:0;">\n  <li><a href="#install">Getting Started</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig">Create an AIConfig</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig">Run a prompt</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/parameters">Pass data into prompts</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain">Prompt Chains</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig">Callbacks and monitoring</a></li>\n  </ul>\n  <li><a href="#aiconfig-sdk">SDK Cheatsheet</a></li>\n  <li><a href="#cookbooks">Cookbooks and guides</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT">CLI Chatbot</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig">RAG with AIConfig</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing">Prompt routing</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI">OpenAI function calling</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification">Chain of Verification</a></li>\n  </ul>\n  <li><a href="#supported-models">Supported models</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama">LLaMA2 example</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace">Hugging Face (Mistral-7B) example</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency">PaLM</a></li>\n  </ul>\n  <li><a href="#extensibility">Extensibility</a></li>\n  <li><a href="#contributing-to-aiconfig">Contributing</a></li>\n  <li><a href="#roadmap">Roadmap</a></li>\n  <li><a href="#faqs">FAQ</a></li>\n</ul>\n\n## Features\n\n- [x] **Source-control friendly** [`aiconfig` format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to save prompts and model settings, which you can use for evaluation, reproducibility and simplifying your application code.\n- [x] **Multi-modal and model agnostic**. Use with any model, and serialize/deserialize data with the same `aiconfig` format.\n- [x] **Prompt chaining and parameterization** with [{{handlebars}}](https://handlebarsjs.com/) templating syntax, allowing you to pass dynamic data into prompts (as well as between prompts).\n- [x] **Streaming** supported out of the box, allowing you to get playground-like streaming wherever you use `aiconfig`.\n- [x] **Notebook editor**. [AI Workbooks editor](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to visually create your `aiconfig`, and use the SDK to connect it to your application code.\n\n## Install\n\nInstall with your favorite package manager for Node or Python.\n\n### Node.js\n\n#### `npm` or `yarn`\n\n```bash\nnpm install aiconfig\n```\n\n```bash\nyarn add aiconfig\n```\n\n### Python\n\n#### `pip3` or `poetry`\n\n```bash\npip3 install python-aiconfig\n```\n\n```bash\npoetry add python-aiconfig\n```\n\n[Detailed installation instructions](https://aiconfig.lastmileai.dev/docs/getting-started/#installation).\n\n### Set your OpenAI API Key\n\n> **Note**: Make sure to specify the API keys (such as [`OPENAI_API_KEY`](https://platform.openai.com/api-keys)) in your environment before proceeding.\n\nIn your CLI, set the environment variable:\n\n```bash\nexport OPENAI_API_KEY=my_key\n```\n\n## Getting Started\n\n> We cover Python instructions here, for Node.js please see the [detailed Getting Started guide](https://aiconfig.lastmileai.dev/docs/getting-started)\n\nIn this quickstart, you will create a customizable NYC travel itinerary using `aiconfig`.\n\nThis AIConfig contains a prompt chain to get a list of travel activities from an LLM and then generate an itinerary in an order specified by the user.\n\n> **Link to tutorial code: [here](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Getting-Started)**\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/805173d1-0f83-44c5-b570-c776bb7dba66\n\n### Download `travel.aiconfig.json`\n\n> **Note**: Don\'t worry if you don\'t understand all the pieces of this yet, we\'ll go over it step by step.\n\n```json\n{\n  "name": "NYC Trip Planner",\n  "description": "Intrepid explorer with ChatGPT and AIConfig",\n  "schema_version": "latest",\n  "metadata": {\n    "models": {\n      "gpt-3.5-turbo": {\n        "model": "gpt-3.5-turbo",\n        "top_p": 1,\n        "temperature": 1\n      },\n      "gpt-4": {\n        "model": "gpt-4",\n        "max_tokens": 3000,\n        "system_prompt": "You are an expert travel coordinator with exquisite taste."\n      }\n    },\n    "default_model": "gpt-3.5-turbo"\n  },\n  "prompts": [\n    {\n      "name": "get_activities",\n      "input": "Tell me 10 fun attractions to do in NYC."\n    },\n    {\n      "name": "gen_itinerary",\n      "input": "Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}.",\n      "metadata": {\n        "model": "gpt-4",\n        "parameters": {\n          "order_by": "geographic location"\n        }\n      }\n    }\n  ]\n}\n```\n\n### Run the `get_activities` prompt.\n\nYou don\'t need to worry about how to run inference for the model; it\'s all handled by AIConfig. The prompt runs with gpt-3.5-turbo since that is the `default_model` for this AIConfig.\n\nCreate a new file called `app.py` and and enter the following code:\n\n```python\nimport asyncio\nfrom aiconfig import AIConfigRuntime, InferenceOptions\n\nasync def main():\n  # Load the aiconfig\n  config = AIConfigRuntime.load(\'travel.aiconfig.json\')\n\n  # Run a single prompt (with streaming)\n  inference_options = InferenceOptions(stream=True)\n  await config.run("get_activities", options=inference_options)\n\nasyncio.run(main())\n```\n\nNow run this in your terminal with the command:\n\n```bash\npython3 app.py\n```\n\n### Run the `gen_itinerary` prompt.\n\nIn your `app.py` file, change the last line to below:\n\n```python\nawait config.run("gen_itinerary", params=None, options=inference_options)\n```\n\nRe-run the command in your terminal:\n\n```bash\npython3 app.py\n```\n\nThis prompt depends on the output of `get_activities`. It also takes in parameters (user input) to determine the customized itinerary.\n\nLet\'s take a closer look:\n\n**`gen_itinerary` prompt:**\n\n```\n"Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}."\n```\n\n**prompt metadata:**\n\n```json\n{\n  "metadata": {\n    "model": "gpt-4",\n    "parameters": {\n      "order_by": "geographic location"\n    }\n  }\n}\n```\n\nObserve the following:\n\n1. The prompt depends on the output of the `get_activities` prompt.\n2. It also depends on an `order_by` parameter (using {{handlebars}} syntax)\n3. It uses **gpt-4**, whereas the `get_activities` prompt it depends on uses **gpt-3.5-turbo**.\n\n> Effectively, this is a prompt chain between `gen_itinerary` and `get_activities` prompts, _as well as_ as a model chain between **gpt-3.5-turbo** and **gpt-4**.\n\nLet\'s run this with AIConfig:\n\nReplace `config.run` above with this:\n\n```python\nawait config.run("gen_itinerary", params={"order_by": "duration"}, options=inference_options, run_with_dependencies=True)\n```\n\nNotice how simple the syntax is to perform a fairly complex task - running 2 different prompts across 2 different models and chaining one\'s output as part of the input of another.\n\nThe code will just run `get_activities`, then pipe its output as an input to `gen_itinerary`, and finally run `gen_itinerary`.\n\n### Save the AIConfig\n\nLet\'s save the AIConfig back to disk, and serialize the outputs from the latest inference run as well:\n\n```python\n# Save the aiconfig to disk. and serialize outputs from the model run\nconfig.save(\'updated.aiconfig.json\', include_outputs=True)\n```\n\n### Edit `aiconfig` in a notebook editor\n\nWe can iterate on an `aiconfig` using a notebook-like editor called an **AI Workbook**. Now that we have an `aiconfig` file artifact that encapsulates the generative AI part of our application, we can iterate on it separately from the application code that uses it.\n\n1. Go to https://lastmileai.dev.\n2. Go to Workbooks page: https://lastmileai.dev/workbooks\n3. Click dropdown from \'+ New Workbook\' and select \'Create from AIConfig\'\n4. Upload `travel.aiconfig.json`\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/5d901493-bbda-4f8e-93c7-dd9a91bf242e\n\nTry out the workbook playground here: **[NYC Travel Workbook](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9)**\n\n> **We are working on a local editor that you can run yourself. For now, please use the hosted version on https://lastmileai.dev.**\n\n### Additional Guides\n\nThere is a lot you can do with `aiconfig`. We have several other tutorials to help get you started:\n\n- [Create an AIConfig from scratch](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig)\n- [Run a prompt](https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig)\n- [Pass data into prompts](https://aiconfig.lastmileai.dev/docs/overview/parameters)\n- [Prompt chains](https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain)\n- [Callbacks and monitoring](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\nHere are some example uses:\n\n- [CLI Chatbot](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT)\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [Chain of thought](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### OpenAI Introspection API\n\nIf you are already using OpenAI completion API\'s in your application, you can get started very quickly to start saving the messages in an `aiconfig`.\n\nUsage: see openai_wrapper.ipynb.\n\nNow you can continue using `openai` completion API as normal. When you want to save the config, just call `new_config.save()` and all your openai completion calls will get serialized to disk.\n\n> [**Detailed guide here**](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig#openai-api-python-wrapper)\n\n## Supported Models\n\nAIConfig supports the following models out of the box:\n\n- OpenAI chat models (GPT-3, GPT-3.5, GPT-4)\n- LLaMA2 (running locally)\n- Google PaLM models (PaLM chat)\n- Hugging Face text generation models (e.g. Mistral-7B)\n\n### Examples\n\n- [OpenAI](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n> If you need to use a model that isn\'t provided out of the box, you can implement a `ModelParser` for it (see [Extending AIConfig](#extending-aiconfig)). **We welcome [contributions](https://aiconfig.lastmileai.dev/docs/contributing)**\n\n## AIConfig Schema\n\n[AIConfig specification](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format)\n\n## AIConfig SDK\n\n> Read the [Usage Guide](https://aiconfig.lastmileai.dev/docs/usage-guide) for more details.\n\nThe AIConfig SDK supports CRUD operations for prompts, models, parameters and metadata. Here are some common examples.\n\nThe root interface is the `AIConfigRuntime` object. That is the entrypoint for interacting with an AIConfig programmatically.\n\nLet\'s go over a few key CRUD operations to give a glimpse.\n\n### AIConfig `create`\n\n```python\nconfig = AIConfigRuntime.create("aiconfig name", "description")\n```\n\n### Prompt `resolve`\n\n`resolve` deserializes an existing `Prompt` into the data object that its model expects.\n\n```python\nconfig.resolve("prompt_name", params)\n```\n\n`params` are overrides you can specify to resolve any `{{handlebars}}` templates in the prompt. See the `gen_itinerary` prompt in the Getting Started example.\n\n### Prompt `serialize`\n\n`serialize` is the inverse of `resolve` -- it serializes the data object that a model understands into a `Prompt` object that can be serialized into the `aiconfig` format.\n\n```python\nconfig.serialize("model_name", data, "prompt_name")\n```\n\n### Prompt `run`\n\n`run` is used to run inference for the specified `Prompt`.\n\n```python\nconfig.run("prompt_name", params)\n```\n\n### `run_with_dependencies`\n\nThis is a variant of `run` -- this re-runs all prompt dependencies.\nFor example, in [`travel.aiconfig.json`](#download-travelaiconfigjson), the `gen_itinerary` prompt references the output of the `get_activities` prompt using `{{get_activities.output}}`.\n\nRunning this function will first execute `get_activities`, and use its output to resolve the `gen_itinerary` prompt before executing it.\nThis is transitive, so it computes the Directed Acyclic Graph of dependencies to execute. Complex relationships can be modeled this way.\n\n```python\nconfig.run_with_dependencies("gen_itinerary")\n```\n\n### Updating metadata and parameters\n\nUse the `get/set_metadata` and `get/set_parameter` methods to interact with metadata and parameters (`set_parameter` is just syntactic sugar to update `"metadata.parameters"`)\n\n```python\nconfig.set_metadata("key", data, "prompt_name")\n```\n\nNote: if `"prompt_name"` is specified, the metadata is updated specifically for that prompt. Otherwise, the global metadata is updated.\n\n### Register new `ModelParser`\n\nUse the `AIConfigRuntime.register_model_parser` if you want to use a different `ModelParser`, or configure AIConfig to work with an additional model.\n\nAIConfig uses the model name string to retrieve the right `ModelParser` for a given Prompt (see `AIConfigRuntime.get_model_parser`), so you can register a different ModelParser for the same ID to override which `ModelParser` handles a Prompt.\n\nFor example, suppose I want to use `MyOpenAIModelParser` to handle `gpt-4` prompts. I can do the following at the start of my application:\n\n```python\nAIConfigRuntime.register_model_parser(myModelParserInstance, ["gpt-4"])\n```\n\n### Callback events\n\nUse callback events to trace and monitor what\'s going on -- helpful for debugging and observability.\n\n```python\nfrom aiconfig import AIConfigRuntime, CallbackEvent, CallbackManager\nconfig = AIConfigRuntime.load(\'aiconfig.json\')\n\nasync def my_custom_callback(event: CallbackEvent) -> None:\n  print(f"Event triggered: {event.name}", event)\n\ncallback_manager = CallbackManager([my_custom_callback])\nconfig.set_callback_manager(callback_manager)\n\nawait config.run("prompt_name")\n```\n\n[**Read more** here](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\n## Extensibility\n\nAIConfig is designed to be customized and extended for your use-case. The [Extensibility](/docs/extensibility) guide goes into more detail.\n\nCurrently, there are 3 core ways to extend AIConfig:\n\n1. [Supporting other models](https://aiconfig.lastmileai.dev/docs/extensibility#1-bring-your-own-model) - define a ModelParser extension\n2. [Callback event handlers](https://aiconfig.lastmileai.dev/docs/extensibility#2-callback-handlers) - tracing and monitoring\n3. [Custom metadata](https://aiconfig.lastmileai.dev/docs/extensibility#3-custom-metadata) - save custom fields in `aiconfig`\n\n## Contributing to `aiconfig`\n\nThis is our first open-source project and we\'d love your help.\n\nSee our [contributing guidelines](https://aiconfig.lastmileai.dev/docs/contributing) -- we would especially love help adding support for additional models that the community wants.\n\n## Cookbooks\n\nWe provide several guides to demonstrate the power of `aiconfig`.\n\n> **See the [`cookbooks`](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks) folder for examples to clone.**\n\n### Chatbot\n\n- [Wizard GPT](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT) - speak to a wizard on your CLI\n\n- [CLI-mate](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Cli-Mate) - help you make code-mods interactively on your codebase.\n\n### Retrieval Augmented Generated (RAG)\n\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n\nAt its core, RAG is about passing data into prompts. Read how to [pass data](/docs/overview/parameters) with AIConfig.\n\n### Function calling\n\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n\n### Prompt routing\n\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n\n### Chain of Thought\n\nA variant of chain-of-thought is Chain of Verification, used to help reduce hallucinations. Check out the aiconfig cookbook for CoVe:\n\n- [Chain of Verification](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### Using local LLaMA2 with `aiconfig`\n\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n\n### Hugging Face text generation\n\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n\n### Google PaLM\n\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n## Roadmap\n\nThis project is under active development.\n\nIf you\'d like to help, please see the [contributing guidelines](#contributing-to-aiconfig).\n\nPlease create issues for additional capabilities you\'d like to see.\n\nHere\'s what\'s already on our roadmap:\n\n- Evaluation interfaces: allow `aiconfig` artifacts to be evaluated with user-defined eval functions.\n  - We are also considering integrating with existing evaluation frameworks.\n- Local editor for `aiconfig`: enable you to interact with aiconfigs more intuitively.\n- OpenAI Assistants API support\n- Multi-modal ModelParsers:\n  - GPT4-V support\n  - DALLE-3\n  - Whisper\n  - HuggingFace image generation\n\n## FAQs\n\n### How should I edit an `aiconfig` file?\n\nEditing a configshould be done either programmatically via SDK or via the UI (workbooks):\n\n- [Programmatic](https://github.com/lastmile-ai/aiconfig/blob/main/cookbooks/Create-AIConfig-Programmatically/create_aiconfig_programmatically.ipynb) editing.\n\n- [Edit with a workbook](#edit-aiconfig-in-a-notebook-editor) editor: this is similar to editing an ipynb file as a notebook (most people never touch the json ipynb directly)\n\nYou should only edit the `aiconfig` by hand for minor modifications, like tweaking a prompt string or updating some metadata.\n\n### Does this support custom endpoints?\n\nOut of the box, AIConfig already supports all OpenAI GPT\\* models, Googles PaLM model and any textgeneration model on Hugging Face (like Mistral). See [Supported Models](#supported-models) for more details.\n\nAdditionally, you can install `aiconfig` [extensions](https://github.com/lastmile-ai/aiconfig/tree/main/extensions) for additional models (see question below).\n\n### Is OpenAI function calling supported?\n\nYes. [This example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI) goes through how to do it.\n\nWe are also working on adding support for the Assistants API.\n\n### How can I use aiconfig with my own model endpoint?\n\nModel support is implemented as ModelParsers in the AIConfig SDK, and the idea is that anyone, including you, can define a ModelParser (and even publish it as an extension package).\n\nAll thats needed to use a model with AIConfig is a ModelParser that knows\n\n- how to serialize data from a model into the aiconfig format\n- how to deserialize data from an aiconfig into the type the model expects\n- how to run inference for model.\n\nFor more details, see [Extensibility](https://aiconfig.lastmileai.dev/docs/extensibility).\n\n### When should I store outputs in an `aiconfig`?\n\nThe `AIConfigRuntime` object is used to interact with an aiconfig programmatically (see [SDK usage guide](#aiconfig-sdk)). As you run prompts, this object keeps track of the outputs returned from the model.\n\nYou can choose to serialize these outputs back into the `aiconfig` by using the `config.save(include_outputs=True)` API. This can be useful for preserving context -- think of it like session state.\n\nFor example, you can use aiconfig to create a chatbot, and use the same format to save the chat history so it can be resumed for the next session.\n\nYou can also choose to save outputs to a _different_ file than the original config -- `config.save("history.aiconfig.json", include_outputs=True)`.\n\n### Why should I use `aiconfig` instead of things like [configurator](https://pypi.org/project/configurator/)?\n\nIt helps to have a [standardized format](http://aiconfig.lastmileai.dev/docs/overview/ai-config-format) specifically for storing generative AI prompts, inference results, model parameters and arbitrary metadata, as opposed to a general-purpose configuration schema.\n\nWith that standardization, you just need a layer that knows how to serialize/deserialize from that format into whatever the inference endpoints require.\n\n### This looks similar to `ipynb` for Jupyter notebooks\n\nWe believe that notebooks are a perfect iteration environment for generative AI -- they are flexible, multi-modal, and collaborative.\n\nThe multi-modality and flexibility offered by notebooks and [`ipynb`](https://ipython.org/ipython-doc/3/notebook/nbformat.html) offers a good interaction model for generative AI. The `aiconfig` file format is extensible like `ipynb`, and AI Workbook editor allows rapid iteration in a notebook-like IDE.\n\n_AI Workbooks are to AIConfig what Jupyter notebooks are to `ipynb`_\n\nThere are 2 areas where we are going beyond what notebooks offer:\n\n1. `aiconfig` is more **source-control friendly** than `ipynb`. `ipynb` stores binary data (images, etc.) by encoding it in the file, while `aiconfig` recommends using file URI references instead.\n2. `aiconfig` can be imported and **connected to application code** using the AIConfig SDK.\n', 'repo_name': 'aiconfig'}} ts_ns=1702175485063260400
2023-12-09 21:31:40,215 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '<div align="center"><picture>\n  <img alt="aiconfig" src="aiconfig-docs/static/img/readme_logo.png" />\n</picture></div>\n<br/>\n\n![Python](https://github.com/lastmile-ai/aiconfig/actions/workflows/main_python.yml/badge.svg)\n![Node](https://github.com/lastmile-ai/aiconfig/actions/workflows/main-typescript.yml/badge.svg)\n![Docs](https://github.com/lastmile-ai/aiconfig/actions/workflows/test-deploy-docs.yml/badge.svg)\n[![Discord](<https://img.shields.io/badge/Discord-LastMile%20AI-Blue?color=rgb(37%2C%20150%2C%20190)>)](https://discord.gg/qMqgzDae)\n\n> Full documentation: **[aiconfig.lastmileai.dev](https://aiconfig.lastmileai.dev/)**\n\n## Overview\n\nAIConfig saves prompts, models and model parameters as source control friendly configs. This allows you to iterate on prompts and model parameters _separately from your application code_.\n\n1. **Prompts as configs**: a [standardized JSON format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to store generative AI model settings, prompt inputs/outputs, and flexible metadata.\n2. **Model-agnostic SDK**: Python & Node SDKs to use `aiconfig` in your application code. AIConfig is designed to be **model-agnostic** and **multi-modal**, so you can extend it to work with any generative AI model, including text, image and audio.\n3. **AI Workbook editor**: A [notebook-like playground](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to edit `aiconfig` files visually, run prompts, tweak models and model settings, and chain things together.\n\n### What problem it solves\n\nToday, application code is tightly coupled with the gen AI settings for the application -- prompts, parameters, and model-specific logic is all jumbled in with app code.\n\n- results in increased complexity\n- makes it hard to iterate on the prompts or try different models easily\n- makes it hard to evaluate prompt/model performance\n\nAIConfig helps unwind complexity by separating prompts, model parameters, and model-specific logic from your application.\n\n- simplifies application code -- simply call `config.run()`\n- open the `aiconfig` in a playground to iterate quickly\n- version control and evaluate the `aiconfig` - it\'s the AI artifact for your application.\n\n![AIConfig flow](aiconfig-docs/static/img/aiconfig_dataflow.png)\n\n### Quicknav\n\n<ul style="margin-bottom:0; padding-bottom:0;">\n  <li><a href="#install">Getting Started</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig">Create an AIConfig</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig">Run a prompt</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/parameters">Pass data into prompts</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain">Prompt Chains</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig">Callbacks and monitoring</a></li>\n  </ul>\n  <li><a href="#aiconfig-sdk">SDK Cheatsheet</a></li>\n  <li><a href="#cookbooks">Cookbooks and guides</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT">CLI Chatbot</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig">RAG with AIConfig</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing">Prompt routing</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI">OpenAI function calling</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification">Chain of Verification</a></li>\n  </ul>\n  <li><a href="#supported-models">Supported models</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama">LLaMA2 example</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace">Hugging Face (Mistral-7B) example</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency">PaLM</a></li>\n  </ul>\n  <li><a href="#extensibility">Extensibility</a></li>\n  <li><a href="#contributing-to-aiconfig">Contributing</a></li>\n  <li><a href="#roadmap">Roadmap</a></li>\n  <li><a href="#faqs">FAQ</a></li>\n</ul>\n\n## Features\n\n- [x] **Source-control friendly** [`aiconfig` format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to save prompts and model settings, which you can use for evaluation, reproducibility and simplifying your application code.\n- [x] **Multi-modal and model agnostic**. Use with any model, and serialize/deserialize data with the same `aiconfig` format.\n- [x] **Prompt chaining and parameterization** with [{{handlebars}}](https://handlebarsjs.com/) templating syntax, allowing you to pass dynamic data into prompts (as well as between prompts).\n- [x] **Streaming** supported out of the box, allowing you to get playground-like streaming wherever you use `aiconfig`.\n- [x] **Notebook editor**. [AI Workbooks editor](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to visually create your `aiconfig`, and use the SDK to connect it to your application code.\n\n## Install\n\nInstall with your favorite package manager for Node or Python.\n\n### Node.js\n\n#### `npm` or `yarn`\n\n```bash\nnpm install aiconfig\n```\n\n```bash\nyarn add aiconfig\n```\n\n### Python\n\n#### `pip3` or `poetry`\n\n```bash\npip3 install python-aiconfig\n```\n\n```bash\npoetry add python-aiconfig\n```\n\n[Detailed installation instructions](https://aiconfig.lastmileai.dev/docs/getting-started/#installation).\n\n### Set your OpenAI API Key\n\n> **Note**: Make sure to specify the API keys (such as [`OPENAI_API_KEY`](https://platform.openai.com/api-keys)) in your environment before proceeding.\n\nIn your CLI, set the environment variable:\n\n```bash\nexport OPENAI_API_KEY=my_key\n```\n\n## Getting Started\n\n> We cover Python instructions here, for Node.js please see the [detailed Getting Started guide](https://aiconfig.lastmileai.dev/docs/getting-started)\n\nIn this quickstart, you will create a customizable NYC travel itinerary using `aiconfig`.\n\nThis AIConfig contains a prompt chain to get a list of travel activities from an LLM and then generate an itinerary in an order specified by the user.\n\n> **Link to tutorial code: [here](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Getting-Started)**\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/805173d1-0f83-44c5-b570-c776bb7dba66\n\n### Download `travel.aiconfig.json`\n\n> **Note**: Don\'t worry if you don\'t understand all the pieces of this yet, we\'ll go over it step by step.\n\n```json\n{\n  "name": "NYC Trip Planner",\n  "description": "Intrepid explorer with ChatGPT and AIConfig",\n  "schema_version": "latest",\n  "metadata": {\n    "models": {\n      "gpt-3.5-turbo": {\n        "model": "gpt-3.5-turbo",\n        "top_p": 1,\n        "temperature": 1\n      },\n      "gpt-4": {\n        "model": "gpt-4",\n        "max_tokens": 3000,\n        "system_prompt": "You are an expert travel coordinator with exquisite taste."\n      }\n    },\n    "default_model": "gpt-3.5-turbo"\n  },\n  "prompts": [\n    {\n      "name": "get_activities",\n      "input": "Tell me 10 fun attractions to do in NYC."\n    },\n    {\n      "name": "gen_itinerary",\n      "input": "Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}.",\n      "metadata": {\n        "model": "gpt-4",\n        "parameters": {\n          "order_by": "geographic location"\n        }\n      }\n    }\n  ]\n}\n```\n\n### Run the `get_activities` prompt.\n\nYou don\'t need to worry about how to run inference for the model; it\'s all handled by AIConfig. The prompt runs with gpt-3.5-turbo since that is the `default_model` for this AIConfig.\n\nCreate a new file called `app.py` and and enter the following code:\n\n```python\nimport asyncio\nfrom aiconfig import AIConfigRuntime, InferenceOptions\n\nasync def main():\n  # Load the aiconfig\n  config = AIConfigRuntime.load(\'travel.aiconfig.json\')\n\n  # Run a single prompt (with streaming)\n  inference_options = InferenceOptions(stream=True)\n  await config.run("get_activities", options=inference_options)\n\nasyncio.run(main())\n```\n\nNow run this in your terminal with the command:\n\n```bash\npython3 app.py\n```\n\n### Run the `gen_itinerary` prompt.\n\nIn your `app.py` file, change the last line to below:\n\n```python\nawait config.run("gen_itinerary", params=None, options=inference_options)\n```\n\nRe-run the command in your terminal:\n\n```bash\npython3 app.py\n```\n\nThis prompt depends on the output of `get_activities`. It also takes in parameters (user input) to determine the customized itinerary.\n\nLet\'s take a closer look:\n\n**`gen_itinerary` prompt:**\n\n```\n"Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}."\n```\n\n**prompt metadata:**\n\n```json\n{\n  "metadata": {\n    "model": "gpt-4",\n    "parameters": {\n      "order_by": "geographic location"\n    }\n  }\n}\n```\n\nObserve the following:\n\n1. The prompt depends on the output of the `get_activities` prompt.\n2. It also depends on an `order_by` parameter (using {{handlebars}} syntax)\n3. It uses **gpt-4**, whereas the `get_activities` prompt it depends on uses **gpt-3.5-turbo**.\n\n> Effectively, this is a prompt chain between `gen_itinerary` and `get_activities` prompts, _as well as_ as a model chain between **gpt-3.5-turbo** and **gpt-4**.\n\nLet\'s run this with AIConfig:\n\nReplace `config.run` above with this:\n\n```python\nawait config.run("gen_itinerary", params={"order_by": "duration"}, options=inference_options, run_with_dependencies=True)\n```\n\nNotice how simple the syntax is to perform a fairly complex task - running 2 different prompts across 2 different models and chaining one\'s output as part of the input of another.\n\nThe code will just run `get_activities`, then pipe its output as an input to `gen_itinerary`, and finally run `gen_itinerary`.\n\n### Save the AIConfig\n\nLet\'s save the AIConfig back to disk, and serialize the outputs from the latest inference run as well:\n\n```python\n# Save the aiconfig to disk. and serialize outputs from the model run\nconfig.save(\'updated.aiconfig.json\', include_outputs=True)\n```\n\n### Edit `aiconfig` in a notebook editor\n\nWe can iterate on an `aiconfig` using a notebook-like editor called an **AI Workbook**. Now that we have an `aiconfig` file artifact that encapsulates the generative AI part of our application, we can iterate on it separately from the application code that uses it.\n\n1. Go to https://lastmileai.dev.\n2. Go to Workbooks page: https://lastmileai.dev/workbooks\n3. Click dropdown from \'+ New Workbook\' and select \'Create from AIConfig\'\n4. Upload `travel.aiconfig.json`\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/5d901493-bbda-4f8e-93c7-dd9a91bf242e\n\nTry out the workbook playground here: **[NYC Travel Workbook](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9)**\n\n> **We are working on a local editor that you can run yourself. For now, please use the hosted version on https://lastmileai.dev.**\n\n### Additional Guides\n\nThere is a lot you can do with `aiconfig`. We have several other tutorials to help get you started:\n\n- [Create an AIConfig from scratch](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig)\n- [Run a prompt](https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig)\n- [Pass data into prompts](https://aiconfig.lastmileai.dev/docs/overview/parameters)\n- [Prompt chains](https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain)\n- [Callbacks and monitoring](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\nHere are some example uses:\n\n- [CLI Chatbot](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT)\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [Chain of thought](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### OpenAI Introspection API\n\nIf you are already using OpenAI completion API\'s in your application, you can get started very quickly to start saving the messages in an `aiconfig`.\n\nUsage: see openai_wrapper.ipynb.\n\nNow you can continue using `openai` completion API as normal. When you want to save the config, just call `new_config.save()` and all your openai completion calls will get serialized to disk.\n\n> [**Detailed guide here**](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig#openai-api-python-wrapper)\n\n## Supported Models\n\nAIConfig supports the following models out of the box:\n\n- OpenAI chat models (GPT-3, GPT-3.5, GPT-4)\n- LLaMA2 (running locally)\n- Google PaLM models (PaLM chat)\n- Hugging Face text generation models (e.g. Mistral-7B)\n\n### Examples\n\n- [OpenAI](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n> If you need to use a model that isn\'t provided out of the box, you can implement a `ModelParser` for it (see [Extending AIConfig](#extending-aiconfig)). **We welcome [contributions](https://aiconfig.lastmileai.dev/docs/contributing)**\n\n## AIConfig Schema\n\n[AIConfig specification](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format)\n\n## AIConfig SDK\n\n> Read the [Usage Guide](https://aiconfig.lastmileai.dev/docs/usage-guide) for more details.\n\nThe AIConfig SDK supports CRUD operations for prompts, models, parameters and metadata. Here are some common examples.\n\nThe root interface is the `AIConfigRuntime` object. That is the entrypoint for interacting with an AIConfig programmatically.\n\nLet\'s go over a few key CRUD operations to give a glimpse.\n\n### AIConfig `create`\n\n```python\nconfig = AIConfigRuntime.create("aiconfig name", "description")\n```\n\n### Prompt `resolve`\n\n`resolve` deserializes an existing `Prompt` into the data object that its model expects.\n\n```python\nconfig.resolve("prompt_name", params)\n```\n\n`params` are overrides you can specify to resolve any `{{handlebars}}` templates in the prompt. See the `gen_itinerary` prompt in the Getting Started example.\n\n### Prompt `serialize`\n\n`serialize` is the inverse of `resolve` -- it serializes the data object that a model understands into a `Prompt` object that can be serialized into the `aiconfig` format.\n\n```python\nconfig.serialize("model_name", data, "prompt_name")\n```\n\n### Prompt `run`\n\n`run` is used to run inference for the specified `Prompt`.\n\n```python\nconfig.run("prompt_name", params)\n```\n\n### `run_with_dependencies`\n\nThis is a variant of `run` -- this re-runs all prompt dependencies.\nFor example, in [`travel.aiconfig.json`](#download-travelaiconfigjson), the `gen_itinerary` prompt references the output of the `get_activities` prompt using `{{get_activities.output}}`.\n\nRunning this function will first execute `get_activities`, and use its output to resolve the `gen_itinerary` prompt before executing it.\nThis is transitive, so it computes the Directed Acyclic Graph of dependencies to execute. Complex relationships can be modeled this way.\n\n```python\nconfig.run_with_dependencies("gen_itinerary")\n```\n\n### Updating metadata and parameters\n\nUse the `get/set_metadata` and `get/set_parameter` methods to interact with metadata and parameters (`set_parameter` is just syntactic sugar to update `"metadata.parameters"`)\n\n```python\nconfig.set_metadata("key", data, "prompt_name")\n```\n\nNote: if `"prompt_name"` is specified, the metadata is updated specifically for that prompt. Otherwise, the global metadata is updated.\n\n### Register new `ModelParser`\n\nUse the `AIConfigRuntime.register_model_parser` if you want to use a different `ModelParser`, or configure AIConfig to work with an additional model.\n\nAIConfig uses the model name string to retrieve the right `ModelParser` for a given Prompt (see `AIConfigRuntime.get_model_parser`), so you can register a different ModelParser for the same ID to override which `ModelParser` handles a Prompt.\n\nFor example, suppose I want to use `MyOpenAIModelParser` to handle `gpt-4` prompts. I can do the following at the start of my application:\n\n```python\nAIConfigRuntime.register_model_parser(myModelParserInstance, ["gpt-4"])\n```\n\n### Callback events\n\nUse callback events to trace and monitor what\'s going on -- helpful for debugging and observability.\n\n```python\nfrom aiconfig import AIConfigRuntime, CallbackEvent, CallbackManager\nconfig = AIConfigRuntime.load(\'aiconfig.json\')\n\nasync def my_custom_callback(event: CallbackEvent) -> None:\n  print(f"Event triggered: {event.name}", event)\n\ncallback_manager = CallbackManager([my_custom_callback])\nconfig.set_callback_manager(callback_manager)\n\nawait config.run("prompt_name")\n```\n\n[**Read more** here](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\n## Extensibility\n\nAIConfig is designed to be customized and extended for your use-case. The [Extensibility](/docs/extensibility) guide goes into more detail.\n\nCurrently, there are 3 core ways to extend AIConfig:\n\n1. [Supporting other models](https://aiconfig.lastmileai.dev/docs/extensibility#1-bring-your-own-model) - define a ModelParser extension\n2. [Callback event handlers](https://aiconfig.lastmileai.dev/docs/extensibility#2-callback-handlers) - tracing and monitoring\n3. [Custom metadata](https://aiconfig.lastmileai.dev/docs/extensibility#3-custom-metadata) - save custom fields in `aiconfig`\n\n## Contributing to `aiconfig`\n\nThis is our first open-source project and we\'d love your help.\n\nSee our [contributing guidelines](https://aiconfig.lastmileai.dev/docs/contributing) -- we would especially love help adding support for additional models that the community wants.\n\n## Cookbooks\n\nWe provide several guides to demonstrate the power of `aiconfig`.\n\n> **See the [`cookbooks`](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks) folder for examples to clone.**\n\n### Chatbot\n\n- [Wizard GPT](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT) - speak to a wizard on your CLI\n\n- [CLI-mate](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Cli-Mate) - help you make code-mods interactively on your codebase.\n\n### Retrieval Augmented Generated (RAG)\n\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n\nAt its core, RAG is about passing data into prompts. Read how to [pass data](/docs/overview/parameters) with AIConfig.\n\n### Function calling\n\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n\n### Prompt routing\n\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n\n### Chain of Thought\n\nA variant of chain-of-thought is Chain of Verification, used to help reduce hallucinations. Check out the aiconfig cookbook for CoVe:\n\n- [Chain of Verification](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### Using local LLaMA2 with `aiconfig`\n\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n\n### Hugging Face text generation\n\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n\n### Google PaLM\n\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n## Roadmap\n\nThis project is under active development.\n\nIf you\'d like to help, please see the [contributing guidelines](#contributing-to-aiconfig).\n\nPlease create issues for additional capabilities you\'d like to see.\n\nHere\'s what\'s already on our roadmap:\n\n- Evaluation interfaces: allow `aiconfig` artifacts to be evaluated with user-defined eval functions.\n  - We are also considering integrating with existing evaluation frameworks.\n- Local editor for `aiconfig`: enable you to interact with aiconfigs more intuitively.\n- OpenAI Assistants API support\n- Multi-modal ModelParsers:\n  - GPT4-V support\n  - DALLE-3\n  - Whisper\n  - HuggingFace image generation\n\n## FAQs\n\n### How should I edit an `aiconfig` file?\n\nEditing a configshould be done either programmatically via SDK or via the UI (workbooks):\n\n- [Programmatic](https://github.com/lastmile-ai/aiconfig/blob/main/cookbooks/Create-AIConfig-Programmatically/create_aiconfig_programmatically.ipynb) editing.\n\n- [Edit with a workbook](#edit-aiconfig-in-a-notebook-editor) editor: this is similar to editing an ipynb file as a notebook (most people never touch the json ipynb directly)\n\nYou should only edit the `aiconfig` by hand for minor modifications, like tweaking a prompt string or updating some metadata.\n\n### Does this support custom endpoints?\n\nOut of the box, AIConfig already supports all OpenAI GPT\\* models, Googles PaLM model and any textgeneration model on Hugging Face (like Mistral). See [Supported Models](#supported-models) for more details.\n\nAdditionally, you can install `aiconfig` [extensions](https://github.com/lastmile-ai/aiconfig/tree/main/extensions) for additional models (see question below).\n\n### Is OpenAI function calling supported?\n\nYes. [This example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI) goes through how to do it.\n\nWe are also working on adding support for the Assistants API.\n\n### How can I use aiconfig with my own model endpoint?\n\nModel support is implemented as ModelParsers in the AIConfig SDK, and the idea is that anyone, including you, can define a ModelParser (and even publish it as an extension package).\n\nAll thats needed to use a model with AIConfig is a ModelParser that knows\n\n- how to serialize data from a model into the aiconfig format\n- how to deserialize data from an aiconfig into the type the model expects\n- how to run inference for model.\n\nFor more details, see [Extensibility](https://aiconfig.lastmileai.dev/docs/extensibility).\n\n### When should I store outputs in an `aiconfig`?\n\nThe `AIConfigRuntime` object is used to interact with an aiconfig programmatically (see [SDK usage guide](#aiconfig-sdk)). As you run prompts, this object keeps track of the outputs returned from the model.\n\nYou can choose to serialize these outputs back into the `aiconfig` by using the `config.save(include_outputs=True)` API. This can be useful for preserving context -- think of it like session state.\n\nFor example, you can use aiconfig to create a chatbot, and use the same format to save the chat history so it can be resumed for the next session.\n\nYou can also choose to save outputs to a _different_ file than the original config -- `config.save("history.aiconfig.json", include_outputs=True)`.\n\n### Why should I use `aiconfig` instead of things like [configurator](https://pypi.org/project/configurator/)?\n\nIt helps to have a [standardized format](http://aiconfig.lastmileai.dev/docs/overview/ai-config-format) specifically for storing generative AI prompts, inference results, model parameters and arbitrary metadata, as opposed to a general-purpose configuration schema.\n\nWith that standardization, you just need a layer that knows how to serialize/deserialize from that format into whatever the inference endpoints require.\n\n### This looks similar to `ipynb` for Jupyter notebooks\n\nWe believe that notebooks are a perfect iteration environment for generative AI -- they are flexible, multi-modal, and collaborative.\n\nThe multi-modality and flexibility offered by notebooks and [`ipynb`](https://ipython.org/ipython-doc/3/notebook/nbformat.html) offers a good interaction model for generative AI. The `aiconfig` file format is extensible like `ipynb`, and AI Workbook editor allows rapid iteration in a notebook-like IDE.\n\n_AI Workbooks are to AIConfig what Jupyter notebooks are to `ipynb`_\n\nThere are 2 areas where we are going beyond what notebooks offer:\n\n1. `aiconfig` is more **source-control friendly** than `ipynb`. `ipynb` stores binary data (images, etc.) by encoding it in the file, while `aiconfig` recommends using file URI references instead.\n2. `aiconfig` can be imported and **connected to application code** using the AIConfig SDK.\n', 'repo_name': 'aiconfig'}} ts_ns=1702175485063260400
2023-12-09 21:31:40,216 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '<div align="center"><picture>\n  <img alt="aiconfig" src="aiconfig-docs/static/img/readme_logo.png" />\n</picture></div>\n<br/>\n\n![Python](https://github.com/lastmile-ai/aiconfig/actions/workflows/main_python.yml/badge.svg)\n![Node](https://github.com/lastmile-ai/aiconfig/actions/workflows/main-typescript.yml/badge.svg)\n![Docs](https://github.com/lastmile-ai/aiconfig/actions/workflows/test-deploy-docs.yml/badge.svg)\n[![Discord](<https://img.shields.io/badge/Discord-LastMile%20AI-Blue?color=rgb(37%2C%20150%2C%20190)>)](https://discord.gg/qMqgzDae)\n\n> Full documentation: **[aiconfig.lastmileai.dev](https://aiconfig.lastmileai.dev/)**\n\n## Overview\n\nAIConfig saves prompts, models and model parameters as source control friendly configs. This allows you to iterate on prompts and model parameters _separately from your application code_.\n\n1. **Prompts as configs**: a [standardized JSON format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to store generative AI model settings, prompt inputs/outputs, and flexible metadata.\n2. **Model-agnostic SDK**: Python & Node SDKs to use `aiconfig` in your application code. AIConfig is designed to be **model-agnostic** and **multi-modal**, so you can extend it to work with any generative AI model, including text, image and audio.\n3. **AI Workbook editor**: A [notebook-like playground](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to edit `aiconfig` files visually, run prompts, tweak models and model settings, and chain things together.\n\n### What problem it solves\n\nToday, application code is tightly coupled with the gen AI settings for the application -- prompts, parameters, and model-specific logic is all jumbled in with app code.\n\n- results in increased complexity\n- makes it hard to iterate on the prompts or try different models easily\n- makes it hard to evaluate prompt/model performance\n\nAIConfig helps unwind complexity by separating prompts, model parameters, and model-specific logic from your application.\n\n- simplifies application code -- simply call `config.run()`\n- open the `aiconfig` in a playground to iterate quickly\n- version control and evaluate the `aiconfig` - it\'s the AI artifact for your application.\n\n![AIConfig flow](aiconfig-docs/static/img/aiconfig_dataflow.png)\n\n### Quicknav\n\n<ul style="margin-bottom:0; padding-bottom:0;">\n  <li><a href="#install">Getting Started</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig">Create an AIConfig</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig">Run a prompt</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/parameters">Pass data into prompts</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain">Prompt Chains</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig">Callbacks and monitoring</a></li>\n  </ul>\n  <li><a href="#aiconfig-sdk">SDK Cheatsheet</a></li>\n  <li><a href="#cookbooks">Cookbooks and guides</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT">CLI Chatbot</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig">RAG with AIConfig</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing">Prompt routing</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI">OpenAI function calling</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification">Chain of Verification</a></li>\n  </ul>\n  <li><a href="#supported-models">Supported models</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama">LLaMA2 example</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace">Hugging Face (Mistral-7B) example</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency">PaLM</a></li>\n  </ul>\n  <li><a href="#extensibility">Extensibility</a></li>\n  <li><a href="#contributing-to-aiconfig">Contributing</a></li>\n  <li><a href="#roadmap">Roadmap</a></li>\n  <li><a href="#faqs">FAQ</a></li>\n</ul>\n\n## Features\n\n- [x] **Source-control friendly** [`aiconfig` format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to save prompts and model settings, which you can use for evaluation, reproducibility and simplifying your application code.\n- [x] **Multi-modal and model agnostic**. Use with any model, and serialize/deserialize data with the same `aiconfig` format.\n- [x] **Prompt chaining and parameterization** with [{{handlebars}}](https://handlebarsjs.com/) templating syntax, allowing you to pass dynamic data into prompts (as well as between prompts).\n- [x] **Streaming** supported out of the box, allowing you to get playground-like streaming wherever you use `aiconfig`.\n- [x] **Notebook editor**. [AI Workbooks editor](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to visually create your `aiconfig`, and use the SDK to connect it to your application code.\n\n## Install\n\nInstall with your favorite package manager for Node or Python.\n\n### Node.js\n\n#### `npm` or `yarn`\n\n```bash\nnpm install aiconfig\n```\n\n```bash\nyarn add aiconfig\n```\n\n### Python\n\n#### `pip3` or `poetry`\n\n```bash\npip3 install python-aiconfig\n```\n\n```bash\npoetry add python-aiconfig\n```\n\n[Detailed installation instructions](https://aiconfig.lastmileai.dev/docs/getting-started/#installation).\n\n### Set your OpenAI API Key\n\n> **Note**: Make sure to specify the API keys (such as [`OPENAI_API_KEY`](https://platform.openai.com/api-keys)) in your environment before proceeding.\n\nIn your CLI, set the environment variable:\n\n```bash\nexport OPENAI_API_KEY=my_key\n```\n\n## Getting Started\n\n> We cover Python instructions here, for Node.js please see the [detailed Getting Started guide](https://aiconfig.lastmileai.dev/docs/getting-started)\n\nIn this quickstart, you will create a customizable NYC travel itinerary using `aiconfig`.\n\nThis AIConfig contains a prompt chain to get a list of travel activities from an LLM and then generate an itinerary in an order specified by the user.\n\n> **Link to tutorial code: [here](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Getting-Started)**\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/805173d1-0f83-44c5-b570-c776bb7dba66\n\n### Download `travel.aiconfig.json`\n\n> **Note**: Don\'t worry if you don\'t understand all the pieces of this yet, we\'ll go over it step by step.\n\n```json\n{\n  "name": "NYC Trip Planner",\n  "description": "Intrepid explorer with ChatGPT and AIConfig",\n  "schema_version": "latest",\n  "metadata": {\n    "models": {\n      "gpt-3.5-turbo": {\n        "model": "gpt-3.5-turbo",\n        "top_p": 1,\n        "temperature": 1\n      },\n      "gpt-4": {\n        "model": "gpt-4",\n        "max_tokens": 3000,\n        "system_prompt": "You are an expert travel coordinator with exquisite taste."\n      }\n    },\n    "default_model": "gpt-3.5-turbo"\n  },\n  "prompts": [\n    {\n      "name": "get_activities",\n      "input": "Tell me 10 fun attractions to do in NYC."\n    },\n    {\n      "name": "gen_itinerary",\n      "input": "Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}.",\n      "metadata": {\n        "model": "gpt-4",\n        "parameters": {\n          "order_by": "geographic location"\n        }\n      }\n    }\n  ]\n}\n```\n\n### Run the `get_activities` prompt.\n\nYou don\'t need to worry about how to run inference for the model; it\'s all handled by AIConfig. The prompt runs with gpt-3.5-turbo since that is the `default_model` for this AIConfig.\n\nCreate a new file called `app.py` and and enter the following code:\n\n```python\nimport asyncio\nfrom aiconfig import AIConfigRuntime, InferenceOptions\n\nasync def main():\n  # Load the aiconfig\n  config = AIConfigRuntime.load(\'travel.aiconfig.json\')\n\n  # Run a single prompt (with streaming)\n  inference_options = InferenceOptions(stream=True)\n  await config.run("get_activities", options=inference_options)\n\nasyncio.run(main())\n```\n\nNow run this in your terminal with the command:\n\n```bash\npython3 app.py\n```\n\n### Run the `gen_itinerary` prompt.\n\nIn your `app.py` file, change the last line to below:\n\n```python\nawait config.run("gen_itinerary", params=None, options=inference_options)\n```\n\nRe-run the command in your terminal:\n\n```bash\npython3 app.py\n```\n\nThis prompt depends on the output of `get_activities`. It also takes in parameters (user input) to determine the customized itinerary.\n\nLet\'s take a closer look:\n\n**`gen_itinerary` prompt:**\n\n```\n"Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}."\n```\n\n**prompt metadata:**\n\n```json\n{\n  "metadata": {\n    "model": "gpt-4",\n    "parameters": {\n      "order_by": "geographic location"\n    }\n  }\n}\n```\n\nObserve the following:\n\n1. The prompt depends on the output of the `get_activities` prompt.\n2. It also depends on an `order_by` parameter (using {{handlebars}} syntax)\n3. It uses **gpt-4**, whereas the `get_activities` prompt it depends on uses **gpt-3.5-turbo**.\n\n> Effectively, this is a prompt chain between `gen_itinerary` and `get_activities` prompts, _as well as_ as a model chain between **gpt-3.5-turbo** and **gpt-4**.\n\nLet\'s run this with AIConfig:\n\nReplace `config.run` above with this:\n\n```python\nawait config.run("gen_itinerary", params={"order_by": "duration"}, options=inference_options, run_with_dependencies=True)\n```\n\nNotice how simple the syntax is to perform a fairly complex task - running 2 different prompts across 2 different models and chaining one\'s output as part of the input of another.\n\nThe code will just run `get_activities`, then pipe its output as an input to `gen_itinerary`, and finally run `gen_itinerary`.\n\n### Save the AIConfig\n\nLet\'s save the AIConfig back to disk, and serialize the outputs from the latest inference run as well:\n\n```python\n# Save the aiconfig to disk. and serialize outputs from the model run\nconfig.save(\'updated.aiconfig.json\', include_outputs=True)\n```\n\n### Edit `aiconfig` in a notebook editor\n\nWe can iterate on an `aiconfig` using a notebook-like editor called an **AI Workbook**. Now that we have an `aiconfig` file artifact that encapsulates the generative AI part of our application, we can iterate on it separately from the application code that uses it.\n\n1. Go to https://lastmileai.dev.\n2. Go to Workbooks page: https://lastmileai.dev/workbooks\n3. Click dropdown from \'+ New Workbook\' and select \'Create from AIConfig\'\n4. Upload `travel.aiconfig.json`\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/5d901493-bbda-4f8e-93c7-dd9a91bf242e\n\nTry out the workbook playground here: **[NYC Travel Workbook](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9)**\n\n> **We are working on a local editor that you can run yourself. For now, please use the hosted version on https://lastmileai.dev.**\n\n### Additional Guides\n\nThere is a lot you can do with `aiconfig`. We have several other tutorials to help get you started:\n\n- [Create an AIConfig from scratch](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig)\n- [Run a prompt](https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig)\n- [Pass data into prompts](https://aiconfig.lastmileai.dev/docs/overview/parameters)\n- [Prompt chains](https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain)\n- [Callbacks and monitoring](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\nHere are some example uses:\n\n- [CLI Chatbot](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT)\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [Chain of thought](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### OpenAI Introspection API\n\nIf you are already using OpenAI completion API\'s in your application, you can get started very quickly to start saving the messages in an `aiconfig`.\n\nUsage: see openai_wrapper.ipynb.\n\nNow you can continue using `openai` completion API as normal. When you want to save the config, just call `new_config.save()` and all your openai completion calls will get serialized to disk.\n\n> [**Detailed guide here**](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig#openai-api-python-wrapper)\n\n## Supported Models\n\nAIConfig supports the following models out of the box:\n\n- OpenAI chat models (GPT-3, GPT-3.5, GPT-4)\n- LLaMA2 (running locally)\n- Google PaLM models (PaLM chat)\n- Hugging Face text generation models (e.g. Mistral-7B)\n\n### Examples\n\n- [OpenAI](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n> If you need to use a model that isn\'t provided out of the box, you can implement a `ModelParser` for it (see [Extending AIConfig](#extending-aiconfig)). **We welcome [contributions](https://aiconfig.lastmileai.dev/docs/contributing)**\n\n## AIConfig Schema\n\n[AIConfig specification](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format)\n\n## AIConfig SDK\n\n> Read the [Usage Guide](https://aiconfig.lastmileai.dev/docs/usage-guide) for more details.\n\nThe AIConfig SDK supports CRUD operations for prompts, models, parameters and metadata. Here are some common examples.\n\nThe root interface is the `AIConfigRuntime` object. That is the entrypoint for interacting with an AIConfig programmatically.\n\nLet\'s go over a few key CRUD operations to give a glimpse.\n\n### AIConfig `create`\n\n```python\nconfig = AIConfigRuntime.create("aiconfig name", "description")\n```\n\n### Prompt `resolve`\n\n`resolve` deserializes an existing `Prompt` into the data object that its model expects.\n\n```python\nconfig.resolve("prompt_name", params)\n```\n\n`params` are overrides you can specify to resolve any `{{handlebars}}` templates in the prompt. See the `gen_itinerary` prompt in the Getting Started example.\n\n### Prompt `serialize`\n\n`serialize` is the inverse of `resolve` -- it serializes the data object that a model understands into a `Prompt` object that can be serialized into the `aiconfig` format.\n\n```python\nconfig.serialize("model_name", data, "prompt_name")\n```\n\n### Prompt `run`\n\n`run` is used to run inference for the specified `Prompt`.\n\n```python\nconfig.run("prompt_name", params)\n```\n\n### `run_with_dependencies`\n\nThis is a variant of `run` -- this re-runs all prompt dependencies.\nFor example, in [`travel.aiconfig.json`](#download-travelaiconfigjson), the `gen_itinerary` prompt references the output of the `get_activities` prompt using `{{get_activities.output}}`.\n\nRunning this function will first execute `get_activities`, and use its output to resolve the `gen_itinerary` prompt before executing it.\nThis is transitive, so it computes the Directed Acyclic Graph of dependencies to execute. Complex relationships can be modeled this way.\n\n```python\nconfig.run_with_dependencies("gen_itinerary")\n```\n\n### Updating metadata and parameters\n\nUse the `get/set_metadata` and `get/set_parameter` methods to interact with metadata and parameters (`set_parameter` is just syntactic sugar to update `"metadata.parameters"`)\n\n```python\nconfig.set_metadata("key", data, "prompt_name")\n```\n\nNote: if `"prompt_name"` is specified, the metadata is updated specifically for that prompt. Otherwise, the global metadata is updated.\n\n### Register new `ModelParser`\n\nUse the `AIConfigRuntime.register_model_parser` if you want to use a different `ModelParser`, or configure AIConfig to work with an additional model.\n\nAIConfig uses the model name string to retrieve the right `ModelParser` for a given Prompt (see `AIConfigRuntime.get_model_parser`), so you can register a different ModelParser for the same ID to override which `ModelParser` handles a Prompt.\n\nFor example, suppose I want to use `MyOpenAIModelParser` to handle `gpt-4` prompts. I can do the following at the start of my application:\n\n```python\nAIConfigRuntime.register_model_parser(myModelParserInstance, ["gpt-4"])\n```\n\n### Callback events\n\nUse callback events to trace and monitor what\'s going on -- helpful for debugging and observability.\n\n```python\nfrom aiconfig import AIConfigRuntime, CallbackEvent, CallbackManager\nconfig = AIConfigRuntime.load(\'aiconfig.json\')\n\nasync def my_custom_callback(event: CallbackEvent) -> None:\n  print(f"Event triggered: {event.name}", event)\n\ncallback_manager = CallbackManager([my_custom_callback])\nconfig.set_callback_manager(callback_manager)\n\nawait config.run("prompt_name")\n```\n\n[**Read more** here](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\n## Extensibility\n\nAIConfig is designed to be customized and extended for your use-case. The [Extensibility](/docs/extensibility) guide goes into more detail.\n\nCurrently, there are 3 core ways to extend AIConfig:\n\n1. [Supporting other models](https://aiconfig.lastmileai.dev/docs/extensibility#1-bring-your-own-model) - define a ModelParser extension\n2. [Callback event handlers](https://aiconfig.lastmileai.dev/docs/extensibility#2-callback-handlers) - tracing and monitoring\n3. [Custom metadata](https://aiconfig.lastmileai.dev/docs/extensibility#3-custom-metadata) - save custom fields in `aiconfig`\n\n## Contributing to `aiconfig`\n\nThis is our first open-source project and we\'d love your help.\n\nSee our [contributing guidelines](https://aiconfig.lastmileai.dev/docs/contributing) -- we would especially love help adding support for additional models that the community wants.\n\n## Cookbooks\n\nWe provide several guides to demonstrate the power of `aiconfig`.\n\n> **See the [`cookbooks`](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks) folder for examples to clone.**\n\n### Chatbot\n\n- [Wizard GPT](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT) - speak to a wizard on your CLI\n\n- [CLI-mate](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Cli-Mate) - help you make code-mods interactively on your codebase.\n\n### Retrieval Augmented Generated (RAG)\n\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n\nAt its core, RAG is about passing data into prompts. Read how to [pass data](/docs/overview/parameters) with AIConfig.\n\n### Function calling\n\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n\n### Prompt routing\n\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n\n### Chain of Thought\n\nA variant of chain-of-thought is Chain of Verification, used to help reduce hallucinations. Check out the aiconfig cookbook for CoVe:\n\n- [Chain of Verification](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### Using local LLaMA2 with `aiconfig`\n\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n\n### Hugging Face text generation\n\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n\n### Google PaLM\n\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n## Roadmap\n\nThis project is under active development.\n\nIf you\'d like to help, please see the [contributing guidelines](#contributing-to-aiconfig).\n\nPlease create issues for additional capabilities you\'d like to see.\n\nHere\'s what\'s already on our roadmap:\n\n- Evaluation interfaces: allow `aiconfig` artifacts to be evaluated with user-defined eval functions.\n  - We are also considering integrating with existing evaluation frameworks.\n- Local editor for `aiconfig`: enable you to interact with aiconfigs more intuitively.\n- OpenAI Assistants API support\n- Multi-modal ModelParsers:\n  - GPT4-V support\n  - DALLE-3\n  - Whisper\n  - HuggingFace image generation\n\n## FAQs\n\n### How should I edit an `aiconfig` file?\n\nEditing a configshould be done either programmatically via SDK or via the UI (workbooks):\n\n- [Programmatic](https://github.com/lastmile-ai/aiconfig/blob/main/cookbooks/Create-AIConfig-Programmatically/create_aiconfig_programmatically.ipynb) editing.\n\n- [Edit with a workbook](#edit-aiconfig-in-a-notebook-editor) editor: this is similar to editing an ipynb file as a notebook (most people never touch the json ipynb directly)\n\nYou should only edit the `aiconfig` by hand for minor modifications, like tweaking a prompt string or updating some metadata.\n\n### Does this support custom endpoints?\n\nOut of the box, AIConfig already supports all OpenAI GPT\\* models, Googles PaLM model and any textgeneration model on Hugging Face (like Mistral). See [Supported Models](#supported-models) for more details.\n\nAdditionally, you can install `aiconfig` [extensions](https://github.com/lastmile-ai/aiconfig/tree/main/extensions) for additional models (see question below).\n\n### Is OpenAI function calling supported?\n\nYes. [This example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI) goes through how to do it.\n\nWe are also working on adding support for the Assistants API.\n\n### How can I use aiconfig with my own model endpoint?\n\nModel support is implemented as ModelParsers in the AIConfig SDK, and the idea is that anyone, including you, can define a ModelParser (and even publish it as an extension package).\n\nAll thats needed to use a model with AIConfig is a ModelParser that knows\n\n- how to serialize data from a model into the aiconfig format\n- how to deserialize data from an aiconfig into the type the model expects\n- how to run inference for model.\n\nFor more details, see [Extensibility](https://aiconfig.lastmileai.dev/docs/extensibility).\n\n### When should I store outputs in an `aiconfig`?\n\nThe `AIConfigRuntime` object is used to interact with an aiconfig programmatically (see [SDK usage guide](#aiconfig-sdk)). As you run prompts, this object keeps track of the outputs returned from the model.\n\nYou can choose to serialize these outputs back into the `aiconfig` by using the `config.save(include_outputs=True)` API. This can be useful for preserving context -- think of it like session state.\n\nFor example, you can use aiconfig to create a chatbot, and use the same format to save the chat history so it can be resumed for the next session.\n\nYou can also choose to save outputs to a _different_ file than the original config -- `config.save("history.aiconfig.json", include_outputs=True)`.\n\n### Why should I use `aiconfig` instead of things like [configurator](https://pypi.org/project/configurator/)?\n\nIt helps to have a [standardized format](http://aiconfig.lastmileai.dev/docs/overview/ai-config-format) specifically for storing generative AI prompts, inference results, model parameters and arbitrary metadata, as opposed to a general-purpose configuration schema.\n\nWith that standardization, you just need a layer that knows how to serialize/deserialize from that format into whatever the inference endpoints require.\n\n### This looks similar to `ipynb` for Jupyter notebooks\n\nWe believe that notebooks are a perfect iteration environment for generative AI -- they are flexible, multi-modal, and collaborative.\n\nThe multi-modality and flexibility offered by notebooks and [`ipynb`](https://ipython.org/ipython-doc/3/notebook/nbformat.html) offers a good interaction model for generative AI. The `aiconfig` file format is extensible like `ipynb`, and AI Workbook editor allows rapid iteration in a notebook-like IDE.\n\n_AI Workbooks are to AIConfig what Jupyter notebooks are to `ipynb`_\n\nThere are 2 areas where we are going beyond what notebooks offer:\n\n1. `aiconfig` is more **source-control friendly** than `ipynb`. `ipynb` stores binary data (images, etc.) by encoding it in the file, while `aiconfig` recommends using file URI references instead.\n2. `aiconfig` can be imported and **connected to application code** using the AIConfig SDK.\n', 'repo_name': 'aiconfig'}} ts_ns=1702175485063260400
2023-12-09 21:31:40,216 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '<div align="center"><picture>\n  <img alt="aiconfig" src="aiconfig-docs/static/img/readme_logo.png" />\n</picture></div>\n<br/>\n\n![Python](https://github.com/lastmile-ai/aiconfig/actions/workflows/main_python.yml/badge.svg)\n![Node](https://github.com/lastmile-ai/aiconfig/actions/workflows/main-typescript.yml/badge.svg)\n![Docs](https://github.com/lastmile-ai/aiconfig/actions/workflows/test-deploy-docs.yml/badge.svg)\n[![Discord](<https://img.shields.io/badge/Discord-LastMile%20AI-Blue?color=rgb(37%2C%20150%2C%20190)>)](https://discord.gg/qMqgzDae)\n\n> Full documentation: **[aiconfig.lastmileai.dev](https://aiconfig.lastmileai.dev/)**\n\n## Overview\n\nAIConfig saves prompts, models and model parameters as source control friendly configs. This allows you to iterate on prompts and model parameters _separately from your application code_.\n\n1. **Prompts as configs**: a [standardized JSON format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to store generative AI model settings, prompt inputs/outputs, and flexible metadata.\n2. **Model-agnostic SDK**: Python & Node SDKs to use `aiconfig` in your application code. AIConfig is designed to be **model-agnostic** and **multi-modal**, so you can extend it to work with any generative AI model, including text, image and audio.\n3. **AI Workbook editor**: A [notebook-like playground](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to edit `aiconfig` files visually, run prompts, tweak models and model settings, and chain things together.\n\n### What problem it solves\n\nToday, application code is tightly coupled with the gen AI settings for the application -- prompts, parameters, and model-specific logic is all jumbled in with app code.\n\n- results in increased complexity\n- makes it hard to iterate on the prompts or try different models easily\n- makes it hard to evaluate prompt/model performance\n\nAIConfig helps unwind complexity by separating prompts, model parameters, and model-specific logic from your application.\n\n- simplifies application code -- simply call `config.run()`\n- open the `aiconfig` in a playground to iterate quickly\n- version control and evaluate the `aiconfig` - it\'s the AI artifact for your application.\n\n![AIConfig flow](aiconfig-docs/static/img/aiconfig_dataflow.png)\n\n### Quicknav\n\n<ul style="margin-bottom:0; padding-bottom:0;">\n  <li><a href="#install">Getting Started</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig">Create an AIConfig</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig">Run a prompt</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/parameters">Pass data into prompts</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain">Prompt Chains</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig">Callbacks and monitoring</a></li>\n  </ul>\n  <li><a href="#aiconfig-sdk">SDK Cheatsheet</a></li>\n  <li><a href="#cookbooks">Cookbooks and guides</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT">CLI Chatbot</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig">RAG with AIConfig</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing">Prompt routing</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI">OpenAI function calling</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification">Chain of Verification</a></li>\n  </ul>\n  <li><a href="#supported-models">Supported models</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama">LLaMA2 example</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace">Hugging Face (Mistral-7B) example</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency">PaLM</a></li>\n  </ul>\n  <li><a href="#extensibility">Extensibility</a></li>\n  <li><a href="#contributing-to-aiconfig">Contributing</a></li>\n  <li><a href="#roadmap">Roadmap</a></li>\n  <li><a href="#faqs">FAQ</a></li>\n</ul>\n\n## Features\n\n- [x] **Source-control friendly** [`aiconfig` format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to save prompts and model settings, which you can use for evaluation, reproducibility and simplifying your application code.\n- [x] **Multi-modal and model agnostic**. Use with any model, and serialize/deserialize data with the same `aiconfig` format.\n- [x] **Prompt chaining and parameterization** with [{{handlebars}}](https://handlebarsjs.com/) templating syntax, allowing you to pass dynamic data into prompts (as well as between prompts).\n- [x] **Streaming** supported out of the box, allowing you to get playground-like streaming wherever you use `aiconfig`.\n- [x] **Notebook editor**. [AI Workbooks editor](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to visually create your `aiconfig`, and use the SDK to connect it to your application code.\n\n## Install\n\nInstall with your favorite package manager for Node or Python.\n\n### Node.js\n\n#### `npm` or `yarn`\n\n```bash\nnpm install aiconfig\n```\n\n```bash\nyarn add aiconfig\n```\n\n### Python\n\n#### `pip3` or `poetry`\n\n```bash\npip3 install python-aiconfig\n```\n\n```bash\npoetry add python-aiconfig\n```\n\n[Detailed installation instructions](https://aiconfig.lastmileai.dev/docs/getting-started/#installation).\n\n### Set your OpenAI API Key\n\n> **Note**: Make sure to specify the API keys (such as [`OPENAI_API_KEY`](https://platform.openai.com/api-keys)) in your environment before proceeding.\n\nIn your CLI, set the environment variable:\n\n```bash\nexport OPENAI_API_KEY=my_key\n```\n\n## Getting Started\n\n> We cover Python instructions here, for Node.js please see the [detailed Getting Started guide](https://aiconfig.lastmileai.dev/docs/getting-started)\n\nIn this quickstart, you will create a customizable NYC travel itinerary using `aiconfig`.\n\nThis AIConfig contains a prompt chain to get a list of travel activities from an LLM and then generate an itinerary in an order specified by the user.\n\n> **Link to tutorial code: [here](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Getting-Started)**\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/805173d1-0f83-44c5-b570-c776bb7dba66\n\n### Download `travel.aiconfig.json`\n\n> **Note**: Don\'t worry if you don\'t understand all the pieces of this yet, we\'ll go over it step by step.\n\n```json\n{\n  "name": "NYC Trip Planner",\n  "description": "Intrepid explorer with ChatGPT and AIConfig",\n  "schema_version": "latest",\n  "metadata": {\n    "models": {\n      "gpt-3.5-turbo": {\n        "model": "gpt-3.5-turbo",\n        "top_p": 1,\n        "temperature": 1\n      },\n      "gpt-4": {\n        "model": "gpt-4",\n        "max_tokens": 3000,\n        "system_prompt": "You are an expert travel coordinator with exquisite taste."\n      }\n    },\n    "default_model": "gpt-3.5-turbo"\n  },\n  "prompts": [\n    {\n      "name": "get_activities",\n      "input": "Tell me 10 fun attractions to do in NYC."\n    },\n    {\n      "name": "gen_itinerary",\n      "input": "Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}.",\n      "metadata": {\n        "model": "gpt-4",\n        "parameters": {\n          "order_by": "geographic location"\n        }\n      }\n    }\n  ]\n}\n```\n\n### Run the `get_activities` prompt.\n\nYou don\'t need to worry about how to run inference for the model; it\'s all handled by AIConfig. The prompt runs with gpt-3.5-turbo since that is the `default_model` for this AIConfig.\n\nCreate a new file called `app.py` and and enter the following code:\n\n```python\nimport asyncio\nfrom aiconfig import AIConfigRuntime, InferenceOptions\n\nasync def main():\n  # Load the aiconfig\n  config = AIConfigRuntime.load(\'travel.aiconfig.json\')\n\n  # Run a single prompt (with streaming)\n  inference_options = InferenceOptions(stream=True)\n  await config.run("get_activities", options=inference_options)\n\nasyncio.run(main())\n```\n\nNow run this in your terminal with the command:\n\n```bash\npython3 app.py\n```\n\n### Run the `gen_itinerary` prompt.\n\nIn your `app.py` file, change the last line to below:\n\n```python\nawait config.run("gen_itinerary", params=None, options=inference_options)\n```\n\nRe-run the command in your terminal:\n\n```bash\npython3 app.py\n```\n\nThis prompt depends on the output of `get_activities`. It also takes in parameters (user input) to determine the customized itinerary.\n\nLet\'s take a closer look:\n\n**`gen_itinerary` prompt:**\n\n```\n"Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}."\n```\n\n**prompt metadata:**\n\n```json\n{\n  "metadata": {\n    "model": "gpt-4",\n    "parameters": {\n      "order_by": "geographic location"\n    }\n  }\n}\n```\n\nObserve the following:\n\n1. The prompt depends on the output of the `get_activities` prompt.\n2. It also depends on an `order_by` parameter (using {{handlebars}} syntax)\n3. It uses **gpt-4**, whereas the `get_activities` prompt it depends on uses **gpt-3.5-turbo**.\n\n> Effectively, this is a prompt chain between `gen_itinerary` and `get_activities` prompts, _as well as_ as a model chain between **gpt-3.5-turbo** and **gpt-4**.\n\nLet\'s run this with AIConfig:\n\nReplace `config.run` above with this:\n\n```python\nawait config.run("gen_itinerary", params={"order_by": "duration"}, options=inference_options, run_with_dependencies=True)\n```\n\nNotice how simple the syntax is to perform a fairly complex task - running 2 different prompts across 2 different models and chaining one\'s output as part of the input of another.\n\nThe code will just run `get_activities`, then pipe its output as an input to `gen_itinerary`, and finally run `gen_itinerary`.\n\n### Save the AIConfig\n\nLet\'s save the AIConfig back to disk, and serialize the outputs from the latest inference run as well:\n\n```python\n# Save the aiconfig to disk. and serialize outputs from the model run\nconfig.save(\'updated.aiconfig.json\', include_outputs=True)\n```\n\n### Edit `aiconfig` in a notebook editor\n\nWe can iterate on an `aiconfig` using a notebook-like editor called an **AI Workbook**. Now that we have an `aiconfig` file artifact that encapsulates the generative AI part of our application, we can iterate on it separately from the application code that uses it.\n\n1. Go to https://lastmileai.dev.\n2. Go to Workbooks page: https://lastmileai.dev/workbooks\n3. Click dropdown from \'+ New Workbook\' and select \'Create from AIConfig\'\n4. Upload `travel.aiconfig.json`\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/5d901493-bbda-4f8e-93c7-dd9a91bf242e\n\nTry out the workbook playground here: **[NYC Travel Workbook](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9)**\n\n> **We are working on a local editor that you can run yourself. For now, please use the hosted version on https://lastmileai.dev.**\n\n### Additional Guides\n\nThere is a lot you can do with `aiconfig`. We have several other tutorials to help get you started:\n\n- [Create an AIConfig from scratch](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig)\n- [Run a prompt](https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig)\n- [Pass data into prompts](https://aiconfig.lastmileai.dev/docs/overview/parameters)\n- [Prompt chains](https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain)\n- [Callbacks and monitoring](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\nHere are some example uses:\n\n- [CLI Chatbot](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT)\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [Chain of thought](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### OpenAI Introspection API\n\nIf you are already using OpenAI completion API\'s in your application, you can get started very quickly to start saving the messages in an `aiconfig`.\n\nUsage: see openai_wrapper.ipynb.\n\nNow you can continue using `openai` completion API as normal. When you want to save the config, just call `new_config.save()` and all your openai completion calls will get serialized to disk.\n\n> [**Detailed guide here**](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig#openai-api-python-wrapper)\n\n## Supported Models\n\nAIConfig supports the following models out of the box:\n\n- OpenAI chat models (GPT-3, GPT-3.5, GPT-4)\n- LLaMA2 (running locally)\n- Google PaLM models (PaLM chat)\n- Hugging Face text generation models (e.g. Mistral-7B)\n\n### Examples\n\n- [OpenAI](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n> If you need to use a model that isn\'t provided out of the box, you can implement a `ModelParser` for it (see [Extending AIConfig](#extending-aiconfig)). **We welcome [contributions](https://aiconfig.lastmileai.dev/docs/contributing)**\n\n## AIConfig Schema\n\n[AIConfig specification](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format)\n\n## AIConfig SDK\n\n> Read the [Usage Guide](https://aiconfig.lastmileai.dev/docs/usage-guide) for more details.\n\nThe AIConfig SDK supports CRUD operations for prompts, models, parameters and metadata. Here are some common examples.\n\nThe root interface is the `AIConfigRuntime` object. That is the entrypoint for interacting with an AIConfig programmatically.\n\nLet\'s go over a few key CRUD operations to give a glimpse.\n\n### AIConfig `create`\n\n```python\nconfig = AIConfigRuntime.create("aiconfig name", "description")\n```\n\n### Prompt `resolve`\n\n`resolve` deserializes an existing `Prompt` into the data object that its model expects.\n\n```python\nconfig.resolve("prompt_name", params)\n```\n\n`params` are overrides you can specify to resolve any `{{handlebars}}` templates in the prompt. See the `gen_itinerary` prompt in the Getting Started example.\n\n### Prompt `serialize`\n\n`serialize` is the inverse of `resolve` -- it serializes the data object that a model understands into a `Prompt` object that can be serialized into the `aiconfig` format.\n\n```python\nconfig.serialize("model_name", data, "prompt_name")\n```\n\n### Prompt `run`\n\n`run` is used to run inference for the specified `Prompt`.\n\n```python\nconfig.run("prompt_name", params)\n```\n\n### `run_with_dependencies`\n\nThis is a variant of `run` -- this re-runs all prompt dependencies.\nFor example, in [`travel.aiconfig.json`](#download-travelaiconfigjson), the `gen_itinerary` prompt references the output of the `get_activities` prompt using `{{get_activities.output}}`.\n\nRunning this function will first execute `get_activities`, and use its output to resolve the `gen_itinerary` prompt before executing it.\nThis is transitive, so it computes the Directed Acyclic Graph of dependencies to execute. Complex relationships can be modeled this way.\n\n```python\nconfig.run_with_dependencies("gen_itinerary")\n```\n\n### Updating metadata and parameters\n\nUse the `get/set_metadata` and `get/set_parameter` methods to interact with metadata and parameters (`set_parameter` is just syntactic sugar to update `"metadata.parameters"`)\n\n```python\nconfig.set_metadata("key", data, "prompt_name")\n```\n\nNote: if `"prompt_name"` is specified, the metadata is updated specifically for that prompt. Otherwise, the global metadata is updated.\n\n### Register new `ModelParser`\n\nUse the `AIConfigRuntime.register_model_parser` if you want to use a different `ModelParser`, or configure AIConfig to work with an additional model.\n\nAIConfig uses the model name string to retrieve the right `ModelParser` for a given Prompt (see `AIConfigRuntime.get_model_parser`), so you can register a different ModelParser for the same ID to override which `ModelParser` handles a Prompt.\n\nFor example, suppose I want to use `MyOpenAIModelParser` to handle `gpt-4` prompts. I can do the following at the start of my application:\n\n```python\nAIConfigRuntime.register_model_parser(myModelParserInstance, ["gpt-4"])\n```\n\n### Callback events\n\nUse callback events to trace and monitor what\'s going on -- helpful for debugging and observability.\n\n```python\nfrom aiconfig import AIConfigRuntime, CallbackEvent, CallbackManager\nconfig = AIConfigRuntime.load(\'aiconfig.json\')\n\nasync def my_custom_callback(event: CallbackEvent) -> None:\n  print(f"Event triggered: {event.name}", event)\n\ncallback_manager = CallbackManager([my_custom_callback])\nconfig.set_callback_manager(callback_manager)\n\nawait config.run("prompt_name")\n```\n\n[**Read more** here](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\n## Extensibility\n\nAIConfig is designed to be customized and extended for your use-case. The [Extensibility](/docs/extensibility) guide goes into more detail.\n\nCurrently, there are 3 core ways to extend AIConfig:\n\n1. [Supporting other models](https://aiconfig.lastmileai.dev/docs/extensibility#1-bring-your-own-model) - define a ModelParser extension\n2. [Callback event handlers](https://aiconfig.lastmileai.dev/docs/extensibility#2-callback-handlers) - tracing and monitoring\n3. [Custom metadata](https://aiconfig.lastmileai.dev/docs/extensibility#3-custom-metadata) - save custom fields in `aiconfig`\n\n## Contributing to `aiconfig`\n\nThis is our first open-source project and we\'d love your help.\n\nSee our [contributing guidelines](https://aiconfig.lastmileai.dev/docs/contributing) -- we would especially love help adding support for additional models that the community wants.\n\n## Cookbooks\n\nWe provide several guides to demonstrate the power of `aiconfig`.\n\n> **See the [`cookbooks`](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks) folder for examples to clone.**\n\n### Chatbot\n\n- [Wizard GPT](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT) - speak to a wizard on your CLI\n\n- [CLI-mate](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Cli-Mate) - help you make code-mods interactively on your codebase.\n\n### Retrieval Augmented Generated (RAG)\n\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n\nAt its core, RAG is about passing data into prompts. Read how to [pass data](/docs/overview/parameters) with AIConfig.\n\n### Function calling\n\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n\n### Prompt routing\n\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n\n### Chain of Thought\n\nA variant of chain-of-thought is Chain of Verification, used to help reduce hallucinations. Check out the aiconfig cookbook for CoVe:\n\n- [Chain of Verification](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### Using local LLaMA2 with `aiconfig`\n\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n\n### Hugging Face text generation\n\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n\n### Google PaLM\n\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n## Roadmap\n\nThis project is under active development.\n\nIf you\'d like to help, please see the [contributing guidelines](#contributing-to-aiconfig).\n\nPlease create issues for additional capabilities you\'d like to see.\n\nHere\'s what\'s already on our roadmap:\n\n- Evaluation interfaces: allow `aiconfig` artifacts to be evaluated with user-defined eval functions.\n  - We are also considering integrating with existing evaluation frameworks.\n- Local editor for `aiconfig`: enable you to interact with aiconfigs more intuitively.\n- OpenAI Assistants API support\n- Multi-modal ModelParsers:\n  - GPT4-V support\n  - DALLE-3\n  - Whisper\n  - HuggingFace image generation\n\n## FAQs\n\n### How should I edit an `aiconfig` file?\n\nEditing a configshould be done either programmatically via SDK or via the UI (workbooks):\n\n- [Programmatic](https://github.com/lastmile-ai/aiconfig/blob/main/cookbooks/Create-AIConfig-Programmatically/create_aiconfig_programmatically.ipynb) editing.\n\n- [Edit with a workbook](#edit-aiconfig-in-a-notebook-editor) editor: this is similar to editing an ipynb file as a notebook (most people never touch the json ipynb directly)\n\nYou should only edit the `aiconfig` by hand for minor modifications, like tweaking a prompt string or updating some metadata.\n\n### Does this support custom endpoints?\n\nOut of the box, AIConfig already supports all OpenAI GPT\\* models, Googles PaLM model and any textgeneration model on Hugging Face (like Mistral). See [Supported Models](#supported-models) for more details.\n\nAdditionally, you can install `aiconfig` [extensions](https://github.com/lastmile-ai/aiconfig/tree/main/extensions) for additional models (see question below).\n\n### Is OpenAI function calling supported?\n\nYes. [This example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI) goes through how to do it.\n\nWe are also working on adding support for the Assistants API.\n\n### How can I use aiconfig with my own model endpoint?\n\nModel support is implemented as ModelParsers in the AIConfig SDK, and the idea is that anyone, including you, can define a ModelParser (and even publish it as an extension package).\n\nAll thats needed to use a model with AIConfig is a ModelParser that knows\n\n- how to serialize data from a model into the aiconfig format\n- how to deserialize data from an aiconfig into the type the model expects\n- how to run inference for model.\n\nFor more details, see [Extensibility](https://aiconfig.lastmileai.dev/docs/extensibility).\n\n### When should I store outputs in an `aiconfig`?\n\nThe `AIConfigRuntime` object is used to interact with an aiconfig programmatically (see [SDK usage guide](#aiconfig-sdk)). As you run prompts, this object keeps track of the outputs returned from the model.\n\nYou can choose to serialize these outputs back into the `aiconfig` by using the `config.save(include_outputs=True)` API. This can be useful for preserving context -- think of it like session state.\n\nFor example, you can use aiconfig to create a chatbot, and use the same format to save the chat history so it can be resumed for the next session.\n\nYou can also choose to save outputs to a _different_ file than the original config -- `config.save("history.aiconfig.json", include_outputs=True)`.\n\n### Why should I use `aiconfig` instead of things like [configurator](https://pypi.org/project/configurator/)?\n\nIt helps to have a [standardized format](http://aiconfig.lastmileai.dev/docs/overview/ai-config-format) specifically for storing generative AI prompts, inference results, model parameters and arbitrary metadata, as opposed to a general-purpose configuration schema.\n\nWith that standardization, you just need a layer that knows how to serialize/deserialize from that format into whatever the inference endpoints require.\n\n### This looks similar to `ipynb` for Jupyter notebooks\n\nWe believe that notebooks are a perfect iteration environment for generative AI -- they are flexible, multi-modal, and collaborative.\n\nThe multi-modality and flexibility offered by notebooks and [`ipynb`](https://ipython.org/ipython-doc/3/notebook/nbformat.html) offers a good interaction model for generative AI. The `aiconfig` file format is extensible like `ipynb`, and AI Workbook editor allows rapid iteration in a notebook-like IDE.\n\n_AI Workbooks are to AIConfig what Jupyter notebooks are to `ipynb`_\n\nThere are 2 areas where we are going beyond what notebooks offer:\n\n1. `aiconfig` is more **source-control friendly** than `ipynb`. `ipynb` stores binary data (images, etc.) by encoding it in the file, while `aiconfig` recommends using file URI references instead.\n2. `aiconfig` can be imported and **connected to application code** using the AIConfig SDK.\n', 'repo_name': 'aiconfig'}} ts_ns=1702175485063260400
2023-12-09 21:31:40,216 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '<div align="center"><picture>\n  <img alt="aiconfig" src="aiconfig-docs/static/img/readme_logo.png" />\n</picture></div>\n<br/>\n\n![Python](https://github.com/lastmile-ai/aiconfig/actions/workflows/main_python.yml/badge.svg)\n![Node](https://github.com/lastmile-ai/aiconfig/actions/workflows/main-typescript.yml/badge.svg)\n![Docs](https://github.com/lastmile-ai/aiconfig/actions/workflows/test-deploy-docs.yml/badge.svg)\n[![Discord](<https://img.shields.io/badge/Discord-LastMile%20AI-Blue?color=rgb(37%2C%20150%2C%20190)>)](https://discord.gg/qMqgzDae)\n\n> Full documentation: **[aiconfig.lastmileai.dev](https://aiconfig.lastmileai.dev/)**\n\n## Overview\n\nAIConfig saves prompts, models and model parameters as source control friendly configs. This allows you to iterate on prompts and model parameters _separately from your application code_.\n\n1. **Prompts as configs**: a [standardized JSON format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to store generative AI model settings, prompt inputs/outputs, and flexible metadata.\n2. **Model-agnostic SDK**: Python & Node SDKs to use `aiconfig` in your application code. AIConfig is designed to be **model-agnostic** and **multi-modal**, so you can extend it to work with any generative AI model, including text, image and audio.\n3. **AI Workbook editor**: A [notebook-like playground](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to edit `aiconfig` files visually, run prompts, tweak models and model settings, and chain things together.\n\n### What problem it solves\n\nToday, application code is tightly coupled with the gen AI settings for the application -- prompts, parameters, and model-specific logic is all jumbled in with app code.\n\n- results in increased complexity\n- makes it hard to iterate on the prompts or try different models easily\n- makes it hard to evaluate prompt/model performance\n\nAIConfig helps unwind complexity by separating prompts, model parameters, and model-specific logic from your application.\n\n- simplifies application code -- simply call `config.run()`\n- open the `aiconfig` in a playground to iterate quickly\n- version control and evaluate the `aiconfig` - it\'s the AI artifact for your application.\n\n![AIConfig flow](aiconfig-docs/static/img/aiconfig_dataflow.png)\n\n### Quicknav\n\n<ul style="margin-bottom:0; padding-bottom:0;">\n  <li><a href="#install">Getting Started</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig">Create an AIConfig</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig">Run a prompt</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/parameters">Pass data into prompts</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain">Prompt Chains</a></li>\n    <li><a href="https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig">Callbacks and monitoring</a></li>\n  </ul>\n  <li><a href="#aiconfig-sdk">SDK Cheatsheet</a></li>\n  <li><a href="#cookbooks">Cookbooks and guides</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT">CLI Chatbot</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig">RAG with AIConfig</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing">Prompt routing</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI">OpenAI function calling</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification">Chain of Verification</a></li>\n  </ul>\n  <li><a href="#supported-models">Supported models</a></li>\n  <ul style="margin-bottom:0; padding-bottom:0;">\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama">LLaMA2 example</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace">Hugging Face (Mistral-7B) example</a></li>\n    <li><a href="https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency">PaLM</a></li>\n  </ul>\n  <li><a href="#extensibility">Extensibility</a></li>\n  <li><a href="#contributing-to-aiconfig">Contributing</a></li>\n  <li><a href="#roadmap">Roadmap</a></li>\n  <li><a href="#faqs">FAQ</a></li>\n</ul>\n\n## Features\n\n- [x] **Source-control friendly** [`aiconfig` format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to save prompts and model settings, which you can use for evaluation, reproducibility and simplifying your application code.\n- [x] **Multi-modal and model agnostic**. Use with any model, and serialize/deserialize data with the same `aiconfig` format.\n- [x] **Prompt chaining and parameterization** with [{{handlebars}}](https://handlebarsjs.com/) templating syntax, allowing you to pass dynamic data into prompts (as well as between prompts).\n- [x] **Streaming** supported out of the box, allowing you to get playground-like streaming wherever you use `aiconfig`.\n- [x] **Notebook editor**. [AI Workbooks editor](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to visually create your `aiconfig`, and use the SDK to connect it to your application code.\n\n## Install\n\nInstall with your favorite package manager for Node or Python.\n\n### Node.js\n\n#### `npm` or `yarn`\n\n```bash\nnpm install aiconfig\n```\n\n```bash\nyarn add aiconfig\n```\n\n### Python\n\n#### `pip3` or `poetry`\n\n```bash\npip3 install python-aiconfig\n```\n\n```bash\npoetry add python-aiconfig\n```\n\n[Detailed installation instructions](https://aiconfig.lastmileai.dev/docs/getting-started/#installation).\n\n### Set your OpenAI API Key\n\n> **Note**: Make sure to specify the API keys (such as [`OPENAI_API_KEY`](https://platform.openai.com/api-keys)) in your environment before proceeding.\n\nIn your CLI, set the environment variable:\n\n```bash\nexport OPENAI_API_KEY=my_key\n```\n\n## Getting Started\n\n> We cover Python instructions here, for Node.js please see the [detailed Getting Started guide](https://aiconfig.lastmileai.dev/docs/getting-started)\n\nIn this quickstart, you will create a customizable NYC travel itinerary using `aiconfig`.\n\nThis AIConfig contains a prompt chain to get a list of travel activities from an LLM and then generate an itinerary in an order specified by the user.\n\n> **Link to tutorial code: [here](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Getting-Started)**\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/805173d1-0f83-44c5-b570-c776bb7dba66\n\n### Download `travel.aiconfig.json`\n\n> **Note**: Don\'t worry if you don\'t understand all the pieces of this yet, we\'ll go over it step by step.\n\n```json\n{\n  "name": "NYC Trip Planner",\n  "description": "Intrepid explorer with ChatGPT and AIConfig",\n  "schema_version": "latest",\n  "metadata": {\n    "models": {\n      "gpt-3.5-turbo": {\n        "model": "gpt-3.5-turbo",\n        "top_p": 1,\n        "temperature": 1\n      },\n      "gpt-4": {\n        "model": "gpt-4",\n        "max_tokens": 3000,\n        "system_prompt": "You are an expert travel coordinator with exquisite taste."\n      }\n    },\n    "default_model": "gpt-3.5-turbo"\n  },\n  "prompts": [\n    {\n      "name": "get_activities",\n      "input": "Tell me 10 fun attractions to do in NYC."\n    },\n    {\n      "name": "gen_itinerary",\n      "input": "Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}.",\n      "metadata": {\n        "model": "gpt-4",\n        "parameters": {\n          "order_by": "geographic location"\n        }\n      }\n    }\n  ]\n}\n```\n\n### Run the `get_activities` prompt.\n\nYou don\'t need to worry about how to run inference for the model; it\'s all handled by AIConfig. The prompt runs with gpt-3.5-turbo since that is the `default_model` for this AIConfig.\n\nCreate a new file called `app.py` and and enter the following code:\n\n```python\nimport asyncio\nfrom aiconfig import AIConfigRuntime, InferenceOptions\n\nasync def main():\n  # Load the aiconfig\n  config = AIConfigRuntime.load(\'travel.aiconfig.json\')\n\n  # Run a single prompt (with streaming)\n  inference_options = InferenceOptions(stream=True)\n  await config.run("get_activities", options=inference_options)\n\nasyncio.run(main())\n```\n\nNow run this in your terminal with the command:\n\n```bash\npython3 app.py\n```\n\n### Run the `gen_itinerary` prompt.\n\nIn your `app.py` file, change the last line to below:\n\n```python\nawait config.run("gen_itinerary", params=None, options=inference_options)\n```\n\nRe-run the command in your terminal:\n\n```bash\npython3 app.py\n```\n\nThis prompt depends on the output of `get_activities`. It also takes in parameters (user input) to determine the customized itinerary.\n\nLet\'s take a closer look:\n\n**`gen_itinerary` prompt:**\n\n```\n"Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}."\n```\n\n**prompt metadata:**\n\n```json\n{\n  "metadata": {\n    "model": "gpt-4",\n    "parameters": {\n      "order_by": "geographic location"\n    }\n  }\n}\n```\n\nObserve the following:\n\n1. The prompt depends on the output of the `get_activities` prompt.\n2. It also depends on an `order_by` parameter (using {{handlebars}} syntax)\n3. It uses **gpt-4**, whereas the `get_activities` prompt it depends on uses **gpt-3.5-turbo**.\n\n> Effectively, this is a prompt chain between `gen_itinerary` and `get_activities` prompts, _as well as_ as a model chain between **gpt-3.5-turbo** and **gpt-4**.\n\nLet\'s run this with AIConfig:\n\nReplace `config.run` above with this:\n\n```python\nawait config.run("gen_itinerary", params={"order_by": "duration"}, options=inference_options, run_with_dependencies=True)\n```\n\nNotice how simple the syntax is to perform a fairly complex task - running 2 different prompts across 2 different models and chaining one\'s output as part of the input of another.\n\nThe code will just run `get_activities`, then pipe its output as an input to `gen_itinerary`, and finally run `gen_itinerary`.\n\n### Save the AIConfig\n\nLet\'s save the AIConfig back to disk, and serialize the outputs from the latest inference run as well:\n\n```python\n# Save the aiconfig to disk. and serialize outputs from the model run\nconfig.save(\'updated.aiconfig.json\', include_outputs=True)\n```\n\n### Edit `aiconfig` in a notebook editor\n\nWe can iterate on an `aiconfig` using a notebook-like editor called an **AI Workbook**. Now that we have an `aiconfig` file artifact that encapsulates the generative AI part of our application, we can iterate on it separately from the application code that uses it.\n\n1. Go to https://lastmileai.dev.\n2. Go to Workbooks page: https://lastmileai.dev/workbooks\n3. Click dropdown from \'+ New Workbook\' and select \'Create from AIConfig\'\n4. Upload `travel.aiconfig.json`\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/5d901493-bbda-4f8e-93c7-dd9a91bf242e\n\nTry out the workbook playground here: **[NYC Travel Workbook](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9)**\n\n> **We are working on a local editor that you can run yourself. For now, please use the hosted version on https://lastmileai.dev.**\n\n### Additional Guides\n\nThere is a lot you can do with `aiconfig`. We have several other tutorials to help get you started:\n\n- [Create an AIConfig from scratch](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig)\n- [Run a prompt](https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig)\n- [Pass data into prompts](https://aiconfig.lastmileai.dev/docs/overview/parameters)\n- [Prompt chains](https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain)\n- [Callbacks and monitoring](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\nHere are some example uses:\n\n- [CLI Chatbot](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT)\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [Chain of thought](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### OpenAI Introspection API\n\nIf you are already using OpenAI completion API\'s in your application, you can get started very quickly to start saving the messages in an `aiconfig`.\n\nUsage: see openai_wrapper.ipynb.\n\nNow you can continue using `openai` completion API as normal. When you want to save the config, just call `new_config.save()` and all your openai completion calls will get serialized to disk.\n\n> [**Detailed guide here**](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig#openai-api-python-wrapper)\n\n## Supported Models\n\nAIConfig supports the following models out of the box:\n\n- OpenAI chat models (GPT-3, GPT-3.5, GPT-4)\n- LLaMA2 (running locally)\n- Google PaLM models (PaLM chat)\n- Hugging Face text generation models (e.g. Mistral-7B)\n\n### Examples\n\n- [OpenAI](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n> If you need to use a model that isn\'t provided out of the box, you can implement a `ModelParser` for it (see [Extending AIConfig](#extending-aiconfig)). **We welcome [contributions](https://aiconfig.lastmileai.dev/docs/contributing)**\n\n## AIConfig Schema\n\n[AIConfig specification](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format)\n\n## AIConfig SDK\n\n> Read the [Usage Guide](https://aiconfig.lastmileai.dev/docs/usage-guide) for more details.\n\nThe AIConfig SDK supports CRUD operations for prompts, models, parameters and metadata. Here are some common examples.\n\nThe root interface is the `AIConfigRuntime` object. That is the entrypoint for interacting with an AIConfig programmatically.\n\nLet\'s go over a few key CRUD operations to give a glimpse.\n\n### AIConfig `create`\n\n```python\nconfig = AIConfigRuntime.create("aiconfig name", "description")\n```\n\n### Prompt `resolve`\n\n`resolve` deserializes an existing `Prompt` into the data object that its model expects.\n\n```python\nconfig.resolve("prompt_name", params)\n```\n\n`params` are overrides you can specify to resolve any `{{handlebars}}` templates in the prompt. See the `gen_itinerary` prompt in the Getting Started example.\n\n### Prompt `serialize`\n\n`serialize` is the inverse of `resolve` -- it serializes the data object that a model understands into a `Prompt` object that can be serialized into the `aiconfig` format.\n\n```python\nconfig.serialize("model_name", data, "prompt_name")\n```\n\n### Prompt `run`\n\n`run` is used to run inference for the specified `Prompt`.\n\n```python\nconfig.run("prompt_name", params)\n```\n\n### `run_with_dependencies`\n\nThis is a variant of `run` -- this re-runs all prompt dependencies.\nFor example, in [`travel.aiconfig.json`](#download-travelaiconfigjson), the `gen_itinerary` prompt references the output of the `get_activities` prompt using `{{get_activities.output}}`.\n\nRunning this function will first execute `get_activities`, and use its output to resolve the `gen_itinerary` prompt before executing it.\nThis is transitive, so it computes the Directed Acyclic Graph of dependencies to execute. Complex relationships can be modeled this way.\n\n```python\nconfig.run_with_dependencies("gen_itinerary")\n```\n\n### Updating metadata and parameters\n\nUse the `get/set_metadata` and `get/set_parameter` methods to interact with metadata and parameters (`set_parameter` is just syntactic sugar to update `"metadata.parameters"`)\n\n```python\nconfig.set_metadata("key", data, "prompt_name")\n```\n\nNote: if `"prompt_name"` is specified, the metadata is updated specifically for that prompt. Otherwise, the global metadata is updated.\n\n### Register new `ModelParser`\n\nUse the `AIConfigRuntime.register_model_parser` if you want to use a different `ModelParser`, or configure AIConfig to work with an additional model.\n\nAIConfig uses the model name string to retrieve the right `ModelParser` for a given Prompt (see `AIConfigRuntime.get_model_parser`), so you can register a different ModelParser for the same ID to override which `ModelParser` handles a Prompt.\n\nFor example, suppose I want to use `MyOpenAIModelParser` to handle `gpt-4` prompts. I can do the following at the start of my application:\n\n```python\nAIConfigRuntime.register_model_parser(myModelParserInstance, ["gpt-4"])\n```\n\n### Callback events\n\nUse callback events to trace and monitor what\'s going on -- helpful for debugging and observability.\n\n```python\nfrom aiconfig import AIConfigRuntime, CallbackEvent, CallbackManager\nconfig = AIConfigRuntime.load(\'aiconfig.json\')\n\nasync def my_custom_callback(event: CallbackEvent) -> None:\n  print(f"Event triggered: {event.name}", event)\n\ncallback_manager = CallbackManager([my_custom_callback])\nconfig.set_callback_manager(callback_manager)\n\nawait config.run("prompt_name")\n```\n\n[**Read more** here](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\n## Extensibility\n\nAIConfig is designed to be customized and extended for your use-case. The [Extensibility](/docs/extensibility) guide goes into more detail.\n\nCurrently, there are 3 core ways to extend AIConfig:\n\n1. [Supporting other models](https://aiconfig.lastmileai.dev/docs/extensibility#1-bring-your-own-model) - define a ModelParser extension\n2. [Callback event handlers](https://aiconfig.lastmileai.dev/docs/extensibility#2-callback-handlers) - tracing and monitoring\n3. [Custom metadata](https://aiconfig.lastmileai.dev/docs/extensibility#3-custom-metadata) - save custom fields in `aiconfig`\n\n## Contributing to `aiconfig`\n\nThis is our first open-source project and we\'d love your help.\n\nSee our [contributing guidelines](https://aiconfig.lastmileai.dev/docs/contributing) -- we would especially love help adding support for additional models that the community wants.\n\n## Cookbooks\n\nWe provide several guides to demonstrate the power of `aiconfig`.\n\n> **See the [`cookbooks`](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks) folder for examples to clone.**\n\n### Chatbot\n\n- [Wizard GPT](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT) - speak to a wizard on your CLI\n\n- [CLI-mate](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Cli-Mate) - help you make code-mods interactively on your codebase.\n\n### Retrieval Augmented Generated (RAG)\n\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n\nAt its core, RAG is about passing data into prompts. Read how to [pass data](/docs/overview/parameters) with AIConfig.\n\n### Function calling\n\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n\n### Prompt routing\n\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n\n### Chain of Thought\n\nA variant of chain-of-thought is Chain of Verification, used to help reduce hallucinations. Check out the aiconfig cookbook for CoVe:\n\n- [Chain of Verification](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### Using local LLaMA2 with `aiconfig`\n\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n\n### Hugging Face text generation\n\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n\n### Google PaLM\n\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n## Roadmap\n\nThis project is under active development.\n\nIf you\'d like to help, please see the [contributing guidelines](#contributing-to-aiconfig).\n\nPlease create issues for additional capabilities you\'d like to see.\n\nHere\'s what\'s already on our roadmap:\n\n- Evaluation interfaces: allow `aiconfig` artifacts to be evaluated with user-defined eval functions.\n  - We are also considering integrating with existing evaluation frameworks.\n- Local editor for `aiconfig`: enable you to interact with aiconfigs more intuitively.\n- OpenAI Assistants API support\n- Multi-modal ModelParsers:\n  - GPT4-V support\n  - DALLE-3\n  - Whisper\n  - HuggingFace image generation\n\n## FAQs\n\n### How should I edit an `aiconfig` file?\n\nEditing a configshould be done either programmatically via SDK or via the UI (workbooks):\n\n- [Programmatic](https://github.com/lastmile-ai/aiconfig/blob/main/cookbooks/Create-AIConfig-Programmatically/create_aiconfig_programmatically.ipynb) editing.\n\n- [Edit with a workbook](#edit-aiconfig-in-a-notebook-editor) editor: this is similar to editing an ipynb file as a notebook (most people never touch the json ipynb directly)\n\nYou should only edit the `aiconfig` by hand for minor modifications, like tweaking a prompt string or updating some metadata.\n\n### Does this support custom endpoints?\n\nOut of the box, AIConfig already supports all OpenAI GPT\\* models, Googles PaLM model and any textgeneration model on Hugging Face (like Mistral). See [Supported Models](#supported-models) for more details.\n\nAdditionally, you can install `aiconfig` [extensions](https://github.com/lastmile-ai/aiconfig/tree/main/extensions) for additional models (see question below).\n\n### Is OpenAI function calling supported?\n\nYes. [This example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI) goes through how to do it.\n\nWe are also working on adding support for the Assistants API.\n\n### How can I use aiconfig with my own model endpoint?\n\nModel support is implemented as ModelParsers in the AIConfig SDK, and the idea is that anyone, including you, can define a ModelParser (and even publish it as an extension package).\n\nAll thats needed to use a model with AIConfig is a ModelParser that knows\n\n- how to serialize data from a model into the aiconfig format\n- how to deserialize data from an aiconfig into the type the model expects\n- how to run inference for model.\n\nFor more details, see [Extensibility](https://aiconfig.lastmileai.dev/docs/extensibility).\n\n### When should I store outputs in an `aiconfig`?\n\nThe `AIConfigRuntime` object is used to interact with an aiconfig programmatically (see [SDK usage guide](#aiconfig-sdk)). As you run prompts, this object keeps track of the outputs returned from the model.\n\nYou can choose to serialize these outputs back into the `aiconfig` by using the `config.save(include_outputs=True)` API. This can be useful for preserving context -- think of it like session state.\n\nFor example, you can use aiconfig to create a chatbot, and use the same format to save the chat history so it can be resumed for the next session.\n\nYou can also choose to save outputs to a _different_ file than the original config -- `config.save("history.aiconfig.json", include_outputs=True)`.\n\n### Why should I use `aiconfig` instead of things like [configurator](https://pypi.org/project/configurator/)?\n\nIt helps to have a [standardized format](http://aiconfig.lastmileai.dev/docs/overview/ai-config-format) specifically for storing generative AI prompts, inference results, model parameters and arbitrary metadata, as opposed to a general-purpose configuration schema.\n\nWith that standardization, you just need a layer that knows how to serialize/deserialize from that format into whatever the inference endpoints require.\n\n### This looks similar to `ipynb` for Jupyter notebooks\n\nWe believe that notebooks are a perfect iteration environment for generative AI -- they are flexible, multi-modal, and collaborative.\n\nThe multi-modality and flexibility offered by notebooks and [`ipynb`](https://ipython.org/ipython-doc/3/notebook/nbformat.html) offers a good interaction model for generative AI. The `aiconfig` file format is extensible like `ipynb`, and AI Workbook editor allows rapid iteration in a notebook-like IDE.\n\n_AI Workbooks are to AIConfig what Jupyter notebooks are to `ipynb`_\n\nThere are 2 areas where we are going beyond what notebooks offer:\n\n1. `aiconfig` is more **source-control friendly** than `ipynb`. `ipynb` stores binary data (images, etc.) by encoding it in the file, while `aiconfig` recommends using file URI references instead.\n2. `aiconfig` can be imported and **connected to application code** using the AIConfig SDK.\n', 'repo_name': 'aiconfig'}} ts_ns=1702175485063260400
2023-12-09 21:31:40,225 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'temperature': 0.75, 'frequency_penalty': 0, 'presence_penalty': 0, 'model': 'gpt-4', 'top_p': 1, 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is aiconfig. The READMEfile is:\n&lt;div align=&quot;center&quot;&gt;&lt;picture&gt;\n  &lt;img alt=&quot;aiconfig&quot; src=&quot;aiconfig-docs/static/img/readme_logo.png&quot; /&gt;\n&lt;/picture&gt;&lt;/div&gt;\n&lt;br/&gt;\n\n![Python](https://github.com/lastmile-ai/aiconfig/actions/workflows/main_python.yml/badge.svg)\n![Node](https://github.com/lastmile-ai/aiconfig/actions/workflows/main-typescript.yml/badge.svg)\n![Docs](https://github.com/lastmile-ai/aiconfig/actions/workflows/test-deploy-docs.yml/badge.svg)\n[![Discord](&lt;https://img.shields.io/badge/Discord-LastMile%20AI-Blue?color=rgb(37%2C%20150%2C%20190)&gt;)](https://discord.gg/qMqgzDae)\n\n&gt; Full documentation: **[aiconfig.lastmileai.dev](https://aiconfig.lastmileai.dev/)**\n\n## Overview\n\nAIConfig saves prompts, models and model parameters as source control friendly configs. This allows you to iterate on prompts and model parameters _separately from your application code_.\n\n1. **Prompts as configs**: a [standardized JSON format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to store generative AI model settings, prompt inputs/outputs, and flexible metadata.\n2. **Model-agnostic SDK**: Python &amp; Node SDKs to use &#x60;aiconfig&#x60; in your application code. AIConfig is designed to be **model-agnostic** and **multi-modal**, so you can extend it to work with any generative AI model, including text, image and audio.\n3. **AI Workbook editor**: A [notebook-like playground](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to edit &#x60;aiconfig&#x60; files visually, run prompts, tweak models and model settings, and chain things together.\n\n### What problem it solves\n\nToday, application code is tightly coupled with the gen AI settings for the application -- prompts, parameters, and model-specific logic is all jumbled in with app code.\n\n- results in increased complexity\n- makes it hard to iterate on the prompts or try different models easily\n- makes it hard to evaluate prompt/model performance\n\nAIConfig helps unwind complexity by separating prompts, model parameters, and model-specific logic from your application.\n\n- simplifies application code -- simply call &#x60;config.run()&#x60;\n- open the &#x60;aiconfig&#x60; in a playground to iterate quickly\n- version control and evaluate the &#x60;aiconfig&#x60; - it&#x27;s the AI artifact for your application.\n\n![AIConfig flow](aiconfig-docs/static/img/aiconfig_dataflow.png)\n\n### Quicknav\n\n&lt;ul style=&quot;margin-bottom:0; padding-bottom:0;&quot;&gt;\n  &lt;li&gt;&lt;a href=&quot;#install&quot;&gt;Getting Started&lt;/a&gt;&lt;/li&gt;\n  &lt;ul style=&quot;margin-bottom:0; padding-bottom:0;&quot;&gt;\n    &lt;li&gt;&lt;a href=&quot;https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig&quot;&gt;Create an AIConfig&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=&quot;https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig&quot;&gt;Run a prompt&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=&quot;https://aiconfig.lastmileai.dev/docs/overview/parameters&quot;&gt;Pass data into prompts&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=&quot;https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain&quot;&gt;Prompt Chains&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=&quot;https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig&quot;&gt;Callbacks and monitoring&lt;/a&gt;&lt;/li&gt;\n  &lt;/ul&gt;\n  &lt;li&gt;&lt;a href=&quot;#aiconfig-sdk&quot;&gt;SDK Cheatsheet&lt;/a&gt;&lt;/li&gt;\n  &lt;li&gt;&lt;a href=&quot;#cookbooks&quot;&gt;Cookbooks and guides&lt;/a&gt;&lt;/li&gt;\n  &lt;ul style=&quot;margin-bottom:0; padding-bottom:0;&quot;&gt;\n    &lt;li&gt;&lt;a href=&quot;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT&quot;&gt;CLI Chatbot&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=&quot;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig&quot;&gt;RAG with AIConfig&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=&quot;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing&quot;&gt;Prompt routing&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=&quot;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI&quot;&gt;OpenAI function calling&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=&quot;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification&quot;&gt;Chain of Verification&lt;/a&gt;&lt;/li&gt;\n  &lt;/ul&gt;\n  &lt;li&gt;&lt;a href=&quot;#supported-models&quot;&gt;Supported models&lt;/a&gt;&lt;/li&gt;\n  &lt;ul style=&quot;margin-bottom:0; padding-bottom:0;&quot;&gt;\n    &lt;li&gt;&lt;a href=&quot;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama&quot;&gt;LLaMA2 example&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=&quot;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace&quot;&gt;Hugging Face (Mistral-7B) example&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=&quot;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency&quot;&gt;PaLM&lt;/a&gt;&lt;/li&gt;\n  &lt;/ul&gt;\n  &lt;li&gt;&lt;a href=&quot;#extensibility&quot;&gt;Extensibility&lt;/a&gt;&lt;/li&gt;\n  &lt;li&gt;&lt;a href=&quot;#contributing-to-aiconfig&quot;&gt;Contributing&lt;/a&gt;&lt;/li&gt;\n  &lt;li&gt;&lt;a href=&quot;#roadmap&quot;&gt;Roadmap&lt;/a&gt;&lt;/li&gt;\n  &lt;li&gt;&lt;a href=&quot;#faqs&quot;&gt;FAQ&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n## Features\n\n- [x] **Source-control friendly** [&#x60;aiconfig&#x60; format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to save prompts and model settings, which you can use for evaluation, reproducibility and simplifying your application code.\n- [x] **Multi-modal and model agnostic**. Use with any model, and serialize/deserialize data with the same &#x60;aiconfig&#x60; format.\n- [x] **Prompt chaining and parameterization** with [{{handlebars}}](https://handlebarsjs.com/) templating syntax, allowing you to pass dynamic data into prompts (as well as between prompts).\n- [x] **Streaming** supported out of the box, allowing you to get playground-like streaming wherever you use &#x60;aiconfig&#x60;.\n- [x] **Notebook editor**. [AI Workbooks editor](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to visually create your &#x60;aiconfig&#x60;, and use the SDK to connect it to your application code.\n\n## Install\n\nInstall with your favorite package manager for Node or Python.\n\n### Node.js\n\n#### &#x60;npm&#x60; or &#x60;yarn&#x60;\n\n&#x60;&#x60;&#x60;bash\nnpm install aiconfig\n&#x60;&#x60;&#x60;\n\n&#x60;&#x60;&#x60;bash\nyarn add aiconfig\n&#x60;&#x60;&#x60;\n\n### Python\n\n#### &#x60;pip3&#x60; or &#x60;poetry&#x60;\n\n&#x60;&#x60;&#x60;bash\npip3 install python-aiconfig\n&#x60;&#x60;&#x60;\n\n&#x60;&#x60;&#x60;bash\npoetry add python-aiconfig\n&#x60;&#x60;&#x60;\n\n[Detailed installation instructions](https://aiconfig.lastmileai.dev/docs/getting-started/#installation).\n\n### Set your OpenAI API Key\n\n&gt; **Note**: Make sure to specify the API keys (such as [&#x60;OPENAI_API_KEY&#x60;](https://platform.openai.com/api-keys)) in your environment before proceeding.\n\nIn your CLI, set the environment variable:\n\n&#x60;&#x60;&#x60;bash\nexport OPENAI_API_KEY=my_key\n&#x60;&#x60;&#x60;\n\n## Getting Started\n\n&gt; We cover Python instructions here, for Node.js please see the [detailed Getting Started guide](https://aiconfig.lastmileai.dev/docs/getting-started)\n\nIn this quickstart, you will create a customizable NYC travel itinerary using &#x60;aiconfig&#x60;.\n\nThis AIConfig contains a prompt chain to get a list of travel activities from an LLM and then generate an itinerary in an order specified by the user.\n\n&gt; **Link to tutorial code: [here](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Getting-Started)**\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/805173d1-0f83-44c5-b570-c776bb7dba66\n\n### Download &#x60;travel.aiconfig.json&#x60;\n\n&gt; **Note**: Don&#x27;t worry if you don&#x27;t understand all the pieces of this yet, we&#x27;ll go over it step by step.\n\n&#x60;&#x60;&#x60;json\n{\n  &quot;name&quot;: &quot;NYC Trip Planner&quot;,\n  &quot;description&quot;: &quot;Intrepid explorer with ChatGPT and AIConfig&quot;,\n  &quot;schema_version&quot;: &quot;latest&quot;,\n  &quot;metadata&quot;: {\n    &quot;models&quot;: {\n      &quot;gpt-3.5-turbo&quot;: {\n        &quot;model&quot;: &quot;gpt-3.5-turbo&quot;,\n        &quot;top_p&quot;: 1,\n        &quot;temperature&quot;: 1\n      },\n      &quot;gpt-4&quot;: {\n        &quot;model&quot;: &quot;gpt-4&quot;,\n        &quot;max_tokens&quot;: 3000,\n        &quot;system_prompt&quot;: &quot;You are an expert travel coordinator with exquisite taste.&quot;\n      }\n    },\n    &quot;default_model&quot;: &quot;gpt-3.5-turbo&quot;\n  },\n  &quot;prompts&quot;: [\n    {\n      &quot;name&quot;: &quot;get_activities&quot;,\n      &quot;input&quot;: &quot;Tell me 10 fun attractions to do in NYC.&quot;\n    },\n    {\n      &quot;name&quot;: &quot;gen_itinerary&quot;,\n      &quot;input&quot;: &quot;Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}.&quot;,\n      &quot;metadata&quot;: {\n        &quot;model&quot;: &quot;gpt-4&quot;,\n        &quot;parameters&quot;: {\n          &quot;order_by&quot;: &quot;geographic location&quot;\n        }\n      }\n    }\n  ]\n}\n&#x60;&#x60;&#x60;\n\n### Run the &#x60;get_activities&#x60; prompt.\n\nYou don&#x27;t need to worry about how to run inference for the model; it&#x27;s all handled by AIConfig. The prompt runs with gpt-3.5-turbo since that is the &#x60;default_model&#x60; for this AIConfig.\n\nCreate a new file called &#x60;app.py&#x60; and and enter the following code:\n\n&#x60;&#x60;&#x60;python\nimport asyncio\nfrom aiconfig import AIConfigRuntime, InferenceOptions\n\nasync def main():\n  # Load the aiconfig\n  config = AIConfigRuntime.load(&#x27;travel.aiconfig.json&#x27;)\n\n  # Run a single prompt (with streaming)\n  inference_options = InferenceOptions(stream=True)\n  await config.run(&quot;get_activities&quot;, options=inference_options)\n\nasyncio.run(main())\n&#x60;&#x60;&#x60;\n\nNow run this in your terminal with the command:\n\n&#x60;&#x60;&#x60;bash\npython3 app.py\n&#x60;&#x60;&#x60;\n\n### Run the &#x60;gen_itinerary&#x60; prompt.\n\nIn your &#x60;app.py&#x60; file, change the last line to below:\n\n&#x60;&#x60;&#x60;python\nawait config.run(&quot;gen_itinerary&quot;, params=None, options=inference_options)\n&#x60;&#x60;&#x60;\n\nRe-run the command in your terminal:\n\n&#x60;&#x60;&#x60;bash\npython3 app.py\n&#x60;&#x60;&#x60;\n\nThis prompt depends on the output of &#x60;get_activities&#x60;. It also takes in parameters (user input) to determine the customized itinerary.\n\nLet&#x27;s take a closer look:\n\n**&#x60;gen_itinerary&#x60; prompt:**\n\n&#x60;&#x60;&#x60;\n&quot;Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}.&quot;\n&#x60;&#x60;&#x60;\n\n**prompt metadata:**\n\n&#x60;&#x60;&#x60;json\n{\n  &quot;metadata&quot;: {\n    &quot;model&quot;: &quot;gpt-4&quot;,\n    &quot;parameters&quot;: {\n      &quot;order_by&quot;: &quot;geographic location&quot;\n    }\n  }\n}\n&#x60;&#x60;&#x60;\n\nObserve the following:\n\n1. The prompt depends on the output of the &#x60;get_activities&#x60; prompt.\n2. It also depends on an &#x60;order_by&#x60; parameter (using {{handlebars}} syntax)\n3. It uses **gpt-4**, whereas the &#x60;get_activities&#x60; prompt it depends on uses **gpt-3.5-turbo**.\n\n&gt; Effectively, this is a prompt chain between &#x60;gen_itinerary&#x60; and &#x60;get_activities&#x60; prompts, _as well as_ as a model chain between **gpt-3.5-turbo** and **gpt-4**.\n\nLet&#x27;s run this with AIConfig:\n\nReplace &#x60;config.run&#x60; above with this:\n\n&#x60;&#x60;&#x60;python\nawait config.run(&quot;gen_itinerary&quot;, params={&quot;order_by&quot;: &quot;duration&quot;}, options=inference_options, run_with_dependencies=True)\n&#x60;&#x60;&#x60;\n\nNotice how simple the syntax is to perform a fairly complex task - running 2 different prompts across 2 different models and chaining one&#x27;s output as part of the input of another.\n\nThe code will just run &#x60;get_activities&#x60;, then pipe its output as an input to &#x60;gen_itinerary&#x60;, and finally run &#x60;gen_itinerary&#x60;.\n\n### Save the AIConfig\n\nLet&#x27;s save the AIConfig back to disk, and serialize the outputs from the latest inference run as well:\n\n&#x60;&#x60;&#x60;python\n# Save the aiconfig to disk. and serialize outputs from the model run\nconfig.save(&#x27;updated.aiconfig.json&#x27;, include_outputs=True)\n&#x60;&#x60;&#x60;\n\n### Edit &#x60;aiconfig&#x60; in a notebook editor\n\nWe can iterate on an &#x60;aiconfig&#x60; using a notebook-like editor called an **AI Workbook**. Now that we have an &#x60;aiconfig&#x60; file artifact that encapsulates the generative AI part of our application, we can iterate on it separately from the application code that uses it.\n\n1. Go to https://lastmileai.dev.\n2. Go to Workbooks page: https://lastmileai.dev/workbooks\n3. Click dropdown from &#x27;+ New Workbook&#x27; and select &#x27;Create from AIConfig&#x27;\n4. Upload &#x60;travel.aiconfig.json&#x60;\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/5d901493-bbda-4f8e-93c7-dd9a91bf242e\n\nTry out the workbook playground here: **[NYC Travel Workbook](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9)**\n\n&gt; **We are working on a local editor that you can run yourself. For now, please use the hosted version on https://lastmileai.dev.**\n\n### Additional Guides\n\nThere is a lot you can do with &#x60;aiconfig&#x60;. We have several other tutorials to help get you started:\n\n- [Create an AIConfig from scratch](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig)\n- [Run a prompt](https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig)\n- [Pass data into prompts](https://aiconfig.lastmileai.dev/docs/overview/parameters)\n- [Prompt chains](https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain)\n- [Callbacks and monitoring](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\nHere are some example uses:\n\n- [CLI Chatbot](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT)\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [Chain of thought](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### OpenAI Introspection API\n\nIf you are already using OpenAI completion API&#x27;s in your application, you can get started very quickly to start saving the messages in an &#x60;aiconfig&#x60;.\n\nUsage: see openai_wrapper.ipynb.\n\nNow you can continue using &#x60;openai&#x60; completion API as normal. When you want to save the config, just call &#x60;new_config.save()&#x60; and all your openai completion calls will get serialized to disk.\n\n&gt; [**Detailed guide here**](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig#openai-api-python-wrapper)\n\n## Supported Models\n\nAIConfig supports the following models out of the box:\n\n- OpenAI chat models (GPT-3, GPT-3.5, GPT-4)\n- LLaMA2 (running locally)\n- Google PaLM models (PaLM chat)\n- Hugging Face text generation models (e.g. Mistral-7B)\n\n### Examples\n\n- [OpenAI](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n&gt; If you need to use a model that isn&#x27;t provided out of the box, you can implement a &#x60;ModelParser&#x60; for it (see [Extending AIConfig](#extending-aiconfig)). **We welcome [contributions](https://aiconfig.lastmileai.dev/docs/contributing)**\n\n## AIConfig Schema\n\n[AIConfig specification](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format)\n\n## AIConfig SDK\n\n&gt; Read the [Usage Guide](https://aiconfig.lastmileai.dev/docs/usage-guide) for more details.\n\nThe AIConfig SDK supports CRUD operations for prompts, models, parameters and metadata. Here are some common examples.\n\nThe root interface is the &#x60;AIConfigRuntime&#x60; object. That is the entrypoint for interacting with an AIConfig programmatically.\n\nLet&#x27;s go over a few key CRUD operations to give a glimpse.\n\n### AIConfig &#x60;create&#x60;\n\n&#x60;&#x60;&#x60;python\nconfig = AIConfigRuntime.create(&quot;aiconfig name&quot;, &quot;description&quot;)\n&#x60;&#x60;&#x60;\n\n### Prompt &#x60;resolve&#x60;\n\n&#x60;resolve&#x60; deserializes an existing &#x60;Prompt&#x60; into the data object that its model expects.\n\n&#x60;&#x60;&#x60;python\nconfig.resolve(&quot;prompt_name&quot;, params)\n&#x60;&#x60;&#x60;\n\n&#x60;params&#x60; are overrides you can specify to resolve any &#x60;{{handlebars}}&#x60; templates in the prompt. See the &#x60;gen_itinerary&#x60; prompt in the Getting Started example.\n\n### Prompt &#x60;serialize&#x60;\n\n&#x60;serialize&#x60; is the inverse of &#x60;resolve&#x60; -- it serializes the data object that a model understands into a &#x60;Prompt&#x60; object that can be serialized into the &#x60;aiconfig&#x60; format.\n\n&#x60;&#x60;&#x60;python\nconfig.serialize(&quot;model_name&quot;, data, &quot;prompt_name&quot;)\n&#x60;&#x60;&#x60;\n\n### Prompt &#x60;run&#x60;\n\n&#x60;run&#x60; is used to run inference for the specified &#x60;Prompt&#x60;.\n\n&#x60;&#x60;&#x60;python\nconfig.run(&quot;prompt_name&quot;, params)\n&#x60;&#x60;&#x60;\n\n### &#x60;run_with_dependencies&#x60;\n\nThis is a variant of &#x60;run&#x60; -- this re-runs all prompt dependencies.\nFor example, in [&#x60;travel.aiconfig.json&#x60;](#download-travelaiconfigjson), the &#x60;gen_itinerary&#x60; prompt references the output of the &#x60;get_activities&#x60; prompt using &#x60;{{get_activities.output}}&#x60;.\n\nRunning this function will first execute &#x60;get_activities&#x60;, and use its output to resolve the &#x60;gen_itinerary&#x60; prompt before executing it.\nThis is transitive, so it computes the Directed Acyclic Graph of dependencies to execute. Complex relationships can be modeled this way.\n\n&#x60;&#x60;&#x60;python\nconfig.run_with_dependencies(&quot;gen_itinerary&quot;)\n&#x60;&#x60;&#x60;\n\n### Updating metadata and parameters\n\nUse the &#x60;get/set_metadata&#x60; and &#x60;get/set_parameter&#x60; methods to interact with metadata and parameters (&#x60;set_parameter&#x60; is just syntactic sugar to update &#x60;&quot;metadata.parameters&quot;&#x60;)\n\n&#x60;&#x60;&#x60;python\nconfig.set_metadata(&quot;key&quot;, data, &quot;prompt_name&quot;)\n&#x60;&#x60;&#x60;\n\nNote: if &#x60;&quot;prompt_name&quot;&#x60; is specified, the metadata is updated specifically for that prompt. Otherwise, the global metadata is updated.\n\n### Register new &#x60;ModelParser&#x60;\n\nUse the &#x60;AIConfigRuntime.register_model_parser&#x60; if you want to use a different &#x60;ModelParser&#x60;, or configure AIConfig to work with an additional model.\n\nAIConfig uses the model name string to retrieve the right &#x60;ModelParser&#x60; for a given Prompt (see &#x60;AIConfigRuntime.get_model_parser&#x60;), so you can register a different ModelParser for the same ID to override which &#x60;ModelParser&#x60; handles a Prompt.\n\nFor example, suppose I want to use &#x60;MyOpenAIModelParser&#x60; to handle &#x60;gpt-4&#x60; prompts. I can do the following at the start of my application:\n\n&#x60;&#x60;&#x60;python\nAIConfigRuntime.register_model_parser(myModelParserInstance, [&quot;gpt-4&quot;])\n&#x60;&#x60;&#x60;\n\n### Callback events\n\nUse callback events to trace and monitor what&#x27;s going on -- helpful for debugging and observability.\n\n&#x60;&#x60;&#x60;python\nfrom aiconfig import AIConfigRuntime, CallbackEvent, CallbackManager\nconfig = AIConfigRuntime.load(&#x27;aiconfig.json&#x27;)\n\nasync def my_custom_callback(event: CallbackEvent) -&gt; None:\n  print(f&quot;Event triggered: {event.name}&quot;, event)\n\ncallback_manager = CallbackManager([my_custom_callback])\nconfig.set_callback_manager(callback_manager)\n\nawait config.run(&quot;prompt_name&quot;)\n&#x60;&#x60;&#x60;\n\n[**Read more** here](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\n## Extensibility\n\nAIConfig is designed to be customized and extended for your use-case. The [Extensibility](/docs/extensibility) guide goes into more detail.\n\nCurrently, there are 3 core ways to extend AIConfig:\n\n1. [Supporting other models](https://aiconfig.lastmileai.dev/docs/extensibility#1-bring-your-own-model) - define a ModelParser extension\n2. [Callback event handlers](https://aiconfig.lastmileai.dev/docs/extensibility#2-callback-handlers) - tracing and monitoring\n3. [Custom metadata](https://aiconfig.lastmileai.dev/docs/extensibility#3-custom-metadata) - save custom fields in &#x60;aiconfig&#x60;\n\n## Contributing to &#x60;aiconfig&#x60;\n\nThis is our first open-source project and we&#x27;d love your help.\n\nSee our [contributing guidelines](https://aiconfig.lastmileai.dev/docs/contributing) -- we would especially love help adding support for additional models that the community wants.\n\n## Cookbooks\n\nWe provide several guides to demonstrate the power of &#x60;aiconfig&#x60;.\n\n&gt; **See the [&#x60;cookbooks&#x60;](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks) folder for examples to clone.**\n\n### Chatbot\n\n- [Wizard GPT](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT) - speak to a wizard on your CLI\n\n- [CLI-mate](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Cli-Mate) - help you make code-mods interactively on your codebase.\n\n### Retrieval Augmented Generated (RAG)\n\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n\nAt its core, RAG is about passing data into prompts. Read how to [pass data](/docs/overview/parameters) with AIConfig.\n\n### Function calling\n\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n\n### Prompt routing\n\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n\n### Chain of Thought\n\nA variant of chain-of-thought is Chain of Verification, used to help reduce hallucinations. Check out the aiconfig cookbook for CoVe:\n\n- [Chain of Verification](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### Using local LLaMA2 with &#x60;aiconfig&#x60;\n\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n\n### Hugging Face text generation\n\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n\n### Google PaLM\n\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n## Roadmap\n\nThis project is under active development.\n\nIf you&#x27;d like to help, please see the [contributing guidelines](#contributing-to-aiconfig).\n\nPlease create issues for additional capabilities you&#x27;d like to see.\n\nHere&#x27;s what&#x27;s already on our roadmap:\n\n- Evaluation interfaces: allow &#x60;aiconfig&#x60; artifacts to be evaluated with user-defined eval functions.\n  - We are also considering integrating with existing evaluation frameworks.\n- Local editor for &#x60;aiconfig&#x60;: enable you to interact with aiconfigs more intuitively.\n- OpenAI Assistants API support\n- Multi-modal ModelParsers:\n  - GPT4-V support\n  - DALLE-3\n  - Whisper\n  - HuggingFace image generation\n\n## FAQs\n\n### How should I edit an &#x60;aiconfig&#x60; file?\n\nEditing a configshould be done either programmatically via SDK or via the UI (workbooks):\n\n- [Programmatic](https://github.com/lastmile-ai/aiconfig/blob/main/cookbooks/Create-AIConfig-Programmatically/create_aiconfig_programmatically.ipynb) editing.\n\n- [Edit with a workbook](#edit-aiconfig-in-a-notebook-editor) editor: this is similar to editing an ipynb file as a notebook (most people never touch the json ipynb directly)\n\nYou should only edit the &#x60;aiconfig&#x60; by hand for minor modifications, like tweaking a prompt string or updating some metadata.\n\n### Does this support custom endpoints?\n\nOut of the box, AIConfig already supports all OpenAI GPT\\* models, Googles PaLM model and any textgeneration model on Hugging Face (like Mistral). See [Supported Models](#supported-models) for more details.\n\nAdditionally, you can install &#x60;aiconfig&#x60; [extensions](https://github.com/lastmile-ai/aiconfig/tree/main/extensions) for additional models (see question below).\n\n### Is OpenAI function calling supported?\n\nYes. [This example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI) goes through how to do it.\n\nWe are also working on adding support for the Assistants API.\n\n### How can I use aiconfig with my own model endpoint?\n\nModel support is implemented as ModelParsers in the AIConfig SDK, and the idea is that anyone, including you, can define a ModelParser (and even publish it as an extension package).\n\nAll thats needed to use a model with AIConfig is a ModelParser that knows\n\n- how to serialize data from a model into the aiconfig format\n- how to deserialize data from an aiconfig into the type the model expects\n- how to run inference for model.\n\nFor more details, see [Extensibility](https://aiconfig.lastmileai.dev/docs/extensibility).\n\n### When should I store outputs in an &#x60;aiconfig&#x60;?\n\nThe &#x60;AIConfigRuntime&#x60; object is used to interact with an aiconfig programmatically (see [SDK usage guide](#aiconfig-sdk)). As you run prompts, this object keeps track of the outputs returned from the model.\n\nYou can choose to serialize these outputs back into the &#x60;aiconfig&#x60; by using the &#x60;config.save(include_outputs=True)&#x60; API. This can be useful for preserving context -- think of it like session state.\n\nFor example, you can use aiconfig to create a chatbot, and use the same format to save the chat history so it can be resumed for the next session.\n\nYou can also choose to save outputs to a _different_ file than the original config -- &#x60;config.save(&quot;history.aiconfig.json&quot;, include_outputs=True)&#x60;.\n\n### Why should I use &#x60;aiconfig&#x60; instead of things like [configurator](https://pypi.org/project/configurator/)?\n\nIt helps to have a [standardized format](http://aiconfig.lastmileai.dev/docs/overview/ai-config-format) specifically for storing generative AI prompts, inference results, model parameters and arbitrary metadata, as opposed to a general-purpose configuration schema.\n\nWith that standardization, you just need a layer that knows how to serialize/deserialize from that format into whatever the inference endpoints require.\n\n### This looks similar to &#x60;ipynb&#x60; for Jupyter notebooks\n\nWe believe that notebooks are a perfect iteration environment for generative AI -- they are flexible, multi-modal, and collaborative.\n\nThe multi-modality and flexibility offered by notebooks and [&#x60;ipynb&#x60;](https://ipython.org/ipython-doc/3/notebook/nbformat.html) offers a good interaction model for generative AI. The &#x60;aiconfig&#x60; file format is extensible like &#x60;ipynb&#x60;, and AI Workbook editor allows rapid iteration in a notebook-like IDE.\n\n_AI Workbooks are to AIConfig what Jupyter notebooks are to &#x60;ipynb&#x60;_\n\nThere are 2 areas where we are going beyond what notebooks offer:\n\n1. &#x60;aiconfig&#x60; is more **source-control friendly** than &#x60;ipynb&#x60;. &#x60;ipynb&#x60; stores binary data (images, etc.) by encoding it in the file, while &#x60;aiconfig&#x60; recommends using file URI references instead.\n2. &#x60;aiconfig&#x60; can be imported and **connected to application code** using the AIConfig SDK.\n', 'role': 'user'}]}} ts_ns=1702175485063260400
2023-12-09 21:31:40,225 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'temperature': 0.75, 'frequency_penalty': 0, 'presence_penalty': 0, 'model': 'gpt-4', 'top_p': 1, 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is aiconfig. The READMEfile is:\n&lt;div align=&quot;center&quot;&gt;&lt;picture&gt;\n  &lt;img alt=&quot;aiconfig&quot; src=&quot;aiconfig-docs/static/img/readme_logo.png&quot; /&gt;\n&lt;/picture&gt;&lt;/div&gt;\n&lt;br/&gt;\n\n![Python](https://github.com/lastmile-ai/aiconfig/actions/workflows/main_python.yml/badge.svg)\n![Node](https://github.com/lastmile-ai/aiconfig/actions/workflows/main-typescript.yml/badge.svg)\n![Docs](https://github.com/lastmile-ai/aiconfig/actions/workflows/test-deploy-docs.yml/badge.svg)\n[![Discord](&lt;https://img.shields.io/badge/Discord-LastMile%20AI-Blue?color=rgb(37%2C%20150%2C%20190)&gt;)](https://discord.gg/qMqgzDae)\n\n&gt; Full documentation: **[aiconfig.lastmileai.dev](https://aiconfig.lastmileai.dev/)**\n\n## Overview\n\nAIConfig saves prompts, models and model parameters as source control friendly configs. This allows you to iterate on prompts and model parameters _separately from your application code_.\n\n1. **Prompts as configs**: a [standardized JSON format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to store generative AI model settings, prompt inputs/outputs, and flexible metadata.\n2. **Model-agnostic SDK**: Python &amp; Node SDKs to use &#x60;aiconfig&#x60; in your application code. AIConfig is designed to be **model-agnostic** and **multi-modal**, so you can extend it to work with any generative AI model, including text, image and audio.\n3. **AI Workbook editor**: A [notebook-like playground](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to edit &#x60;aiconfig&#x60; files visually, run prompts, tweak models and model settings, and chain things together.\n\n### What problem it solves\n\nToday, application code is tightly coupled with the gen AI settings for the application -- prompts, parameters, and model-specific logic is all jumbled in with app code.\n\n- results in increased complexity\n- makes it hard to iterate on the prompts or try different models easily\n- makes it hard to evaluate prompt/model performance\n\nAIConfig helps unwind complexity by separating prompts, model parameters, and model-specific logic from your application.\n\n- simplifies application code -- simply call &#x60;config.run()&#x60;\n- open the &#x60;aiconfig&#x60; in a playground to iterate quickly\n- version control and evaluate the &#x60;aiconfig&#x60; - it&#x27;s the AI artifact for your application.\n\n![AIConfig flow](aiconfig-docs/static/img/aiconfig_dataflow.png)\n\n### Quicknav\n\n&lt;ul style=&quot;margin-bottom:0; padding-bottom:0;&quot;&gt;\n  &lt;li&gt;&lt;a href=&quot;#install&quot;&gt;Getting Started&lt;/a&gt;&lt;/li&gt;\n  &lt;ul style=&quot;margin-bottom:0; padding-bottom:0;&quot;&gt;\n    &lt;li&gt;&lt;a href=&quot;https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig&quot;&gt;Create an AIConfig&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=&quot;https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig&quot;&gt;Run a prompt&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=&quot;https://aiconfig.lastmileai.dev/docs/overview/parameters&quot;&gt;Pass data into prompts&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=&quot;https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain&quot;&gt;Prompt Chains&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=&quot;https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig&quot;&gt;Callbacks and monitoring&lt;/a&gt;&lt;/li&gt;\n  &lt;/ul&gt;\n  &lt;li&gt;&lt;a href=&quot;#aiconfig-sdk&quot;&gt;SDK Cheatsheet&lt;/a&gt;&lt;/li&gt;\n  &lt;li&gt;&lt;a href=&quot;#cookbooks&quot;&gt;Cookbooks and guides&lt;/a&gt;&lt;/li&gt;\n  &lt;ul style=&quot;margin-bottom:0; padding-bottom:0;&quot;&gt;\n    &lt;li&gt;&lt;a href=&quot;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT&quot;&gt;CLI Chatbot&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=&quot;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig&quot;&gt;RAG with AIConfig&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=&quot;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing&quot;&gt;Prompt routing&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=&quot;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI&quot;&gt;OpenAI function calling&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=&quot;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification&quot;&gt;Chain of Verification&lt;/a&gt;&lt;/li&gt;\n  &lt;/ul&gt;\n  &lt;li&gt;&lt;a href=&quot;#supported-models&quot;&gt;Supported models&lt;/a&gt;&lt;/li&gt;\n  &lt;ul style=&quot;margin-bottom:0; padding-bottom:0;&quot;&gt;\n    &lt;li&gt;&lt;a href=&quot;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama&quot;&gt;LLaMA2 example&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=&quot;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace&quot;&gt;Hugging Face (Mistral-7B) example&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=&quot;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency&quot;&gt;PaLM&lt;/a&gt;&lt;/li&gt;\n  &lt;/ul&gt;\n  &lt;li&gt;&lt;a href=&quot;#extensibility&quot;&gt;Extensibility&lt;/a&gt;&lt;/li&gt;\n  &lt;li&gt;&lt;a href=&quot;#contributing-to-aiconfig&quot;&gt;Contributing&lt;/a&gt;&lt;/li&gt;\n  &lt;li&gt;&lt;a href=&quot;#roadmap&quot;&gt;Roadmap&lt;/a&gt;&lt;/li&gt;\n  &lt;li&gt;&lt;a href=&quot;#faqs&quot;&gt;FAQ&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n## Features\n\n- [x] **Source-control friendly** [&#x60;aiconfig&#x60; format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to save prompts and model settings, which you can use for evaluation, reproducibility and simplifying your application code.\n- [x] **Multi-modal and model agnostic**. Use with any model, and serialize/deserialize data with the same &#x60;aiconfig&#x60; format.\n- [x] **Prompt chaining and parameterization** with [{{handlebars}}](https://handlebarsjs.com/) templating syntax, allowing you to pass dynamic data into prompts (as well as between prompts).\n- [x] **Streaming** supported out of the box, allowing you to get playground-like streaming wherever you use &#x60;aiconfig&#x60;.\n- [x] **Notebook editor**. [AI Workbooks editor](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to visually create your &#x60;aiconfig&#x60;, and use the SDK to connect it to your application code.\n\n## Install\n\nInstall with your favorite package manager for Node or Python.\n\n### Node.js\n\n#### &#x60;npm&#x60; or &#x60;yarn&#x60;\n\n&#x60;&#x60;&#x60;bash\nnpm install aiconfig\n&#x60;&#x60;&#x60;\n\n&#x60;&#x60;&#x60;bash\nyarn add aiconfig\n&#x60;&#x60;&#x60;\n\n### Python\n\n#### &#x60;pip3&#x60; or &#x60;poetry&#x60;\n\n&#x60;&#x60;&#x60;bash\npip3 install python-aiconfig\n&#x60;&#x60;&#x60;\n\n&#x60;&#x60;&#x60;bash\npoetry add python-aiconfig\n&#x60;&#x60;&#x60;\n\n[Detailed installation instructions](https://aiconfig.lastmileai.dev/docs/getting-started/#installation).\n\n### Set your OpenAI API Key\n\n&gt; **Note**: Make sure to specify the API keys (such as [&#x60;OPENAI_API_KEY&#x60;](https://platform.openai.com/api-keys)) in your environment before proceeding.\n\nIn your CLI, set the environment variable:\n\n&#x60;&#x60;&#x60;bash\nexport OPENAI_API_KEY=my_key\n&#x60;&#x60;&#x60;\n\n## Getting Started\n\n&gt; We cover Python instructions here, for Node.js please see the [detailed Getting Started guide](https://aiconfig.lastmileai.dev/docs/getting-started)\n\nIn this quickstart, you will create a customizable NYC travel itinerary using &#x60;aiconfig&#x60;.\n\nThis AIConfig contains a prompt chain to get a list of travel activities from an LLM and then generate an itinerary in an order specified by the user.\n\n&gt; **Link to tutorial code: [here](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Getting-Started)**\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/805173d1-0f83-44c5-b570-c776bb7dba66\n\n### Download &#x60;travel.aiconfig.json&#x60;\n\n&gt; **Note**: Don&#x27;t worry if you don&#x27;t understand all the pieces of this yet, we&#x27;ll go over it step by step.\n\n&#x60;&#x60;&#x60;json\n{\n  &quot;name&quot;: &quot;NYC Trip Planner&quot;,\n  &quot;description&quot;: &quot;Intrepid explorer with ChatGPT and AIConfig&quot;,\n  &quot;schema_version&quot;: &quot;latest&quot;,\n  &quot;metadata&quot;: {\n    &quot;models&quot;: {\n      &quot;gpt-3.5-turbo&quot;: {\n        &quot;model&quot;: &quot;gpt-3.5-turbo&quot;,\n        &quot;top_p&quot;: 1,\n        &quot;temperature&quot;: 1\n      },\n      &quot;gpt-4&quot;: {\n        &quot;model&quot;: &quot;gpt-4&quot;,\n        &quot;max_tokens&quot;: 3000,\n        &quot;system_prompt&quot;: &quot;You are an expert travel coordinator with exquisite taste.&quot;\n      }\n    },\n    &quot;default_model&quot;: &quot;gpt-3.5-turbo&quot;\n  },\n  &quot;prompts&quot;: [\n    {\n      &quot;name&quot;: &quot;get_activities&quot;,\n      &quot;input&quot;: &quot;Tell me 10 fun attractions to do in NYC.&quot;\n    },\n    {\n      &quot;name&quot;: &quot;gen_itinerary&quot;,\n      &quot;input&quot;: &quot;Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}.&quot;,\n      &quot;metadata&quot;: {\n        &quot;model&quot;: &quot;gpt-4&quot;,\n        &quot;parameters&quot;: {\n          &quot;order_by&quot;: &quot;geographic location&quot;\n        }\n      }\n    }\n  ]\n}\n&#x60;&#x60;&#x60;\n\n### Run the &#x60;get_activities&#x60; prompt.\n\nYou don&#x27;t need to worry about how to run inference for the model; it&#x27;s all handled by AIConfig. The prompt runs with gpt-3.5-turbo since that is the &#x60;default_model&#x60; for this AIConfig.\n\nCreate a new file called &#x60;app.py&#x60; and and enter the following code:\n\n&#x60;&#x60;&#x60;python\nimport asyncio\nfrom aiconfig import AIConfigRuntime, InferenceOptions\n\nasync def main():\n  # Load the aiconfig\n  config = AIConfigRuntime.load(&#x27;travel.aiconfig.json&#x27;)\n\n  # Run a single prompt (with streaming)\n  inference_options = InferenceOptions(stream=True)\n  await config.run(&quot;get_activities&quot;, options=inference_options)\n\nasyncio.run(main())\n&#x60;&#x60;&#x60;\n\nNow run this in your terminal with the command:\n\n&#x60;&#x60;&#x60;bash\npython3 app.py\n&#x60;&#x60;&#x60;\n\n### Run the &#x60;gen_itinerary&#x60; prompt.\n\nIn your &#x60;app.py&#x60; file, change the last line to below:\n\n&#x60;&#x60;&#x60;python\nawait config.run(&quot;gen_itinerary&quot;, params=None, options=inference_options)\n&#x60;&#x60;&#x60;\n\nRe-run the command in your terminal:\n\n&#x60;&#x60;&#x60;bash\npython3 app.py\n&#x60;&#x60;&#x60;\n\nThis prompt depends on the output of &#x60;get_activities&#x60;. It also takes in parameters (user input) to determine the customized itinerary.\n\nLet&#x27;s take a closer look:\n\n**&#x60;gen_itinerary&#x60; prompt:**\n\n&#x60;&#x60;&#x60;\n&quot;Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}.&quot;\n&#x60;&#x60;&#x60;\n\n**prompt metadata:**\n\n&#x60;&#x60;&#x60;json\n{\n  &quot;metadata&quot;: {\n    &quot;model&quot;: &quot;gpt-4&quot;,\n    &quot;parameters&quot;: {\n      &quot;order_by&quot;: &quot;geographic location&quot;\n    }\n  }\n}\n&#x60;&#x60;&#x60;\n\nObserve the following:\n\n1. The prompt depends on the output of the &#x60;get_activities&#x60; prompt.\n2. It also depends on an &#x60;order_by&#x60; parameter (using {{handlebars}} syntax)\n3. It uses **gpt-4**, whereas the &#x60;get_activities&#x60; prompt it depends on uses **gpt-3.5-turbo**.\n\n&gt; Effectively, this is a prompt chain between &#x60;gen_itinerary&#x60; and &#x60;get_activities&#x60; prompts, _as well as_ as a model chain between **gpt-3.5-turbo** and **gpt-4**.\n\nLet&#x27;s run this with AIConfig:\n\nReplace &#x60;config.run&#x60; above with this:\n\n&#x60;&#x60;&#x60;python\nawait config.run(&quot;gen_itinerary&quot;, params={&quot;order_by&quot;: &quot;duration&quot;}, options=inference_options, run_with_dependencies=True)\n&#x60;&#x60;&#x60;\n\nNotice how simple the syntax is to perform a fairly complex task - running 2 different prompts across 2 different models and chaining one&#x27;s output as part of the input of another.\n\nThe code will just run &#x60;get_activities&#x60;, then pipe its output as an input to &#x60;gen_itinerary&#x60;, and finally run &#x60;gen_itinerary&#x60;.\n\n### Save the AIConfig\n\nLet&#x27;s save the AIConfig back to disk, and serialize the outputs from the latest inference run as well:\n\n&#x60;&#x60;&#x60;python\n# Save the aiconfig to disk. and serialize outputs from the model run\nconfig.save(&#x27;updated.aiconfig.json&#x27;, include_outputs=True)\n&#x60;&#x60;&#x60;\n\n### Edit &#x60;aiconfig&#x60; in a notebook editor\n\nWe can iterate on an &#x60;aiconfig&#x60; using a notebook-like editor called an **AI Workbook**. Now that we have an &#x60;aiconfig&#x60; file artifact that encapsulates the generative AI part of our application, we can iterate on it separately from the application code that uses it.\n\n1. Go to https://lastmileai.dev.\n2. Go to Workbooks page: https://lastmileai.dev/workbooks\n3. Click dropdown from &#x27;+ New Workbook&#x27; and select &#x27;Create from AIConfig&#x27;\n4. Upload &#x60;travel.aiconfig.json&#x60;\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/5d901493-bbda-4f8e-93c7-dd9a91bf242e\n\nTry out the workbook playground here: **[NYC Travel Workbook](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9)**\n\n&gt; **We are working on a local editor that you can run yourself. For now, please use the hosted version on https://lastmileai.dev.**\n\n### Additional Guides\n\nThere is a lot you can do with &#x60;aiconfig&#x60;. We have several other tutorials to help get you started:\n\n- [Create an AIConfig from scratch](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig)\n- [Run a prompt](https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig)\n- [Pass data into prompts](https://aiconfig.lastmileai.dev/docs/overview/parameters)\n- [Prompt chains](https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain)\n- [Callbacks and monitoring](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\nHere are some example uses:\n\n- [CLI Chatbot](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT)\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [Chain of thought](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### OpenAI Introspection API\n\nIf you are already using OpenAI completion API&#x27;s in your application, you can get started very quickly to start saving the messages in an &#x60;aiconfig&#x60;.\n\nUsage: see openai_wrapper.ipynb.\n\nNow you can continue using &#x60;openai&#x60; completion API as normal. When you want to save the config, just call &#x60;new_config.save()&#x60; and all your openai completion calls will get serialized to disk.\n\n&gt; [**Detailed guide here**](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig#openai-api-python-wrapper)\n\n## Supported Models\n\nAIConfig supports the following models out of the box:\n\n- OpenAI chat models (GPT-3, GPT-3.5, GPT-4)\n- LLaMA2 (running locally)\n- Google PaLM models (PaLM chat)\n- Hugging Face text generation models (e.g. Mistral-7B)\n\n### Examples\n\n- [OpenAI](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n&gt; If you need to use a model that isn&#x27;t provided out of the box, you can implement a &#x60;ModelParser&#x60; for it (see [Extending AIConfig](#extending-aiconfig)). **We welcome [contributions](https://aiconfig.lastmileai.dev/docs/contributing)**\n\n## AIConfig Schema\n\n[AIConfig specification](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format)\n\n## AIConfig SDK\n\n&gt; Read the [Usage Guide](https://aiconfig.lastmileai.dev/docs/usage-guide) for more details.\n\nThe AIConfig SDK supports CRUD operations for prompts, models, parameters and metadata. Here are some common examples.\n\nThe root interface is the &#x60;AIConfigRuntime&#x60; object. That is the entrypoint for interacting with an AIConfig programmatically.\n\nLet&#x27;s go over a few key CRUD operations to give a glimpse.\n\n### AIConfig &#x60;create&#x60;\n\n&#x60;&#x60;&#x60;python\nconfig = AIConfigRuntime.create(&quot;aiconfig name&quot;, &quot;description&quot;)\n&#x60;&#x60;&#x60;\n\n### Prompt &#x60;resolve&#x60;\n\n&#x60;resolve&#x60; deserializes an existing &#x60;Prompt&#x60; into the data object that its model expects.\n\n&#x60;&#x60;&#x60;python\nconfig.resolve(&quot;prompt_name&quot;, params)\n&#x60;&#x60;&#x60;\n\n&#x60;params&#x60; are overrides you can specify to resolve any &#x60;{{handlebars}}&#x60; templates in the prompt. See the &#x60;gen_itinerary&#x60; prompt in the Getting Started example.\n\n### Prompt &#x60;serialize&#x60;\n\n&#x60;serialize&#x60; is the inverse of &#x60;resolve&#x60; -- it serializes the data object that a model understands into a &#x60;Prompt&#x60; object that can be serialized into the &#x60;aiconfig&#x60; format.\n\n&#x60;&#x60;&#x60;python\nconfig.serialize(&quot;model_name&quot;, data, &quot;prompt_name&quot;)\n&#x60;&#x60;&#x60;\n\n### Prompt &#x60;run&#x60;\n\n&#x60;run&#x60; is used to run inference for the specified &#x60;Prompt&#x60;.\n\n&#x60;&#x60;&#x60;python\nconfig.run(&quot;prompt_name&quot;, params)\n&#x60;&#x60;&#x60;\n\n### &#x60;run_with_dependencies&#x60;\n\nThis is a variant of &#x60;run&#x60; -- this re-runs all prompt dependencies.\nFor example, in [&#x60;travel.aiconfig.json&#x60;](#download-travelaiconfigjson), the &#x60;gen_itinerary&#x60; prompt references the output of the &#x60;get_activities&#x60; prompt using &#x60;{{get_activities.output}}&#x60;.\n\nRunning this function will first execute &#x60;get_activities&#x60;, and use its output to resolve the &#x60;gen_itinerary&#x60; prompt before executing it.\nThis is transitive, so it computes the Directed Acyclic Graph of dependencies to execute. Complex relationships can be modeled this way.\n\n&#x60;&#x60;&#x60;python\nconfig.run_with_dependencies(&quot;gen_itinerary&quot;)\n&#x60;&#x60;&#x60;\n\n### Updating metadata and parameters\n\nUse the &#x60;get/set_metadata&#x60; and &#x60;get/set_parameter&#x60; methods to interact with metadata and parameters (&#x60;set_parameter&#x60; is just syntactic sugar to update &#x60;&quot;metadata.parameters&quot;&#x60;)\n\n&#x60;&#x60;&#x60;python\nconfig.set_metadata(&quot;key&quot;, data, &quot;prompt_name&quot;)\n&#x60;&#x60;&#x60;\n\nNote: if &#x60;&quot;prompt_name&quot;&#x60; is specified, the metadata is updated specifically for that prompt. Otherwise, the global metadata is updated.\n\n### Register new &#x60;ModelParser&#x60;\n\nUse the &#x60;AIConfigRuntime.register_model_parser&#x60; if you want to use a different &#x60;ModelParser&#x60;, or configure AIConfig to work with an additional model.\n\nAIConfig uses the model name string to retrieve the right &#x60;ModelParser&#x60; for a given Prompt (see &#x60;AIConfigRuntime.get_model_parser&#x60;), so you can register a different ModelParser for the same ID to override which &#x60;ModelParser&#x60; handles a Prompt.\n\nFor example, suppose I want to use &#x60;MyOpenAIModelParser&#x60; to handle &#x60;gpt-4&#x60; prompts. I can do the following at the start of my application:\n\n&#x60;&#x60;&#x60;python\nAIConfigRuntime.register_model_parser(myModelParserInstance, [&quot;gpt-4&quot;])\n&#x60;&#x60;&#x60;\n\n### Callback events\n\nUse callback events to trace and monitor what&#x27;s going on -- helpful for debugging and observability.\n\n&#x60;&#x60;&#x60;python\nfrom aiconfig import AIConfigRuntime, CallbackEvent, CallbackManager\nconfig = AIConfigRuntime.load(&#x27;aiconfig.json&#x27;)\n\nasync def my_custom_callback(event: CallbackEvent) -&gt; None:\n  print(f&quot;Event triggered: {event.name}&quot;, event)\n\ncallback_manager = CallbackManager([my_custom_callback])\nconfig.set_callback_manager(callback_manager)\n\nawait config.run(&quot;prompt_name&quot;)\n&#x60;&#x60;&#x60;\n\n[**Read more** here](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\n## Extensibility\n\nAIConfig is designed to be customized and extended for your use-case. The [Extensibility](/docs/extensibility) guide goes into more detail.\n\nCurrently, there are 3 core ways to extend AIConfig:\n\n1. [Supporting other models](https://aiconfig.lastmileai.dev/docs/extensibility#1-bring-your-own-model) - define a ModelParser extension\n2. [Callback event handlers](https://aiconfig.lastmileai.dev/docs/extensibility#2-callback-handlers) - tracing and monitoring\n3. [Custom metadata](https://aiconfig.lastmileai.dev/docs/extensibility#3-custom-metadata) - save custom fields in &#x60;aiconfig&#x60;\n\n## Contributing to &#x60;aiconfig&#x60;\n\nThis is our first open-source project and we&#x27;d love your help.\n\nSee our [contributing guidelines](https://aiconfig.lastmileai.dev/docs/contributing) -- we would especially love help adding support for additional models that the community wants.\n\n## Cookbooks\n\nWe provide several guides to demonstrate the power of &#x60;aiconfig&#x60;.\n\n&gt; **See the [&#x60;cookbooks&#x60;](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks) folder for examples to clone.**\n\n### Chatbot\n\n- [Wizard GPT](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT) - speak to a wizard on your CLI\n\n- [CLI-mate](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Cli-Mate) - help you make code-mods interactively on your codebase.\n\n### Retrieval Augmented Generated (RAG)\n\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n\nAt its core, RAG is about passing data into prompts. Read how to [pass data](/docs/overview/parameters) with AIConfig.\n\n### Function calling\n\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n\n### Prompt routing\n\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n\n### Chain of Thought\n\nA variant of chain-of-thought is Chain of Verification, used to help reduce hallucinations. Check out the aiconfig cookbook for CoVe:\n\n- [Chain of Verification](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### Using local LLaMA2 with &#x60;aiconfig&#x60;\n\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n\n### Hugging Face text generation\n\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n\n### Google PaLM\n\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n## Roadmap\n\nThis project is under active development.\n\nIf you&#x27;d like to help, please see the [contributing guidelines](#contributing-to-aiconfig).\n\nPlease create issues for additional capabilities you&#x27;d like to see.\n\nHere&#x27;s what&#x27;s already on our roadmap:\n\n- Evaluation interfaces: allow &#x60;aiconfig&#x60; artifacts to be evaluated with user-defined eval functions.\n  - We are also considering integrating with existing evaluation frameworks.\n- Local editor for &#x60;aiconfig&#x60;: enable you to interact with aiconfigs more intuitively.\n- OpenAI Assistants API support\n- Multi-modal ModelParsers:\n  - GPT4-V support\n  - DALLE-3\n  - Whisper\n  - HuggingFace image generation\n\n## FAQs\n\n### How should I edit an &#x60;aiconfig&#x60; file?\n\nEditing a configshould be done either programmatically via SDK or via the UI (workbooks):\n\n- [Programmatic](https://github.com/lastmile-ai/aiconfig/blob/main/cookbooks/Create-AIConfig-Programmatically/create_aiconfig_programmatically.ipynb) editing.\n\n- [Edit with a workbook](#edit-aiconfig-in-a-notebook-editor) editor: this is similar to editing an ipynb file as a notebook (most people never touch the json ipynb directly)\n\nYou should only edit the &#x60;aiconfig&#x60; by hand for minor modifications, like tweaking a prompt string or updating some metadata.\n\n### Does this support custom endpoints?\n\nOut of the box, AIConfig already supports all OpenAI GPT\\* models, Googles PaLM model and any textgeneration model on Hugging Face (like Mistral). See [Supported Models](#supported-models) for more details.\n\nAdditionally, you can install &#x60;aiconfig&#x60; [extensions](https://github.com/lastmile-ai/aiconfig/tree/main/extensions) for additional models (see question below).\n\n### Is OpenAI function calling supported?\n\nYes. [This example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI) goes through how to do it.\n\nWe are also working on adding support for the Assistants API.\n\n### How can I use aiconfig with my own model endpoint?\n\nModel support is implemented as ModelParsers in the AIConfig SDK, and the idea is that anyone, including you, can define a ModelParser (and even publish it as an extension package).\n\nAll thats needed to use a model with AIConfig is a ModelParser that knows\n\n- how to serialize data from a model into the aiconfig format\n- how to deserialize data from an aiconfig into the type the model expects\n- how to run inference for model.\n\nFor more details, see [Extensibility](https://aiconfig.lastmileai.dev/docs/extensibility).\n\n### When should I store outputs in an &#x60;aiconfig&#x60;?\n\nThe &#x60;AIConfigRuntime&#x60; object is used to interact with an aiconfig programmatically (see [SDK usage guide](#aiconfig-sdk)). As you run prompts, this object keeps track of the outputs returned from the model.\n\nYou can choose to serialize these outputs back into the &#x60;aiconfig&#x60; by using the &#x60;config.save(include_outputs=True)&#x60; API. This can be useful for preserving context -- think of it like session state.\n\nFor example, you can use aiconfig to create a chatbot, and use the same format to save the chat history so it can be resumed for the next session.\n\nYou can also choose to save outputs to a _different_ file than the original config -- &#x60;config.save(&quot;history.aiconfig.json&quot;, include_outputs=True)&#x60;.\n\n### Why should I use &#x60;aiconfig&#x60; instead of things like [configurator](https://pypi.org/project/configurator/)?\n\nIt helps to have a [standardized format](http://aiconfig.lastmileai.dev/docs/overview/ai-config-format) specifically for storing generative AI prompts, inference results, model parameters and arbitrary metadata, as opposed to a general-purpose configuration schema.\n\nWith that standardization, you just need a layer that knows how to serialize/deserialize from that format into whatever the inference endpoints require.\n\n### This looks similar to &#x60;ipynb&#x60; for Jupyter notebooks\n\nWe believe that notebooks are a perfect iteration environment for generative AI -- they are flexible, multi-modal, and collaborative.\n\nThe multi-modality and flexibility offered by notebooks and [&#x60;ipynb&#x60;](https://ipython.org/ipython-doc/3/notebook/nbformat.html) offers a good interaction model for generative AI. The &#x60;aiconfig&#x60; file format is extensible like &#x60;ipynb&#x60;, and AI Workbook editor allows rapid iteration in a notebook-like IDE.\n\n_AI Workbooks are to AIConfig what Jupyter notebooks are to &#x60;ipynb&#x60;_\n\nThere are 2 areas where we are going beyond what notebooks offer:\n\n1. &#x60;aiconfig&#x60; is more **source-control friendly** than &#x60;ipynb&#x60;. &#x60;ipynb&#x60; stores binary data (images, etc.) by encoding it in the file, while &#x60;aiconfig&#x60; recommends using file URI references instead.\n2. &#x60;aiconfig&#x60; can be imported and **connected to application code** using the AIConfig SDK.\n', 'role': 'user'}]}} ts_ns=1702175485063260400
2023-12-09 21:31:40,225 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'temperature': 0.75, 'frequency_penalty': 0, 'presence_penalty': 0, 'model': 'gpt-4', 'top_p': 1, 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is aiconfig. The READMEfile is:\n&lt;div align=&quot;center&quot;&gt;&lt;picture&gt;\n  &lt;img alt=&quot;aiconfig&quot; src=&quot;aiconfig-docs/static/img/readme_logo.png&quot; /&gt;\n&lt;/picture&gt;&lt;/div&gt;\n&lt;br/&gt;\n\n![Python](https://github.com/lastmile-ai/aiconfig/actions/workflows/main_python.yml/badge.svg)\n![Node](https://github.com/lastmile-ai/aiconfig/actions/workflows/main-typescript.yml/badge.svg)\n![Docs](https://github.com/lastmile-ai/aiconfig/actions/workflows/test-deploy-docs.yml/badge.svg)\n[![Discord](&lt;https://img.shields.io/badge/Discord-LastMile%20AI-Blue?color=rgb(37%2C%20150%2C%20190)&gt;)](https://discord.gg/qMqgzDae)\n\n&gt; Full documentation: **[aiconfig.lastmileai.dev](https://aiconfig.lastmileai.dev/)**\n\n## Overview\n\nAIConfig saves prompts, models and model parameters as source control friendly configs. This allows you to iterate on prompts and model parameters _separately from your application code_.\n\n1. **Prompts as configs**: a [standardized JSON format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to store generative AI model settings, prompt inputs/outputs, and flexible metadata.\n2. **Model-agnostic SDK**: Python &amp; Node SDKs to use &#x60;aiconfig&#x60; in your application code. AIConfig is designed to be **model-agnostic** and **multi-modal**, so you can extend it to work with any generative AI model, including text, image and audio.\n3. **AI Workbook editor**: A [notebook-like playground](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to edit &#x60;aiconfig&#x60; files visually, run prompts, tweak models and model settings, and chain things together.\n\n### What problem it solves\n\nToday, application code is tightly coupled with the gen AI settings for the application -- prompts, parameters, and model-specific logic is all jumbled in with app code.\n\n- results in increased complexity\n- makes it hard to iterate on the prompts or try different models easily\n- makes it hard to evaluate prompt/model performance\n\nAIConfig helps unwind complexity by separating prompts, model parameters, and model-specific logic from your application.\n\n- simplifies application code -- simply call &#x60;config.run()&#x60;\n- open the &#x60;aiconfig&#x60; in a playground to iterate quickly\n- version control and evaluate the &#x60;aiconfig&#x60; - it&#x27;s the AI artifact for your application.\n\n![AIConfig flow](aiconfig-docs/static/img/aiconfig_dataflow.png)\n\n### Quicknav\n\n&lt;ul style=&quot;margin-bottom:0; padding-bottom:0;&quot;&gt;\n  &lt;li&gt;&lt;a href=&quot;#install&quot;&gt;Getting Started&lt;/a&gt;&lt;/li&gt;\n  &lt;ul style=&quot;margin-bottom:0; padding-bottom:0;&quot;&gt;\n    &lt;li&gt;&lt;a href=&quot;https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig&quot;&gt;Create an AIConfig&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=&quot;https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig&quot;&gt;Run a prompt&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=&quot;https://aiconfig.lastmileai.dev/docs/overview/parameters&quot;&gt;Pass data into prompts&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=&quot;https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain&quot;&gt;Prompt Chains&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=&quot;https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig&quot;&gt;Callbacks and monitoring&lt;/a&gt;&lt;/li&gt;\n  &lt;/ul&gt;\n  &lt;li&gt;&lt;a href=&quot;#aiconfig-sdk&quot;&gt;SDK Cheatsheet&lt;/a&gt;&lt;/li&gt;\n  &lt;li&gt;&lt;a href=&quot;#cookbooks&quot;&gt;Cookbooks and guides&lt;/a&gt;&lt;/li&gt;\n  &lt;ul style=&quot;margin-bottom:0; padding-bottom:0;&quot;&gt;\n    &lt;li&gt;&lt;a href=&quot;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT&quot;&gt;CLI Chatbot&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=&quot;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig&quot;&gt;RAG with AIConfig&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=&quot;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing&quot;&gt;Prompt routing&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=&quot;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI&quot;&gt;OpenAI function calling&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=&quot;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification&quot;&gt;Chain of Verification&lt;/a&gt;&lt;/li&gt;\n  &lt;/ul&gt;\n  &lt;li&gt;&lt;a href=&quot;#supported-models&quot;&gt;Supported models&lt;/a&gt;&lt;/li&gt;\n  &lt;ul style=&quot;margin-bottom:0; padding-bottom:0;&quot;&gt;\n    &lt;li&gt;&lt;a href=&quot;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama&quot;&gt;LLaMA2 example&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=&quot;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace&quot;&gt;Hugging Face (Mistral-7B) example&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=&quot;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency&quot;&gt;PaLM&lt;/a&gt;&lt;/li&gt;\n  &lt;/ul&gt;\n  &lt;li&gt;&lt;a href=&quot;#extensibility&quot;&gt;Extensibility&lt;/a&gt;&lt;/li&gt;\n  &lt;li&gt;&lt;a href=&quot;#contributing-to-aiconfig&quot;&gt;Contributing&lt;/a&gt;&lt;/li&gt;\n  &lt;li&gt;&lt;a href=&quot;#roadmap&quot;&gt;Roadmap&lt;/a&gt;&lt;/li&gt;\n  &lt;li&gt;&lt;a href=&quot;#faqs&quot;&gt;FAQ&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n## Features\n\n- [x] **Source-control friendly** [&#x60;aiconfig&#x60; format](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format) to save prompts and model settings, which you can use for evaluation, reproducibility and simplifying your application code.\n- [x] **Multi-modal and model agnostic**. Use with any model, and serialize/deserialize data with the same &#x60;aiconfig&#x60; format.\n- [x] **Prompt chaining and parameterization** with [{{handlebars}}](https://handlebarsjs.com/) templating syntax, allowing you to pass dynamic data into prompts (as well as between prompts).\n- [x] **Streaming** supported out of the box, allowing you to get playground-like streaming wherever you use &#x60;aiconfig&#x60;.\n- [x] **Notebook editor**. [AI Workbooks editor](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9) to visually create your &#x60;aiconfig&#x60;, and use the SDK to connect it to your application code.\n\n## Install\n\nInstall with your favorite package manager for Node or Python.\n\n### Node.js\n\n#### &#x60;npm&#x60; or &#x60;yarn&#x60;\n\n&#x60;&#x60;&#x60;bash\nnpm install aiconfig\n&#x60;&#x60;&#x60;\n\n&#x60;&#x60;&#x60;bash\nyarn add aiconfig\n&#x60;&#x60;&#x60;\n\n### Python\n\n#### &#x60;pip3&#x60; or &#x60;poetry&#x60;\n\n&#x60;&#x60;&#x60;bash\npip3 install python-aiconfig\n&#x60;&#x60;&#x60;\n\n&#x60;&#x60;&#x60;bash\npoetry add python-aiconfig\n&#x60;&#x60;&#x60;\n\n[Detailed installation instructions](https://aiconfig.lastmileai.dev/docs/getting-started/#installation).\n\n### Set your OpenAI API Key\n\n&gt; **Note**: Make sure to specify the API keys (such as [&#x60;OPENAI_API_KEY&#x60;](https://platform.openai.com/api-keys)) in your environment before proceeding.\n\nIn your CLI, set the environment variable:\n\n&#x60;&#x60;&#x60;bash\nexport OPENAI_API_KEY=my_key\n&#x60;&#x60;&#x60;\n\n## Getting Started\n\n&gt; We cover Python instructions here, for Node.js please see the [detailed Getting Started guide](https://aiconfig.lastmileai.dev/docs/getting-started)\n\nIn this quickstart, you will create a customizable NYC travel itinerary using &#x60;aiconfig&#x60;.\n\nThis AIConfig contains a prompt chain to get a list of travel activities from an LLM and then generate an itinerary in an order specified by the user.\n\n&gt; **Link to tutorial code: [here](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Getting-Started)**\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/805173d1-0f83-44c5-b570-c776bb7dba66\n\n### Download &#x60;travel.aiconfig.json&#x60;\n\n&gt; **Note**: Don&#x27;t worry if you don&#x27;t understand all the pieces of this yet, we&#x27;ll go over it step by step.\n\n&#x60;&#x60;&#x60;json\n{\n  &quot;name&quot;: &quot;NYC Trip Planner&quot;,\n  &quot;description&quot;: &quot;Intrepid explorer with ChatGPT and AIConfig&quot;,\n  &quot;schema_version&quot;: &quot;latest&quot;,\n  &quot;metadata&quot;: {\n    &quot;models&quot;: {\n      &quot;gpt-3.5-turbo&quot;: {\n        &quot;model&quot;: &quot;gpt-3.5-turbo&quot;,\n        &quot;top_p&quot;: 1,\n        &quot;temperature&quot;: 1\n      },\n      &quot;gpt-4&quot;: {\n        &quot;model&quot;: &quot;gpt-4&quot;,\n        &quot;max_tokens&quot;: 3000,\n        &quot;system_prompt&quot;: &quot;You are an expert travel coordinator with exquisite taste.&quot;\n      }\n    },\n    &quot;default_model&quot;: &quot;gpt-3.5-turbo&quot;\n  },\n  &quot;prompts&quot;: [\n    {\n      &quot;name&quot;: &quot;get_activities&quot;,\n      &quot;input&quot;: &quot;Tell me 10 fun attractions to do in NYC.&quot;\n    },\n    {\n      &quot;name&quot;: &quot;gen_itinerary&quot;,\n      &quot;input&quot;: &quot;Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}.&quot;,\n      &quot;metadata&quot;: {\n        &quot;model&quot;: &quot;gpt-4&quot;,\n        &quot;parameters&quot;: {\n          &quot;order_by&quot;: &quot;geographic location&quot;\n        }\n      }\n    }\n  ]\n}\n&#x60;&#x60;&#x60;\n\n### Run the &#x60;get_activities&#x60; prompt.\n\nYou don&#x27;t need to worry about how to run inference for the model; it&#x27;s all handled by AIConfig. The prompt runs with gpt-3.5-turbo since that is the &#x60;default_model&#x60; for this AIConfig.\n\nCreate a new file called &#x60;app.py&#x60; and and enter the following code:\n\n&#x60;&#x60;&#x60;python\nimport asyncio\nfrom aiconfig import AIConfigRuntime, InferenceOptions\n\nasync def main():\n  # Load the aiconfig\n  config = AIConfigRuntime.load(&#x27;travel.aiconfig.json&#x27;)\n\n  # Run a single prompt (with streaming)\n  inference_options = InferenceOptions(stream=True)\n  await config.run(&quot;get_activities&quot;, options=inference_options)\n\nasyncio.run(main())\n&#x60;&#x60;&#x60;\n\nNow run this in your terminal with the command:\n\n&#x60;&#x60;&#x60;bash\npython3 app.py\n&#x60;&#x60;&#x60;\n\n### Run the &#x60;gen_itinerary&#x60; prompt.\n\nIn your &#x60;app.py&#x60; file, change the last line to below:\n\n&#x60;&#x60;&#x60;python\nawait config.run(&quot;gen_itinerary&quot;, params=None, options=inference_options)\n&#x60;&#x60;&#x60;\n\nRe-run the command in your terminal:\n\n&#x60;&#x60;&#x60;bash\npython3 app.py\n&#x60;&#x60;&#x60;\n\nThis prompt depends on the output of &#x60;get_activities&#x60;. It also takes in parameters (user input) to determine the customized itinerary.\n\nLet&#x27;s take a closer look:\n\n**&#x60;gen_itinerary&#x60; prompt:**\n\n&#x60;&#x60;&#x60;\n&quot;Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}.&quot;\n&#x60;&#x60;&#x60;\n\n**prompt metadata:**\n\n&#x60;&#x60;&#x60;json\n{\n  &quot;metadata&quot;: {\n    &quot;model&quot;: &quot;gpt-4&quot;,\n    &quot;parameters&quot;: {\n      &quot;order_by&quot;: &quot;geographic location&quot;\n    }\n  }\n}\n&#x60;&#x60;&#x60;\n\nObserve the following:\n\n1. The prompt depends on the output of the &#x60;get_activities&#x60; prompt.\n2. It also depends on an &#x60;order_by&#x60; parameter (using {{handlebars}} syntax)\n3. It uses **gpt-4**, whereas the &#x60;get_activities&#x60; prompt it depends on uses **gpt-3.5-turbo**.\n\n&gt; Effectively, this is a prompt chain between &#x60;gen_itinerary&#x60; and &#x60;get_activities&#x60; prompts, _as well as_ as a model chain between **gpt-3.5-turbo** and **gpt-4**.\n\nLet&#x27;s run this with AIConfig:\n\nReplace &#x60;config.run&#x60; above with this:\n\n&#x60;&#x60;&#x60;python\nawait config.run(&quot;gen_itinerary&quot;, params={&quot;order_by&quot;: &quot;duration&quot;}, options=inference_options, run_with_dependencies=True)\n&#x60;&#x60;&#x60;\n\nNotice how simple the syntax is to perform a fairly complex task - running 2 different prompts across 2 different models and chaining one&#x27;s output as part of the input of another.\n\nThe code will just run &#x60;get_activities&#x60;, then pipe its output as an input to &#x60;gen_itinerary&#x60;, and finally run &#x60;gen_itinerary&#x60;.\n\n### Save the AIConfig\n\nLet&#x27;s save the AIConfig back to disk, and serialize the outputs from the latest inference run as well:\n\n&#x60;&#x60;&#x60;python\n# Save the aiconfig to disk. and serialize outputs from the model run\nconfig.save(&#x27;updated.aiconfig.json&#x27;, include_outputs=True)\n&#x60;&#x60;&#x60;\n\n### Edit &#x60;aiconfig&#x60; in a notebook editor\n\nWe can iterate on an &#x60;aiconfig&#x60; using a notebook-like editor called an **AI Workbook**. Now that we have an &#x60;aiconfig&#x60; file artifact that encapsulates the generative AI part of our application, we can iterate on it separately from the application code that uses it.\n\n1. Go to https://lastmileai.dev.\n2. Go to Workbooks page: https://lastmileai.dev/workbooks\n3. Click dropdown from &#x27;+ New Workbook&#x27; and select &#x27;Create from AIConfig&#x27;\n4. Upload &#x60;travel.aiconfig.json&#x60;\n\nhttps://github.com/lastmile-ai/aiconfig/assets/81494782/5d901493-bbda-4f8e-93c7-dd9a91bf242e\n\nTry out the workbook playground here: **[NYC Travel Workbook](https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9)**\n\n&gt; **We are working on a local editor that you can run yourself. For now, please use the hosted version on https://lastmileai.dev.**\n\n### Additional Guides\n\nThere is a lot you can do with &#x60;aiconfig&#x60;. We have several other tutorials to help get you started:\n\n- [Create an AIConfig from scratch](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig)\n- [Run a prompt](https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig)\n- [Pass data into prompts](https://aiconfig.lastmileai.dev/docs/overview/parameters)\n- [Prompt chains](https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain)\n- [Callbacks and monitoring](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\nHere are some example uses:\n\n- [CLI Chatbot](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT)\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [Chain of thought](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### OpenAI Introspection API\n\nIf you are already using OpenAI completion API&#x27;s in your application, you can get started very quickly to start saving the messages in an &#x60;aiconfig&#x60;.\n\nUsage: see openai_wrapper.ipynb.\n\nNow you can continue using &#x60;openai&#x60; completion API as normal. When you want to save the config, just call &#x60;new_config.save()&#x60; and all your openai completion calls will get serialized to disk.\n\n&gt; [**Detailed guide here**](https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig#openai-api-python-wrapper)\n\n## Supported Models\n\nAIConfig supports the following models out of the box:\n\n- OpenAI chat models (GPT-3, GPT-3.5, GPT-4)\n- LLaMA2 (running locally)\n- Google PaLM models (PaLM chat)\n- Hugging Face text generation models (e.g. Mistral-7B)\n\n### Examples\n\n- [OpenAI](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n&gt; If you need to use a model that isn&#x27;t provided out of the box, you can implement a &#x60;ModelParser&#x60; for it (see [Extending AIConfig](#extending-aiconfig)). **We welcome [contributions](https://aiconfig.lastmileai.dev/docs/contributing)**\n\n## AIConfig Schema\n\n[AIConfig specification](https://aiconfig.lastmileai.dev/docs/overview/ai-config-format)\n\n## AIConfig SDK\n\n&gt; Read the [Usage Guide](https://aiconfig.lastmileai.dev/docs/usage-guide) for more details.\n\nThe AIConfig SDK supports CRUD operations for prompts, models, parameters and metadata. Here are some common examples.\n\nThe root interface is the &#x60;AIConfigRuntime&#x60; object. That is the entrypoint for interacting with an AIConfig programmatically.\n\nLet&#x27;s go over a few key CRUD operations to give a glimpse.\n\n### AIConfig &#x60;create&#x60;\n\n&#x60;&#x60;&#x60;python\nconfig = AIConfigRuntime.create(&quot;aiconfig name&quot;, &quot;description&quot;)\n&#x60;&#x60;&#x60;\n\n### Prompt &#x60;resolve&#x60;\n\n&#x60;resolve&#x60; deserializes an existing &#x60;Prompt&#x60; into the data object that its model expects.\n\n&#x60;&#x60;&#x60;python\nconfig.resolve(&quot;prompt_name&quot;, params)\n&#x60;&#x60;&#x60;\n\n&#x60;params&#x60; are overrides you can specify to resolve any &#x60;{{handlebars}}&#x60; templates in the prompt. See the &#x60;gen_itinerary&#x60; prompt in the Getting Started example.\n\n### Prompt &#x60;serialize&#x60;\n\n&#x60;serialize&#x60; is the inverse of &#x60;resolve&#x60; -- it serializes the data object that a model understands into a &#x60;Prompt&#x60; object that can be serialized into the &#x60;aiconfig&#x60; format.\n\n&#x60;&#x60;&#x60;python\nconfig.serialize(&quot;model_name&quot;, data, &quot;prompt_name&quot;)\n&#x60;&#x60;&#x60;\n\n### Prompt &#x60;run&#x60;\n\n&#x60;run&#x60; is used to run inference for the specified &#x60;Prompt&#x60;.\n\n&#x60;&#x60;&#x60;python\nconfig.run(&quot;prompt_name&quot;, params)\n&#x60;&#x60;&#x60;\n\n### &#x60;run_with_dependencies&#x60;\n\nThis is a variant of &#x60;run&#x60; -- this re-runs all prompt dependencies.\nFor example, in [&#x60;travel.aiconfig.json&#x60;](#download-travelaiconfigjson), the &#x60;gen_itinerary&#x60; prompt references the output of the &#x60;get_activities&#x60; prompt using &#x60;{{get_activities.output}}&#x60;.\n\nRunning this function will first execute &#x60;get_activities&#x60;, and use its output to resolve the &#x60;gen_itinerary&#x60; prompt before executing it.\nThis is transitive, so it computes the Directed Acyclic Graph of dependencies to execute. Complex relationships can be modeled this way.\n\n&#x60;&#x60;&#x60;python\nconfig.run_with_dependencies(&quot;gen_itinerary&quot;)\n&#x60;&#x60;&#x60;\n\n### Updating metadata and parameters\n\nUse the &#x60;get/set_metadata&#x60; and &#x60;get/set_parameter&#x60; methods to interact with metadata and parameters (&#x60;set_parameter&#x60; is just syntactic sugar to update &#x60;&quot;metadata.parameters&quot;&#x60;)\n\n&#x60;&#x60;&#x60;python\nconfig.set_metadata(&quot;key&quot;, data, &quot;prompt_name&quot;)\n&#x60;&#x60;&#x60;\n\nNote: if &#x60;&quot;prompt_name&quot;&#x60; is specified, the metadata is updated specifically for that prompt. Otherwise, the global metadata is updated.\n\n### Register new &#x60;ModelParser&#x60;\n\nUse the &#x60;AIConfigRuntime.register_model_parser&#x60; if you want to use a different &#x60;ModelParser&#x60;, or configure AIConfig to work with an additional model.\n\nAIConfig uses the model name string to retrieve the right &#x60;ModelParser&#x60; for a given Prompt (see &#x60;AIConfigRuntime.get_model_parser&#x60;), so you can register a different ModelParser for the same ID to override which &#x60;ModelParser&#x60; handles a Prompt.\n\nFor example, suppose I want to use &#x60;MyOpenAIModelParser&#x60; to handle &#x60;gpt-4&#x60; prompts. I can do the following at the start of my application:\n\n&#x60;&#x60;&#x60;python\nAIConfigRuntime.register_model_parser(myModelParserInstance, [&quot;gpt-4&quot;])\n&#x60;&#x60;&#x60;\n\n### Callback events\n\nUse callback events to trace and monitor what&#x27;s going on -- helpful for debugging and observability.\n\n&#x60;&#x60;&#x60;python\nfrom aiconfig import AIConfigRuntime, CallbackEvent, CallbackManager\nconfig = AIConfigRuntime.load(&#x27;aiconfig.json&#x27;)\n\nasync def my_custom_callback(event: CallbackEvent) -&gt; None:\n  print(f&quot;Event triggered: {event.name}&quot;, event)\n\ncallback_manager = CallbackManager([my_custom_callback])\nconfig.set_callback_manager(callback_manager)\n\nawait config.run(&quot;prompt_name&quot;)\n&#x60;&#x60;&#x60;\n\n[**Read more** here](https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig)\n\n## Extensibility\n\nAIConfig is designed to be customized and extended for your use-case. The [Extensibility](/docs/extensibility) guide goes into more detail.\n\nCurrently, there are 3 core ways to extend AIConfig:\n\n1. [Supporting other models](https://aiconfig.lastmileai.dev/docs/extensibility#1-bring-your-own-model) - define a ModelParser extension\n2. [Callback event handlers](https://aiconfig.lastmileai.dev/docs/extensibility#2-callback-handlers) - tracing and monitoring\n3. [Custom metadata](https://aiconfig.lastmileai.dev/docs/extensibility#3-custom-metadata) - save custom fields in &#x60;aiconfig&#x60;\n\n## Contributing to &#x60;aiconfig&#x60;\n\nThis is our first open-source project and we&#x27;d love your help.\n\nSee our [contributing guidelines](https://aiconfig.lastmileai.dev/docs/contributing) -- we would especially love help adding support for additional models that the community wants.\n\n## Cookbooks\n\nWe provide several guides to demonstrate the power of &#x60;aiconfig&#x60;.\n\n&gt; **See the [&#x60;cookbooks&#x60;](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks) folder for examples to clone.**\n\n### Chatbot\n\n- [Wizard GPT](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT) - speak to a wizard on your CLI\n\n- [CLI-mate](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Cli-Mate) - help you make code-mods interactively on your codebase.\n\n### Retrieval Augmented Generated (RAG)\n\n- [RAG with AIConfig](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig)\n\nAt its core, RAG is about passing data into prompts. Read how to [pass data](/docs/overview/parameters) with AIConfig.\n\n### Function calling\n\n- [OpenAI function calling](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI)\n\n### Prompt routing\n\n- [Prompt routing](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing)\n\n### Chain of Thought\n\nA variant of chain-of-thought is Chain of Verification, used to help reduce hallucinations. Check out the aiconfig cookbook for CoVe:\n\n- [Chain of Verification](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification)\n\n### Using local LLaMA2 with &#x60;aiconfig&#x60;\n\n- [LLaMA example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama)\n\n### Hugging Face text generation\n\n- [Hugging Face (Mistral-7B) example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace)\n\n### Google PaLM\n\n- [PaLM](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)\n\n## Roadmap\n\nThis project is under active development.\n\nIf you&#x27;d like to help, please see the [contributing guidelines](#contributing-to-aiconfig).\n\nPlease create issues for additional capabilities you&#x27;d like to see.\n\nHere&#x27;s what&#x27;s already on our roadmap:\n\n- Evaluation interfaces: allow &#x60;aiconfig&#x60; artifacts to be evaluated with user-defined eval functions.\n  - We are also considering integrating with existing evaluation frameworks.\n- Local editor for &#x60;aiconfig&#x60;: enable you to interact with aiconfigs more intuitively.\n- OpenAI Assistants API support\n- Multi-modal ModelParsers:\n  - GPT4-V support\n  - DALLE-3\n  - Whisper\n  - HuggingFace image generation\n\n## FAQs\n\n### How should I edit an &#x60;aiconfig&#x60; file?\n\nEditing a configshould be done either programmatically via SDK or via the UI (workbooks):\n\n- [Programmatic](https://github.com/lastmile-ai/aiconfig/blob/main/cookbooks/Create-AIConfig-Programmatically/create_aiconfig_programmatically.ipynb) editing.\n\n- [Edit with a workbook](#edit-aiconfig-in-a-notebook-editor) editor: this is similar to editing an ipynb file as a notebook (most people never touch the json ipynb directly)\n\nYou should only edit the &#x60;aiconfig&#x60; by hand for minor modifications, like tweaking a prompt string or updating some metadata.\n\n### Does this support custom endpoints?\n\nOut of the box, AIConfig already supports all OpenAI GPT\\* models, Googles PaLM model and any textgeneration model on Hugging Face (like Mistral). See [Supported Models](#supported-models) for more details.\n\nAdditionally, you can install &#x60;aiconfig&#x60; [extensions](https://github.com/lastmile-ai/aiconfig/tree/main/extensions) for additional models (see question below).\n\n### Is OpenAI function calling supported?\n\nYes. [This example](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI) goes through how to do it.\n\nWe are also working on adding support for the Assistants API.\n\n### How can I use aiconfig with my own model endpoint?\n\nModel support is implemented as ModelParsers in the AIConfig SDK, and the idea is that anyone, including you, can define a ModelParser (and even publish it as an extension package).\n\nAll thats needed to use a model with AIConfig is a ModelParser that knows\n\n- how to serialize data from a model into the aiconfig format\n- how to deserialize data from an aiconfig into the type the model expects\n- how to run inference for model.\n\nFor more details, see [Extensibility](https://aiconfig.lastmileai.dev/docs/extensibility).\n\n### When should I store outputs in an &#x60;aiconfig&#x60;?\n\nThe &#x60;AIConfigRuntime&#x60; object is used to interact with an aiconfig programmatically (see [SDK usage guide](#aiconfig-sdk)). As you run prompts, this object keeps track of the outputs returned from the model.\n\nYou can choose to serialize these outputs back into the &#x60;aiconfig&#x60; by using the &#x60;config.save(include_outputs=True)&#x60; API. This can be useful for preserving context -- think of it like session state.\n\nFor example, you can use aiconfig to create a chatbot, and use the same format to save the chat history so it can be resumed for the next session.\n\nYou can also choose to save outputs to a _different_ file than the original config -- &#x60;config.save(&quot;history.aiconfig.json&quot;, include_outputs=True)&#x60;.\n\n### Why should I use &#x60;aiconfig&#x60; instead of things like [configurator](https://pypi.org/project/configurator/)?\n\nIt helps to have a [standardized format](http://aiconfig.lastmileai.dev/docs/overview/ai-config-format) specifically for storing generative AI prompts, inference results, model parameters and arbitrary metadata, as opposed to a general-purpose configuration schema.\n\nWith that standardization, you just need a layer that knows how to serialize/deserialize from that format into whatever the inference endpoints require.\n\n### This looks similar to &#x60;ipynb&#x60; for Jupyter notebooks\n\nWe believe that notebooks are a perfect iteration environment for generative AI -- they are flexible, multi-modal, and collaborative.\n\nThe multi-modality and flexibility offered by notebooks and [&#x60;ipynb&#x60;](https://ipython.org/ipython-doc/3/notebook/nbformat.html) offers a good interaction model for generative AI. The &#x60;aiconfig&#x60; file format is extensible like &#x60;ipynb&#x60;, and AI Workbook editor allows rapid iteration in a notebook-like IDE.\n\n_AI Workbooks are to AIConfig what Jupyter notebooks are to &#x60;ipynb&#x60;_\n\nThere are 2 areas where we are going beyond what notebooks offer:\n\n1. &#x60;aiconfig&#x60; is more **source-control friendly** than &#x60;ipynb&#x60;. &#x60;ipynb&#x60; stores binary data (images, etc.) by encoding it in the file, while &#x60;aiconfig&#x60; recommends using file URI references instead.\n2. &#x60;aiconfig&#x60; can be imported and **connected to application code** using the AIConfig SDK.\n', 'role': 'user'}]}} ts_ns=1702175485063260400
2023-12-09 21:31:45,519 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The 'aiconfig' repository is an open-source software that provides a standardized framework for managing prompts, models, and model parameters for generative AI applications. It stores these as source control friendly configurations, allowing developers to modify and iterate on these elements separately from the application code. \n\nKey features include: \n1. A standardized JSON format to store gener", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'length'})]} ts_ns=1702175485063260400
2023-12-09 21:31:45,519 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The 'aiconfig' repository is an open-source software that provides a standardized framework for managing prompts, models, and model parameters for generative AI applications. It stores these as source control friendly configurations, allowing developers to modify and iterate on these elements separately from the application code. \n\nKey features include: \n1. A standardized JSON format to store gener", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'length'})]} ts_ns=1702175485063260400
2023-12-09 21:31:45,519 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The 'aiconfig' repository is an open-source software that provides a standardized framework for managing prompts, models, and model parameters for generative AI applications. It stores these as source control friendly configurations, allowing developers to modify and iterate on these elements separately from the application code. \n\nKey features include: \n1. A standardized JSON format to store gener", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'length'})]} ts_ns=1702175485063260400
2023-12-09 21:31:45,519 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The 'aiconfig' repository is an open-source software that provides a standardized framework for managing prompts, models, and model parameters for generative AI applications. It stores these as source control friendly configurations, allowing developers to modify and iterate on these elements separately from the application code. \n\nKey features include: \n1. A standardized JSON format to store gener", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'length'})]} ts_ns=1702175485063260400
2023-12-09 21:31:45,519 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The 'aiconfig' repository is an open-source software that provides a standardized framework for managing prompts, models, and model parameters for generative AI applications. It stores these as source control friendly configurations, allowing developers to modify and iterate on these elements separately from the application code. \n\nKey features include: \n1. A standardized JSON format to store gener", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'length'})]} ts_ns=1702175485063260400
2023-12-09 21:31:45,519 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The 'aiconfig' repository is an open-source software that provides a standardized framework for managing prompts, models, and model parameters for generative AI applications. It stores these as source control friendly configurations, allowing developers to modify and iterate on these elements separately from the application code. \n\nKey features include: \n1. A standardized JSON format to store gener", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'length'})]} ts_ns=1702175485063260400
2023-12-09 21:40:45,672 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The Supabase repository is an open-source alternative to Firebase, providing a suite of backend services using enterprise-grade, open-source tools. Supabase offers a combination of features such as a hosted Postgres database, authentication and authorization, auto-generated APIs including REST, GraphQL and real-time subscriptions, functions including database and edge functions, file storage, and an AI toolkit. \n\nThe repository contains extensive documentation links for each feature, as well as a visual representation of the Supabase dashboard. Additional resources for community discussion, bug reporting, and support are provided. Supabase's architecture supports both hosted and self-hosted options, with a focus on modularity in client library support. Links to a variety of client libraries in different programming languages are included.\n\nThe repository also provides badges for users to display on their own project READMEs to indicate they are using Supabase. The README file for the Supabase repository has been translated into many languages to make the platform accessible to a global user base.\n", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175567735757000
2023-12-09 21:40:45,672 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The Supabase repository is an open-source alternative to Firebase, providing a suite of backend services using enterprise-grade, open-source tools. Supabase offers a combination of features such as a hosted Postgres database, authentication and authorization, auto-generated APIs including REST, GraphQL and real-time subscriptions, functions including database and edge functions, file storage, and an AI toolkit. \n\nThe repository contains extensive documentation links for each feature, as well as a visual representation of the Supabase dashboard. Additional resources for community discussion, bug reporting, and support are provided. Supabase's architecture supports both hosted and self-hosted options, with a focus on modularity in client library support. Links to a variety of client libraries in different programming languages are included.\n\nThe repository also provides badges for users to display on their own project READMEs to indicate they are using Supabase. The README file for the Supabase repository has been translated into many languages to make the platform accessible to a global user base.\n", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175567735757000
2023-12-09 21:40:45,672 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The Supabase repository is an open-source alternative to Firebase, providing a suite of backend services using enterprise-grade, open-source tools. Supabase offers a combination of features such as a hosted Postgres database, authentication and authorization, auto-generated APIs including REST, GraphQL and real-time subscriptions, functions including database and edge functions, file storage, and an AI toolkit. \n\nThe repository contains extensive documentation links for each feature, as well as a visual representation of the Supabase dashboard. Additional resources for community discussion, bug reporting, and support are provided. Supabase's architecture supports both hosted and self-hosted options, with a focus on modularity in client library support. Links to a variety of client libraries in different programming languages are included.\n\nThe repository also provides badges for users to display on their own project READMEs to indicate they are using Supabase. The README file for the Supabase repository has been translated into many languages to make the platform accessible to a global user base.\n", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175567735757000
2023-12-09 21:40:45,672 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The Supabase repository is an open-source alternative to Firebase, providing a suite of backend services using enterprise-grade, open-source tools. Supabase offers a combination of features such as a hosted Postgres database, authentication and authorization, auto-generated APIs including REST, GraphQL and real-time subscriptions, functions including database and edge functions, file storage, and an AI toolkit. \n\nThe repository contains extensive documentation links for each feature, as well as a visual representation of the Supabase dashboard. Additional resources for community discussion, bug reporting, and support are provided. Supabase's architecture supports both hosted and self-hosted options, with a focus on modularity in client library support. Links to a variety of client libraries in different programming languages are included.\n\nThe repository also provides badges for users to display on their own project READMEs to indicate they are using Supabase. The README file for the Supabase repository has been translated into many languages to make the platform accessible to a global user base.\n", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175567735757000
2023-12-09 21:40:45,672 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The Supabase repository is an open-source alternative to Firebase, providing a suite of backend services using enterprise-grade, open-source tools. Supabase offers a combination of features such as a hosted Postgres database, authentication and authorization, auto-generated APIs including REST, GraphQL and real-time subscriptions, functions including database and edge functions, file storage, and an AI toolkit. \n\nThe repository contains extensive documentation links for each feature, as well as a visual representation of the Supabase dashboard. Additional resources for community discussion, bug reporting, and support are provided. Supabase's architecture supports both hosted and self-hosted options, with a focus on modularity in client library support. Links to a variety of client libraries in different programming languages are included.\n\nThe repository also provides badges for users to display on their own project READMEs to indicate they are using Supabase. The README file for the Supabase repository has been translated into many languages to make the platform accessible to a global user base.\n", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175567735757000
2023-12-09 21:40:45,672 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The Supabase repository is an open-source alternative to Firebase, providing a suite of backend services using enterprise-grade, open-source tools. Supabase offers a combination of features such as a hosted Postgres database, authentication and authorization, auto-generated APIs including REST, GraphQL and real-time subscriptions, functions including database and edge functions, file storage, and an AI toolkit. \n\nThe repository contains extensive documentation links for each feature, as well as a visual representation of the Supabase dashboard. Additional resources for community discussion, bug reporting, and support are provided. Supabase's architecture supports both hosted and self-hosted options, with a focus on modularity in client library support. Links to a variety of client libraries in different programming languages are included.\n\nThe repository also provides badges for users to display on their own project READMEs to indicate they are using Supabase. The README file for the Supabase repository has been translated into many languages to make the platform accessible to a global user base.\n", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175567735757000
2023-12-09 21:40:45,672 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The Supabase repository is an open-source alternative to Firebase, providing a suite of backend services using enterprise-grade, open-source tools. Supabase offers a combination of features such as a hosted Postgres database, authentication and authorization, auto-generated APIs including REST, GraphQL and real-time subscriptions, functions including database and edge functions, file storage, and an AI toolkit. \n\nThe repository contains extensive documentation links for each feature, as well as a visual representation of the Supabase dashboard. Additional resources for community discussion, bug reporting, and support are provided. Supabase's architecture supports both hosted and self-hosted options, with a focus on modularity in client library support. Links to a variety of client libraries in different programming languages are included.\n\nThe repository also provides badges for users to display on their own project READMEs to indicate they are using Supabase. The README file for the Supabase repository has been translated into many languages to make the platform accessible to a global user base.\n", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175567735757000
2023-12-09 21:40:45,672 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The Supabase repository is an open-source alternative to Firebase, providing a suite of backend services using enterprise-grade, open-source tools. Supabase offers a combination of features such as a hosted Postgres database, authentication and authorization, auto-generated APIs including REST, GraphQL and real-time subscriptions, functions including database and edge functions, file storage, and an AI toolkit. \n\nThe repository contains extensive documentation links for each feature, as well as a visual representation of the Supabase dashboard. Additional resources for community discussion, bug reporting, and support are provided. Supabase's architecture supports both hosted and self-hosted options, with a focus on modularity in client library support. Links to a variety of client libraries in different programming languages are included.\n\nThe repository also provides badges for users to display on their own project READMEs to indicate they are using Supabase. The README file for the Supabase repository has been translated into many languages to make the platform accessible to a global user base.\n", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175567735757000
2023-12-09 21:40:45,672 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The Supabase repository is an open-source alternative to Firebase, providing a suite of backend services using enterprise-grade, open-source tools. Supabase offers a combination of features such as a hosted Postgres database, authentication and authorization, auto-generated APIs including REST, GraphQL and real-time subscriptions, functions including database and edge functions, file storage, and an AI toolkit. \n\nThe repository contains extensive documentation links for each feature, as well as a visual representation of the Supabase dashboard. Additional resources for community discussion, bug reporting, and support are provided. Supabase's architecture supports both hosted and self-hosted options, with a focus on modularity in client library support. Links to a variety of client libraries in different programming languages are included.\n\nThe repository also provides badges for users to display on their own project READMEs to indicate they are using Supabase. The README file for the Supabase repository has been translated into many languages to make the platform accessible to a global user base.\n", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175567735757000
2023-12-09 21:40:45,672 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The Supabase repository is an open-source alternative to Firebase, providing a suite of backend services using enterprise-grade, open-source tools. Supabase offers a combination of features such as a hosted Postgres database, authentication and authorization, auto-generated APIs including REST, GraphQL and real-time subscriptions, functions including database and edge functions, file storage, and an AI toolkit. \n\nThe repository contains extensive documentation links for each feature, as well as a visual representation of the Supabase dashboard. Additional resources for community discussion, bug reporting, and support are provided. Supabase's architecture supports both hosted and self-hosted options, with a focus on modularity in client library support. Links to a variety of client libraries in different programming languages are included.\n\nThe repository also provides badges for users to display on their own project READMEs to indicate they are using Supabase. The README file for the Supabase repository has been translated into many languages to make the platform accessible to a global user base.\n", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175567735757000
2023-12-09 21:40:45,672 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The Supabase repository is an open-source alternative to Firebase, providing a suite of backend services using enterprise-grade, open-source tools. Supabase offers a combination of features such as a hosted Postgres database, authentication and authorization, auto-generated APIs including REST, GraphQL and real-time subscriptions, functions including database and edge functions, file storage, and an AI toolkit. \n\nThe repository contains extensive documentation links for each feature, as well as a visual representation of the Supabase dashboard. Additional resources for community discussion, bug reporting, and support are provided. Supabase's architecture supports both hosted and self-hosted options, with a focus on modularity in client library support. Links to a variety of client libraries in different programming languages are included.\n\nThe repository also provides badges for users to display on their own project READMEs to indicate they are using Supabase. The README file for the Supabase repository has been translated into many languages to make the platform accessible to a global user base.\n", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175567735757000
2023-12-09 21:40:45,675 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The Supabase repository is an open-source alternative to Firebase, providing a suite of backend services using enterprise-grade, open-source tools. Supabase offers a combination of features such as a hosted Postgres database, authentication and authorization, auto-generated APIs including REST, GraphQL and real-time subscriptions, functions including database and edge functions, file storage, and an AI toolkit. \n\nThe repository contains extensive documentation links for each feature, as well as a visual representation of the Supabase dashboard. Additional resources for community discussion, bug reporting, and support are provided. Supabase's architecture supports both hosted and self-hosted options, with a focus on modularity in client library support. Links to a variety of client libraries in different programming languages are included.\n\nThe repository also provides badges for users to display on their own project READMEs to indicate they are using Supabase. The README file for the Supabase repository has been translated into many languages to make the platform accessible to a global user base.\n", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175567735757000
2023-12-09 21:40:45,675 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The Supabase repository is an open-source alternative to Firebase, providing a suite of backend services using enterprise-grade, open-source tools. Supabase offers a combination of features such as a hosted Postgres database, authentication and authorization, auto-generated APIs including REST, GraphQL and real-time subscriptions, functions including database and edge functions, file storage, and an AI toolkit. \n\nThe repository contains extensive documentation links for each feature, as well as a visual representation of the Supabase dashboard. Additional resources for community discussion, bug reporting, and support are provided. Supabase's architecture supports both hosted and self-hosted options, with a focus on modularity in client library support. Links to a variety of client libraries in different programming languages are included.\n\nThe repository also provides badges for users to display on their own project READMEs to indicate they are using Supabase. The README file for the Supabase repository has been translated into many languages to make the platform accessible to a global user base.\n", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175567735757000
2023-12-09 21:40:45,675 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The Supabase repository is an open-source alternative to Firebase, providing a suite of backend services using enterprise-grade, open-source tools. Supabase offers a combination of features such as a hosted Postgres database, authentication and authorization, auto-generated APIs including REST, GraphQL and real-time subscriptions, functions including database and edge functions, file storage, and an AI toolkit. \n\nThe repository contains extensive documentation links for each feature, as well as a visual representation of the Supabase dashboard. Additional resources for community discussion, bug reporting, and support are provided. Supabase's architecture supports both hosted and self-hosted options, with a focus on modularity in client library support. Links to a variety of client libraries in different programming languages are included.\n\nThe repository also provides badges for users to display on their own project READMEs to indicate they are using Supabase. The README file for the Supabase repository has been translated into many languages to make the platform accessible to a global user base.\n", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175567735757000
2023-12-09 21:40:45,675 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The Supabase repository is an open-source alternative to Firebase, providing a suite of backend services using enterprise-grade, open-source tools. Supabase offers a combination of features such as a hosted Postgres database, authentication and authorization, auto-generated APIs including REST, GraphQL and real-time subscriptions, functions including database and edge functions, file storage, and an AI toolkit. \n\nThe repository contains extensive documentation links for each feature, as well as a visual representation of the Supabase dashboard. Additional resources for community discussion, bug reporting, and support are provided. Supabase's architecture supports both hosted and self-hosted options, with a focus on modularity in client library support. Links to a variety of client libraries in different programming languages are included.\n\nThe repository also provides badges for users to display on their own project READMEs to indicate they are using Supabase. The README file for the Supabase repository has been translated into many languages to make the platform accessible to a global user base.\n", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175567735757000
2023-12-09 21:40:45,675 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The Supabase repository is an open-source alternative to Firebase, providing a suite of backend services using enterprise-grade, open-source tools. Supabase offers a combination of features such as a hosted Postgres database, authentication and authorization, auto-generated APIs including REST, GraphQL and real-time subscriptions, functions including database and edge functions, file storage, and an AI toolkit. \n\nThe repository contains extensive documentation links for each feature, as well as a visual representation of the Supabase dashboard. Additional resources for community discussion, bug reporting, and support are provided. Supabase's architecture supports both hosted and self-hosted options, with a focus on modularity in client library support. Links to a variety of client libraries in different programming languages are included.\n\nThe repository also provides badges for users to display on their own project READMEs to indicate they are using Supabase. The README file for the Supabase repository has been translated into many languages to make the platform accessible to a global user base.\n", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175567735757000
2023-12-09 21:40:45,675 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The Supabase repository is an open-source alternative to Firebase, providing a suite of backend services using enterprise-grade, open-source tools. Supabase offers a combination of features such as a hosted Postgres database, authentication and authorization, auto-generated APIs including REST, GraphQL and real-time subscriptions, functions including database and edge functions, file storage, and an AI toolkit. \n\nThe repository contains extensive documentation links for each feature, as well as a visual representation of the Supabase dashboard. Additional resources for community discussion, bug reporting, and support are provided. Supabase's architecture supports both hosted and self-hosted options, with a focus on modularity in client library support. Links to a variety of client libraries in different programming languages are included.\n\nThe repository also provides badges for users to display on their own project READMEs to indicate they are using Supabase. The README file for the Supabase repository has been translated into many languages to make the platform accessible to a global user base.\n", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175567735757000
2023-12-09 21:40:45,675 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The Supabase repository is an open-source alternative to Firebase, providing a suite of backend services using enterprise-grade, open-source tools. Supabase offers a combination of features such as a hosted Postgres database, authentication and authorization, auto-generated APIs including REST, GraphQL and real-time subscriptions, functions including database and edge functions, file storage, and an AI toolkit. \n\nThe repository contains extensive documentation links for each feature, as well as a visual representation of the Supabase dashboard. Additional resources for community discussion, bug reporting, and support are provided. Supabase's architecture supports both hosted and self-hosted options, with a focus on modularity in client library support. Links to a variety of client libraries in different programming languages are included.\n\nThe repository also provides badges for users to display on their own project READMEs to indicate they are using Supabase. The README file for the Supabase repository has been translated into many languages to make the platform accessible to a global user base.\n", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175567735757000
2023-12-09 21:40:45,675 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The Supabase repository is an open-source alternative to Firebase, providing a suite of backend services using enterprise-grade, open-source tools. Supabase offers a combination of features such as a hosted Postgres database, authentication and authorization, auto-generated APIs including REST, GraphQL and real-time subscriptions, functions including database and edge functions, file storage, and an AI toolkit. \n\nThe repository contains extensive documentation links for each feature, as well as a visual representation of the Supabase dashboard. Additional resources for community discussion, bug reporting, and support are provided. Supabase's architecture supports both hosted and self-hosted options, with a focus on modularity in client library support. Links to a variety of client libraries in different programming languages are included.\n\nThe repository also provides badges for users to display on their own project READMEs to indicate they are using Supabase. The README file for the Supabase repository has been translated into many languages to make the platform accessible to a global user base.\n", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175567735757000
2023-12-09 21:40:45,675 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The Supabase repository is an open-source alternative to Firebase, providing a suite of backend services using enterprise-grade, open-source tools. Supabase offers a combination of features such as a hosted Postgres database, authentication and authorization, auto-generated APIs including REST, GraphQL and real-time subscriptions, functions including database and edge functions, file storage, and an AI toolkit. \n\nThe repository contains extensive documentation links for each feature, as well as a visual representation of the Supabase dashboard. Additional resources for community discussion, bug reporting, and support are provided. Supabase's architecture supports both hosted and self-hosted options, with a focus on modularity in client library support. Links to a variety of client libraries in different programming languages are included.\n\nThe repository also provides badges for users to display on their own project READMEs to indicate they are using Supabase. The README file for the Supabase repository has been translated into many languages to make the platform accessible to a global user base.\n", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175567735757000
2023-12-09 21:40:45,675 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The Supabase repository is an open-source alternative to Firebase, providing a suite of backend services using enterprise-grade, open-source tools. Supabase offers a combination of features such as a hosted Postgres database, authentication and authorization, auto-generated APIs including REST, GraphQL and real-time subscriptions, functions including database and edge functions, file storage, and an AI toolkit. \n\nThe repository contains extensive documentation links for each feature, as well as a visual representation of the Supabase dashboard. Additional resources for community discussion, bug reporting, and support are provided. Supabase's architecture supports both hosted and self-hosted options, with a focus on modularity in client library support. Links to a variety of client libraries in different programming languages are included.\n\nThe repository also provides badges for users to display on their own project READMEs to indicate they are using Supabase. The README file for the Supabase repository has been translated into many languages to make the platform accessible to a global user base.\n", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175567735757000
2023-12-09 21:40:45,675 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The Supabase repository is an open-source alternative to Firebase, providing a suite of backend services using enterprise-grade, open-source tools. Supabase offers a combination of features such as a hosted Postgres database, authentication and authorization, auto-generated APIs including REST, GraphQL and real-time subscriptions, functions including database and edge functions, file storage, and an AI toolkit. \n\nThe repository contains extensive documentation links for each feature, as well as a visual representation of the Supabase dashboard. Additional resources for community discussion, bug reporting, and support are provided. Supabase's architecture supports both hosted and self-hosted options, with a focus on modularity in client library support. Links to a variety of client libraries in different programming languages are included.\n\nThe repository also provides badges for users to display on their own project READMEs to indicate they are using Supabase. The README file for the Supabase repository has been translated into many languages to make the platform accessible to a global user base.\n", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702175567735757000
=======
2023-12-09 20:44:25,124 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702172657644256400
2023-12-09 20:44:25,124 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702172657644256400
2023-12-09 20:44:25,124 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702172657644256400
2023-12-09 20:44:25,124 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:44:25,124 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:44:25,124 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:44:25,124 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:44:25,124 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:44:25,124 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:44:25,124 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'temperature': 0.75, 'presence_penalty': 0, 'top_p': 1, 'frequency_penalty': 0, 'model': 'gpt-4', 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702172657644256400
2023-12-09 20:44:25,124 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'temperature': 0.75, 'presence_penalty': 0, 'top_p': 1, 'frequency_penalty': 0, 'model': 'gpt-4', 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702172657644256400
2023-12-09 20:44:25,124 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'temperature': 0.75, 'presence_penalty': 0, 'top_p': 1, 'frequency_penalty': 0, 'model': 'gpt-4', 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702172657644256400
2023-12-09 20:44:38,295 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The GitHub repository named "generative-models" is maintained by Stability AI. It provides a comprehensive suite of generative models including "SDXL-Turbo" for fast text-to-image conversion, "Stable Video Diffusion" for image-to-video conversion, and several versions of "SDXL" diffusion models. All these models are released for research purposes and some require users to request access via Hugging Face.\n\nThe repository leverages a modular, config-driven architecture and uses PyTorch Lightning for training. It has undergone improvements in order to handle all types of conditioning inputs in a single class "GeneralConditioner". It also separates "guiders" from "samplers", making the latter independent of the model. It supports the "denoiser framework" for both training and inference, enabling the option to train continuous time models.\n\nIn addition to providing a streamlit demo for text-to-image and image-to-image sampling, the repository includes features like invisible watermark detection for images generated through its code. The repository also provides guidance on how to build new diffusion models, handle datasets, and train models using provided config examples.\n\nThe repository also provides a comprehensive guide for installation, which requires Python 3.10 and PyTorch 2.0. It uses a PEP 517 compliant packaging method using Hatch.\n\nThis repository is designed for researchers interested in generative models, text-to-image, and image-to-video conversion.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:44:38,295 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The GitHub repository named "generative-models" is maintained by Stability AI. It provides a comprehensive suite of generative models including "SDXL-Turbo" for fast text-to-image conversion, "Stable Video Diffusion" for image-to-video conversion, and several versions of "SDXL" diffusion models. All these models are released for research purposes and some require users to request access via Hugging Face.\n\nThe repository leverages a modular, config-driven architecture and uses PyTorch Lightning for training. It has undergone improvements in order to handle all types of conditioning inputs in a single class "GeneralConditioner". It also separates "guiders" from "samplers", making the latter independent of the model. It supports the "denoiser framework" for both training and inference, enabling the option to train continuous time models.\n\nIn addition to providing a streamlit demo for text-to-image and image-to-image sampling, the repository includes features like invisible watermark detection for images generated through its code. The repository also provides guidance on how to build new diffusion models, handle datasets, and train models using provided config examples.\n\nThe repository also provides a comprehensive guide for installation, which requires Python 3.10 and PyTorch 2.0. It uses a PEP 517 compliant packaging method using Hatch.\n\nThis repository is designed for researchers interested in generative models, text-to-image, and image-to-video conversion.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:44:38,295 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The GitHub repository named "generative-models" is maintained by Stability AI. It provides a comprehensive suite of generative models including "SDXL-Turbo" for fast text-to-image conversion, "Stable Video Diffusion" for image-to-video conversion, and several versions of "SDXL" diffusion models. All these models are released for research purposes and some require users to request access via Hugging Face.\n\nThe repository leverages a modular, config-driven architecture and uses PyTorch Lightning for training. It has undergone improvements in order to handle all types of conditioning inputs in a single class "GeneralConditioner". It also separates "guiders" from "samplers", making the latter independent of the model. It supports the "denoiser framework" for both training and inference, enabling the option to train continuous time models.\n\nIn addition to providing a streamlit demo for text-to-image and image-to-image sampling, the repository includes features like invisible watermark detection for images generated through its code. The repository also provides guidance on how to build new diffusion models, handle datasets, and train models using provided config examples.\n\nThe repository also provides a comprehensive guide for installation, which requires Python 3.10 and PyTorch 2.0. It uses a PEP 517 compliant packaging method using Hatch.\n\nThis repository is designed for researchers interested in generative models, text-to-image, and image-to-video conversion.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:44:38,295 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The GitHub repository named "generative-models" is maintained by Stability AI. It provides a comprehensive suite of generative models including "SDXL-Turbo" for fast text-to-image conversion, "Stable Video Diffusion" for image-to-video conversion, and several versions of "SDXL" diffusion models. All these models are released for research purposes and some require users to request access via Hugging Face.\n\nThe repository leverages a modular, config-driven architecture and uses PyTorch Lightning for training. It has undergone improvements in order to handle all types of conditioning inputs in a single class "GeneralConditioner". It also separates "guiders" from "samplers", making the latter independent of the model. It supports the "denoiser framework" for both training and inference, enabling the option to train continuous time models.\n\nIn addition to providing a streamlit demo for text-to-image and image-to-image sampling, the repository includes features like invisible watermark detection for images generated through its code. The repository also provides guidance on how to build new diffusion models, handle datasets, and train models using provided config examples.\n\nThe repository also provides a comprehensive guide for installation, which requires Python 3.10 and PyTorch 2.0. It uses a PEP 517 compliant packaging method using Hatch.\n\nThis repository is designed for researchers interested in generative models, text-to-image, and image-to-video conversion.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:44:38,295 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The GitHub repository named "generative-models" is maintained by Stability AI. It provides a comprehensive suite of generative models including "SDXL-Turbo" for fast text-to-image conversion, "Stable Video Diffusion" for image-to-video conversion, and several versions of "SDXL" diffusion models. All these models are released for research purposes and some require users to request access via Hugging Face.\n\nThe repository leverages a modular, config-driven architecture and uses PyTorch Lightning for training. It has undergone improvements in order to handle all types of conditioning inputs in a single class "GeneralConditioner". It also separates "guiders" from "samplers", making the latter independent of the model. It supports the "denoiser framework" for both training and inference, enabling the option to train continuous time models.\n\nIn addition to providing a streamlit demo for text-to-image and image-to-image sampling, the repository includes features like invisible watermark detection for images generated through its code. The repository also provides guidance on how to build new diffusion models, handle datasets, and train models using provided config examples.\n\nThe repository also provides a comprehensive guide for installation, which requires Python 3.10 and PyTorch 2.0. It uses a PEP 517 compliant packaging method using Hatch.\n\nThis repository is designed for researchers interested in generative models, text-to-image, and image-to-video conversion.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:44:38,295 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The GitHub repository named "generative-models" is maintained by Stability AI. It provides a comprehensive suite of generative models including "SDXL-Turbo" for fast text-to-image conversion, "Stable Video Diffusion" for image-to-video conversion, and several versions of "SDXL" diffusion models. All these models are released for research purposes and some require users to request access via Hugging Face.\n\nThe repository leverages a modular, config-driven architecture and uses PyTorch Lightning for training. It has undergone improvements in order to handle all types of conditioning inputs in a single class "GeneralConditioner". It also separates "guiders" from "samplers", making the latter independent of the model. It supports the "denoiser framework" for both training and inference, enabling the option to train continuous time models.\n\nIn addition to providing a streamlit demo for text-to-image and image-to-image sampling, the repository includes features like invisible watermark detection for images generated through its code. The repository also provides guidance on how to build new diffusion models, handle datasets, and train models using provided config examples.\n\nThe repository also provides a comprehensive guide for installation, which requires Python 3.10 and PyTorch 2.0. It uses a PEP 517 compliant packaging method using Hatch.\n\nThis repository is designed for researchers interested in generative models, text-to-image, and image-to-video conversion.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:44:38,295 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The GitHub repository named "generative-models" is maintained by Stability AI. It provides a comprehensive suite of generative models including "SDXL-Turbo" for fast text-to-image conversion, "Stable Video Diffusion" for image-to-video conversion, and several versions of "SDXL" diffusion models. All these models are released for research purposes and some require users to request access via Hugging Face.\n\nThe repository leverages a modular, config-driven architecture and uses PyTorch Lightning for training. It has undergone improvements in order to handle all types of conditioning inputs in a single class "GeneralConditioner". It also separates "guiders" from "samplers", making the latter independent of the model. It supports the "denoiser framework" for both training and inference, enabling the option to train continuous time models.\n\nIn addition to providing a streamlit demo for text-to-image and image-to-image sampling, the repository includes features like invisible watermark detection for images generated through its code. The repository also provides guidance on how to build new diffusion models, handle datasets, and train models using provided config examples.\n\nThe repository also provides a comprehensive guide for installation, which requires Python 3.10 and PyTorch 2.0. It uses a PEP 517 compliant packaging method using Hatch.\n\nThis repository is designed for researchers interested in generative models, text-to-image, and image-to-video conversion.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:44:38,295 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The GitHub repository named "generative-models" is maintained by Stability AI. It provides a comprehensive suite of generative models including "SDXL-Turbo" for fast text-to-image conversion, "Stable Video Diffusion" for image-to-video conversion, and several versions of "SDXL" diffusion models. All these models are released for research purposes and some require users to request access via Hugging Face.\n\nThe repository leverages a modular, config-driven architecture and uses PyTorch Lightning for training. It has undergone improvements in order to handle all types of conditioning inputs in a single class "GeneralConditioner". It also separates "guiders" from "samplers", making the latter independent of the model. It supports the "denoiser framework" for both training and inference, enabling the option to train continuous time models.\n\nIn addition to providing a streamlit demo for text-to-image and image-to-image sampling, the repository includes features like invisible watermark detection for images generated through its code. The repository also provides guidance on how to build new diffusion models, handle datasets, and train models using provided config examples.\n\nThe repository also provides a comprehensive guide for installation, which requires Python 3.10 and PyTorch 2.0. It uses a PEP 517 compliant packaging method using Hatch.\n\nThis repository is designed for researchers interested in generative models, text-to-image, and image-to-video conversion.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'temperature': 0.75, 'presence_penalty': 0, 'top_p': 1, 'frequency_penalty': 0, 'model': 'gpt-4', 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'temperature': 0.75, 'presence_penalty': 0, 'top_p': 1, 'frequency_penalty': 0, 'model': 'gpt-4', 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'temperature': 0.75, 'presence_penalty': 0, 'top_p': 1, 'frequency_penalty': 0, 'model': 'gpt-4', 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'temperature': 0.75, 'presence_penalty': 0, 'top_p': 1, 'frequency_penalty': 0, 'model': 'gpt-4', 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'temperature': 0.75, 'presence_penalty': 0, 'top_p': 1, 'frequency_penalty': 0, 'model': 'gpt-4', 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702172657644256400
2023-12-09 20:49:14,202 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'temperature': 0.75, 'presence_penalty': 0, 'top_p': 1, 'frequency_penalty': 0, 'model': 'gpt-4', 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702172657644256400
2023-12-09 20:49:27,251 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The "generative-models" repository by Stability AI is a collection of generative models, with a focus on text-to-image and image-to-image diffusion models. The repository provides a series of models and updates, the latest being "SDXL-Turbo" and "Stable Video Diffusion". \n\nKey features of the repository:\n\n1. "SDXL-Turbo" is a text-to-image model that is very fast. It comes with a technical report and instructions for installation and usage.\n2. "Stable Video Diffusion" is an image-to-video model intended for research. It was trained to generate 14 frames at 576x1024 resolution given a context frame of the same size. It uses the standard image encoder from SD 2.1, with a "deflickering decoder" replacing the original decoder.\n3. Previous releases include open models under a "CreativeML Open RAIL++-M" license, and diffusion models for research purposes.\n4. The codebase follows a config-driven approach, where submodules are built and combined based on yaml configs. It supports both PyTorch 1.13 and 2.0 for training generative models.\n5. The repository provides detailed instructions for the installation of the models and any necessary dependencies. \n6. It provides a PEP 517 compliant packaging using Hatch for building a distributable wheel.\n7. Inference can be done via a streamlit demo for text-to-image and image-to-image sampling. There is also a script provided to detect invisible watermarks in the generated model output. \n8. Training is supported with example training configs provided in the repository. The repository also supports building new diffusion models.\n9. For large-scale training, it recommends using the data pipelines from their "data pipelines" project. \n\nThe repository provides comprehensive instructions and necessary scripts for the usage, training, and building of new models based on the provided generative models. It is intended to be used for research and development in generative AI and machine learning.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:49:27,251 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The "generative-models" repository by Stability AI is a collection of generative models, with a focus on text-to-image and image-to-image diffusion models. The repository provides a series of models and updates, the latest being "SDXL-Turbo" and "Stable Video Diffusion". \n\nKey features of the repository:\n\n1. "SDXL-Turbo" is a text-to-image model that is very fast. It comes with a technical report and instructions for installation and usage.\n2. "Stable Video Diffusion" is an image-to-video model intended for research. It was trained to generate 14 frames at 576x1024 resolution given a context frame of the same size. It uses the standard image encoder from SD 2.1, with a "deflickering decoder" replacing the original decoder.\n3. Previous releases include open models under a "CreativeML Open RAIL++-M" license, and diffusion models for research purposes.\n4. The codebase follows a config-driven approach, where submodules are built and combined based on yaml configs. It supports both PyTorch 1.13 and 2.0 for training generative models.\n5. The repository provides detailed instructions for the installation of the models and any necessary dependencies. \n6. It provides a PEP 517 compliant packaging using Hatch for building a distributable wheel.\n7. Inference can be done via a streamlit demo for text-to-image and image-to-image sampling. There is also a script provided to detect invisible watermarks in the generated model output. \n8. Training is supported with example training configs provided in the repository. The repository also supports building new diffusion models.\n9. For large-scale training, it recommends using the data pipelines from their "data pipelines" project. \n\nThe repository provides comprehensive instructions and necessary scripts for the usage, training, and building of new models based on the provided generative models. It is intended to be used for research and development in generative AI and machine learning.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:49:27,251 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The "generative-models" repository by Stability AI is a collection of generative models, with a focus on text-to-image and image-to-image diffusion models. The repository provides a series of models and updates, the latest being "SDXL-Turbo" and "Stable Video Diffusion". \n\nKey features of the repository:\n\n1. "SDXL-Turbo" is a text-to-image model that is very fast. It comes with a technical report and instructions for installation and usage.\n2. "Stable Video Diffusion" is an image-to-video model intended for research. It was trained to generate 14 frames at 576x1024 resolution given a context frame of the same size. It uses the standard image encoder from SD 2.1, with a "deflickering decoder" replacing the original decoder.\n3. Previous releases include open models under a "CreativeML Open RAIL++-M" license, and diffusion models for research purposes.\n4. The codebase follows a config-driven approach, where submodules are built and combined based on yaml configs. It supports both PyTorch 1.13 and 2.0 for training generative models.\n5. The repository provides detailed instructions for the installation of the models and any necessary dependencies. \n6. It provides a PEP 517 compliant packaging using Hatch for building a distributable wheel.\n7. Inference can be done via a streamlit demo for text-to-image and image-to-image sampling. There is also a script provided to detect invisible watermarks in the generated model output. \n8. Training is supported with example training configs provided in the repository. The repository also supports building new diffusion models.\n9. For large-scale training, it recommends using the data pipelines from their "data pipelines" project. \n\nThe repository provides comprehensive instructions and necessary scripts for the usage, training, and building of new models based on the provided generative models. It is intended to be used for research and development in generative AI and machine learning.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:49:27,251 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The "generative-models" repository by Stability AI is a collection of generative models, with a focus on text-to-image and image-to-image diffusion models. The repository provides a series of models and updates, the latest being "SDXL-Turbo" and "Stable Video Diffusion". \n\nKey features of the repository:\n\n1. "SDXL-Turbo" is a text-to-image model that is very fast. It comes with a technical report and instructions for installation and usage.\n2. "Stable Video Diffusion" is an image-to-video model intended for research. It was trained to generate 14 frames at 576x1024 resolution given a context frame of the same size. It uses the standard image encoder from SD 2.1, with a "deflickering decoder" replacing the original decoder.\n3. Previous releases include open models under a "CreativeML Open RAIL++-M" license, and diffusion models for research purposes.\n4. The codebase follows a config-driven approach, where submodules are built and combined based on yaml configs. It supports both PyTorch 1.13 and 2.0 for training generative models.\n5. The repository provides detailed instructions for the installation of the models and any necessary dependencies. \n6. It provides a PEP 517 compliant packaging using Hatch for building a distributable wheel.\n7. Inference can be done via a streamlit demo for text-to-image and image-to-image sampling. There is also a script provided to detect invisible watermarks in the generated model output. \n8. Training is supported with example training configs provided in the repository. The repository also supports building new diffusion models.\n9. For large-scale training, it recommends using the data pipelines from their "data pipelines" project. \n\nThe repository provides comprehensive instructions and necessary scripts for the usage, training, and building of new models based on the provided generative models. It is intended to be used for research and development in generative AI and machine learning.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:49:27,251 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The "generative-models" repository by Stability AI is a collection of generative models, with a focus on text-to-image and image-to-image diffusion models. The repository provides a series of models and updates, the latest being "SDXL-Turbo" and "Stable Video Diffusion". \n\nKey features of the repository:\n\n1. "SDXL-Turbo" is a text-to-image model that is very fast. It comes with a technical report and instructions for installation and usage.\n2. "Stable Video Diffusion" is an image-to-video model intended for research. It was trained to generate 14 frames at 576x1024 resolution given a context frame of the same size. It uses the standard image encoder from SD 2.1, with a "deflickering decoder" replacing the original decoder.\n3. Previous releases include open models under a "CreativeML Open RAIL++-M" license, and diffusion models for research purposes.\n4. The codebase follows a config-driven approach, where submodules are built and combined based on yaml configs. It supports both PyTorch 1.13 and 2.0 for training generative models.\n5. The repository provides detailed instructions for the installation of the models and any necessary dependencies. \n6. It provides a PEP 517 compliant packaging using Hatch for building a distributable wheel.\n7. Inference can be done via a streamlit demo for text-to-image and image-to-image sampling. There is also a script provided to detect invisible watermarks in the generated model output. \n8. Training is supported with example training configs provided in the repository. The repository also supports building new diffusion models.\n9. For large-scale training, it recommends using the data pipelines from their "data pipelines" project. \n\nThe repository provides comprehensive instructions and necessary scripts for the usage, training, and building of new models based on the provided generative models. It is intended to be used for research and development in generative AI and machine learning.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:49:27,251 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The "generative-models" repository by Stability AI is a collection of generative models, with a focus on text-to-image and image-to-image diffusion models. The repository provides a series of models and updates, the latest being "SDXL-Turbo" and "Stable Video Diffusion". \n\nKey features of the repository:\n\n1. "SDXL-Turbo" is a text-to-image model that is very fast. It comes with a technical report and instructions for installation and usage.\n2. "Stable Video Diffusion" is an image-to-video model intended for research. It was trained to generate 14 frames at 576x1024 resolution given a context frame of the same size. It uses the standard image encoder from SD 2.1, with a "deflickering decoder" replacing the original decoder.\n3. Previous releases include open models under a "CreativeML Open RAIL++-M" license, and diffusion models for research purposes.\n4. The codebase follows a config-driven approach, where submodules are built and combined based on yaml configs. It supports both PyTorch 1.13 and 2.0 for training generative models.\n5. The repository provides detailed instructions for the installation of the models and any necessary dependencies. \n6. It provides a PEP 517 compliant packaging using Hatch for building a distributable wheel.\n7. Inference can be done via a streamlit demo for text-to-image and image-to-image sampling. There is also a script provided to detect invisible watermarks in the generated model output. \n8. Training is supported with example training configs provided in the repository. The repository also supports building new diffusion models.\n9. For large-scale training, it recommends using the data pipelines from their "data pipelines" project. \n\nThe repository provides comprehensive instructions and necessary scripts for the usage, training, and building of new models based on the provided generative models. It is intended to be used for research and development in generative AI and machine learning.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:49:27,251 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The "generative-models" repository by Stability AI is a collection of generative models, with a focus on text-to-image and image-to-image diffusion models. The repository provides a series of models and updates, the latest being "SDXL-Turbo" and "Stable Video Diffusion". \n\nKey features of the repository:\n\n1. "SDXL-Turbo" is a text-to-image model that is very fast. It comes with a technical report and instructions for installation and usage.\n2. "Stable Video Diffusion" is an image-to-video model intended for research. It was trained to generate 14 frames at 576x1024 resolution given a context frame of the same size. It uses the standard image encoder from SD 2.1, with a "deflickering decoder" replacing the original decoder.\n3. Previous releases include open models under a "CreativeML Open RAIL++-M" license, and diffusion models for research purposes.\n4. The codebase follows a config-driven approach, where submodules are built and combined based on yaml configs. It supports both PyTorch 1.13 and 2.0 for training generative models.\n5. The repository provides detailed instructions for the installation of the models and any necessary dependencies. \n6. It provides a PEP 517 compliant packaging using Hatch for building a distributable wheel.\n7. Inference can be done via a streamlit demo for text-to-image and image-to-image sampling. There is also a script provided to detect invisible watermarks in the generated model output. \n8. Training is supported with example training configs provided in the repository. The repository also supports building new diffusion models.\n9. For large-scale training, it recommends using the data pipelines from their "data pipelines" project. \n\nThe repository provides comprehensive instructions and necessary scripts for the usage, training, and building of new models based on the provided generative models. It is intended to be used for research and development in generative AI and machine learning.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:49:27,251 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The "generative-models" repository by Stability AI is a collection of generative models, with a focus on text-to-image and image-to-image diffusion models. The repository provides a series of models and updates, the latest being "SDXL-Turbo" and "Stable Video Diffusion". \n\nKey features of the repository:\n\n1. "SDXL-Turbo" is a text-to-image model that is very fast. It comes with a technical report and instructions for installation and usage.\n2. "Stable Video Diffusion" is an image-to-video model intended for research. It was trained to generate 14 frames at 576x1024 resolution given a context frame of the same size. It uses the standard image encoder from SD 2.1, with a "deflickering decoder" replacing the original decoder.\n3. Previous releases include open models under a "CreativeML Open RAIL++-M" license, and diffusion models for research purposes.\n4. The codebase follows a config-driven approach, where submodules are built and combined based on yaml configs. It supports both PyTorch 1.13 and 2.0 for training generative models.\n5. The repository provides detailed instructions for the installation of the models and any necessary dependencies. \n6. It provides a PEP 517 compliant packaging using Hatch for building a distributable wheel.\n7. Inference can be done via a streamlit demo for text-to-image and image-to-image sampling. There is also a script provided to detect invisible watermarks in the generated model output. \n8. Training is supported with example training configs provided in the repository. The repository also supports building new diffusion models.\n9. For large-scale training, it recommends using the data pipelines from their "data pipelines" project. \n\nThe repository provides comprehensive instructions and necessary scripts for the usage, training, and building of new models based on the provided generative models. It is intended to be used for research and development in generative AI and machine learning.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:49:27,251 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The "generative-models" repository by Stability AI is a collection of generative models, with a focus on text-to-image and image-to-image diffusion models. The repository provides a series of models and updates, the latest being "SDXL-Turbo" and "Stable Video Diffusion". \n\nKey features of the repository:\n\n1. "SDXL-Turbo" is a text-to-image model that is very fast. It comes with a technical report and instructions for installation and usage.\n2. "Stable Video Diffusion" is an image-to-video model intended for research. It was trained to generate 14 frames at 576x1024 resolution given a context frame of the same size. It uses the standard image encoder from SD 2.1, with a "deflickering decoder" replacing the original decoder.\n3. Previous releases include open models under a "CreativeML Open RAIL++-M" license, and diffusion models for research purposes.\n4. The codebase follows a config-driven approach, where submodules are built and combined based on yaml configs. It supports both PyTorch 1.13 and 2.0 for training generative models.\n5. The repository provides detailed instructions for the installation of the models and any necessary dependencies. \n6. It provides a PEP 517 compliant packaging using Hatch for building a distributable wheel.\n7. Inference can be done via a streamlit demo for text-to-image and image-to-image sampling. There is also a script provided to detect invisible watermarks in the generated model output. \n8. Training is supported with example training configs provided in the repository. The repository also supports building new diffusion models.\n9. For large-scale training, it recommends using the data pipelines from their "data pipelines" project. \n\nThe repository provides comprehensive instructions and necessary scripts for the usage, training, and building of new models based on the provided generative models. It is intended to be used for research and development in generative AI and machine learning.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:49:27,251 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The "generative-models" repository by Stability AI is a collection of generative models, with a focus on text-to-image and image-to-image diffusion models. The repository provides a series of models and updates, the latest being "SDXL-Turbo" and "Stable Video Diffusion". \n\nKey features of the repository:\n\n1. "SDXL-Turbo" is a text-to-image model that is very fast. It comes with a technical report and instructions for installation and usage.\n2. "Stable Video Diffusion" is an image-to-video model intended for research. It was trained to generate 14 frames at 576x1024 resolution given a context frame of the same size. It uses the standard image encoder from SD 2.1, with a "deflickering decoder" replacing the original decoder.\n3. Previous releases include open models under a "CreativeML Open RAIL++-M" license, and diffusion models for research purposes.\n4. The codebase follows a config-driven approach, where submodules are built and combined based on yaml configs. It supports both PyTorch 1.13 and 2.0 for training generative models.\n5. The repository provides detailed instructions for the installation of the models and any necessary dependencies. \n6. It provides a PEP 517 compliant packaging using Hatch for building a distributable wheel.\n7. Inference can be done via a streamlit demo for text-to-image and image-to-image sampling. There is also a script provided to detect invisible watermarks in the generated model output. \n8. Training is supported with example training configs provided in the repository. The repository also supports building new diffusion models.\n9. For large-scale training, it recommends using the data pipelines from their "data pipelines" project. \n\nThe repository provides comprehensive instructions and necessary scripts for the usage, training, and building of new models based on the provided generative models. It is intended to be used for research and development in generative AI and machine learning.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:49:27,251 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The "generative-models" repository by Stability AI is a collection of generative models, with a focus on text-to-image and image-to-image diffusion models. The repository provides a series of models and updates, the latest being "SDXL-Turbo" and "Stable Video Diffusion". \n\nKey features of the repository:\n\n1. "SDXL-Turbo" is a text-to-image model that is very fast. It comes with a technical report and instructions for installation and usage.\n2. "Stable Video Diffusion" is an image-to-video model intended for research. It was trained to generate 14 frames at 576x1024 resolution given a context frame of the same size. It uses the standard image encoder from SD 2.1, with a "deflickering decoder" replacing the original decoder.\n3. Previous releases include open models under a "CreativeML Open RAIL++-M" license, and diffusion models for research purposes.\n4. The codebase follows a config-driven approach, where submodules are built and combined based on yaml configs. It supports both PyTorch 1.13 and 2.0 for training generative models.\n5. The repository provides detailed instructions for the installation of the models and any necessary dependencies. \n6. It provides a PEP 517 compliant packaging using Hatch for building a distributable wheel.\n7. Inference can be done via a streamlit demo for text-to-image and image-to-image sampling. There is also a script provided to detect invisible watermarks in the generated model output. \n8. Training is supported with example training configs provided in the repository. The repository also supports building new diffusion models.\n9. For large-scale training, it recommends using the data pipelines from their "data pipelines" project. \n\nThe repository provides comprehensive instructions and necessary scripts for the usage, training, and building of new models based on the provided generative models. It is intended to be used for research and development in generative AI and machine learning.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 20:49:27,251 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': 'The "generative-models" repository by Stability AI is a collection of generative models, with a focus on text-to-image and image-to-image diffusion models. The repository provides a series of models and updates, the latest being "SDXL-Turbo" and "Stable Video Diffusion". \n\nKey features of the repository:\n\n1. "SDXL-Turbo" is a text-to-image model that is very fast. It comes with a technical report and instructions for installation and usage.\n2. "Stable Video Diffusion" is an image-to-video model intended for research. It was trained to generate 14 frames at 576x1024 resolution given a context frame of the same size. It uses the standard image encoder from SD 2.1, with a "deflickering decoder" replacing the original decoder.\n3. Previous releases include open models under a "CreativeML Open RAIL++-M" license, and diffusion models for research purposes.\n4. The codebase follows a config-driven approach, where submodules are built and combined based on yaml configs. It supports both PyTorch 1.13 and 2.0 for training generative models.\n5. The repository provides detailed instructions for the installation of the models and any necessary dependencies. \n6. It provides a PEP 517 compliant packaging using Hatch for building a distributable wheel.\n7. Inference can be done via a streamlit demo for text-to-image and image-to-image sampling. There is also a script provided to detect invisible watermarks in the generated model output. \n8. Training is supported with example training configs provided in the repository. The repository also supports building new diffusion models.\n9. For large-scale training, it recommends using the data pipelines from their "data pipelines" project. \n\nThe repository provides comprehensive instructions and necessary scripts for the usage, training, and building of new models based on the provided generative models. It is intended to be used for research and development in generative AI and machine learning.', 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702172657644256400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'chatbot', 'params': {'user_question': 'Hi bob', 'file_structure': ''}, 'options': None, 'kwargs': {}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'chatbot', 'params': {'user_question': 'Hi bob', 'file_structure': ''}, 'options': None, 'kwargs': {}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'chatbot', 'params': {'user_question': 'Hi bob', 'file_structure': ''}, 'options': None, 'kwargs': {}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'chatbot', 'params': {'user_question': 'Hi bob', 'file_structure': ''}, 'options': None, 'kwargs': {}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'chatbot', 'params': {'user_question': 'Hi bob', 'file_structure': ''}, 'options': None, 'kwargs': {}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'chatbot', 'params': {'user_question': 'Hi bob', 'file_structure': ''}, 'options': None, 'kwargs': {}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'options': None, 'parameters': {'user_question': 'Hi bob', 'file_structure': ''}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'options': None, 'parameters': {'user_question': 'Hi bob', 'file_structure': ''}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'options': None, 'parameters': {'user_question': 'Hi bob', 'file_structure': ''}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'options': None, 'parameters': {'user_question': 'Hi bob', 'file_structure': ''}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'options': None, 'parameters': {'user_question': 'Hi bob', 'file_structure': ''}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'options': None, 'parameters': {'user_question': 'Hi bob', 'file_structure': ''}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'params': {'user_question': 'Hi bob', 'file_structure': ''}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'params': {'user_question': 'Hi bob', 'file_structure': ''}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'params': {'user_question': 'Hi bob', 'file_structure': ''}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'params': {'user_question': 'Hi bob', 'file_structure': ''}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'params': {'user_question': 'Hi bob', 'file_structure': ''}} ts_ns=1702173609569775400
2023-12-09 21:02:38,034 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'params': {'user_question': 'Hi bob', 'file_structure': ''}} ts_ns=1702173609569775400
2023-12-09 21:02:38,051 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'presence_penalty': 0, 'temperature': 0.75, 'model': 'gpt-4', 'top_p': 1, 'frequency_penalty': 0, 'messages': [{'content': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is . The READMEfile is:\n', 'role': 'user'}, {'content': 'README Summary: \n\nFile Structure:\n\n\nUser Question: Hi bob', 'role': 'user'}]}} ts_ns=1702173609569775400
2023-12-09 21:02:38,051 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'presence_penalty': 0, 'temperature': 0.75, 'model': 'gpt-4', 'top_p': 1, 'frequency_penalty': 0, 'messages': [{'content': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is . The READMEfile is:\n', 'role': 'user'}, {'content': 'README Summary: \n\nFile Structure:\n\n\nUser Question: Hi bob', 'role': 'user'}]}} ts_ns=1702173609569775400
2023-12-09 21:02:38,051 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'presence_penalty': 0, 'temperature': 0.75, 'model': 'gpt-4', 'top_p': 1, 'frequency_penalty': 0, 'messages': [{'content': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is . The READMEfile is:\n', 'role': 'user'}, {'content': 'README Summary: \n\nFile Structure:\n\n\nUser Question: Hi bob', 'role': 'user'}]}} ts_ns=1702173609569775400
2023-12-09 21:02:38,051 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'presence_penalty': 0, 'temperature': 0.75, 'model': 'gpt-4', 'top_p': 1, 'frequency_penalty': 0, 'messages': [{'content': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is . The READMEfile is:\n', 'role': 'user'}, {'content': 'README Summary: \n\nFile Structure:\n\n\nUser Question: Hi bob', 'role': 'user'}]}} ts_ns=1702173609569775400
2023-12-09 21:02:38,051 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'presence_penalty': 0, 'temperature': 0.75, 'model': 'gpt-4', 'top_p': 1, 'frequency_penalty': 0, 'messages': [{'content': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is . The READMEfile is:\n', 'role': 'user'}, {'content': 'README Summary: \n\nFile Structure:\n\n\nUser Question: Hi bob', 'role': 'user'}]}} ts_ns=1702173609569775400
2023-12-09 21:02:38,051 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'presence_penalty': 0, 'temperature': 0.75, 'model': 'gpt-4', 'top_p': 1, 'frequency_penalty': 0, 'messages': [{'content': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is . The READMEfile is:\n', 'role': 'user'}, {'content': 'README Summary: \n\nFile Structure:\n\n\nUser Question: Hi bob', 'role': 'user'}]}} ts_ns=1702173609569775400
2023-12-09 21:02:40,247 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but the information provided is incomplete. To give a summary of the repository, I need the content of the README file and the file structure of the repository. Could you please provide those?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173609569775400
2023-12-09 21:02:40,247 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but the information provided is incomplete. To give a summary of the repository, I need the content of the README file and the file structure of the repository. Could you please provide those?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173609569775400
2023-12-09 21:02:40,247 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but the information provided is incomplete. To give a summary of the repository, I need the content of the README file and the file structure of the repository. Could you please provide those?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173609569775400
2023-12-09 21:02:40,247 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but the information provided is incomplete. To give a summary of the repository, I need the content of the README file and the file structure of the repository. Could you please provide those?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173609569775400
2023-12-09 21:02:40,247 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but the information provided is incomplete. To give a summary of the repository, I need the content of the README file and the file structure of the repository. Could you please provide those?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173609569775400
2023-12-09 21:02:40,247 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but the information provided is incomplete. To give a summary of the repository, I need the content of the README file and the file structure of the repository. Could you please provide those?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173609569775400
2023-12-09 21:02:40,247 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but the information provided is incomplete. To give a summary of the repository, I need the content of the README file and the file structure of the repository. Could you please provide those?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173609569775400
2023-12-09 21:02:40,247 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but the information provided is incomplete. To give a summary of the repository, I need the content of the README file and the file structure of the repository. Could you please provide those?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173609569775400
2023-12-09 21:02:40,247 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but the information provided is incomplete. To give a summary of the repository, I need the content of the README file and the file structure of the repository. Could you please provide those?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173609569775400
2023-12-09 21:02:40,247 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but the information provided is incomplete. To give a summary of the repository, I need the content of the README file and the file structure of the repository. Could you please provide those?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173609569775400
2023-12-09 21:02:40,247 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but the information provided is incomplete. To give a summary of the repository, I need the content of the README file and the file structure of the repository. Could you please provide those?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173609569775400
2023-12-09 21:02:40,247 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but the information provided is incomplete. To give a summary of the repository, I need the content of the README file and the file structure of the repository. Could you please provide those?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173609569775400
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'summarize_readme', 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}, 'options': None, 'kwargs': {}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'options': None, 'parameters': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702173808368226500
2023-12-09 21:03:48,632 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='summarize_readme', input='Please summarize this repository. The repository name is {{repo_name}}. The READMEfile is:\n{{readme_file}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. '}), tags=None, parameters={'repo_name': '', 'readme_file': ''}, remember_chat_context=False), outputs=[]), 'params': {'readme_file': '# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the ["denoiser framework"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name="installation"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install "numpy>=1.17" "PyWavelets>=1.1.1" "opencv-python>=4.1.0.25"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don\'t forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {"jpg": x,  # this is a tensor -1...1 chw\n           "txt": "a beautiful image"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n', 'repo_name': 'generative-models'}} ts_ns=1702173808368226500
2023-12-09 21:03:48,645 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'frequency_penalty': 0, 'model': 'gpt-4', 'top_p': 1, 'presence_penalty': 0, 'temperature': 0.75, 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702173808368226500
2023-12-09 21:03:48,645 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'frequency_penalty': 0, 'model': 'gpt-4', 'top_p': 1, 'presence_penalty': 0, 'temperature': 0.75, 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702173808368226500
2023-12-09 21:03:48,645 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'frequency_penalty': 0, 'model': 'gpt-4', 'top_p': 1, 'presence_penalty': 0, 'temperature': 0.75, 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702173808368226500
2023-12-09 21:03:48,645 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'frequency_penalty': 0, 'model': 'gpt-4', 'top_p': 1, 'presence_penalty': 0, 'temperature': 0.75, 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702173808368226500
2023-12-09 21:03:48,645 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'frequency_penalty': 0, 'model': 'gpt-4', 'top_p': 1, 'presence_penalty': 0, 'temperature': 0.75, 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702173808368226500
2023-12-09 21:03:48,645 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'frequency_penalty': 0, 'model': 'gpt-4', 'top_p': 1, 'presence_penalty': 0, 'temperature': 0.75, 'messages': [{'content': 'You are an expert on open-source software and GitHub.  You will be provided with the README file for a GitHub repository that you will summarize. It is important to maintain as much information as possible about the README file in your summary. This summary will be used in future prompts to answer specific questions related to the repository and its purpose. ', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is generative-models. The READMEfile is:\n# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with &#x60;pip install streamlit-keyup&#x60;.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the &#x60;checkpoints/&#x60; directory.\n        - Run &#x60;streamlit run scripts/demo/turbo.py&#x60;.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware &#x60;deflickering decoder&#x60;.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as &#x60;SVD&#x60; but finetuned\n      for 25 frame generation.\n    - We provide a streamlit demo &#x60;scripts/demo/video_sampling.py&#x60; and a standalone python script &#x60;scripts/sampling/simple_video_sample.py&#x60; for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over &#x60;SDXL-base-0.9&#x60;.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over &#x60;SDXL-refiner-0.9&#x60;.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - &#x60;SDXL-base-0.9&#x60;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - &#x60;SDXL-refiner-0.9&#x60;: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling &#x60;instantiate_from_config()&#x60; on objects defined in yaml configs. See &#x60;configs/&#x60; for many examples.\n\n### Changelog from the old &#x60;ldm&#x60; codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly &#x60;LatentDiffusion&#x60;,\nnow &#x60;DiffusionEngine&#x60;) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: &#x60;GeneralConditioner&#x60;,\n  see &#x60;sgm/modules/encoders/modules.py&#x60;.\n- We separate guiders (such as classifier-free guidance, see &#x60;sgm/modules/diffusionmodules/guiders.py&#x60;) from the\n  samplers (&#x60;sgm/modules/diffusionmodules/sampling.py&#x60;), and the samplers are independent of the model.\n- We adopt the [&quot;denoiser framework&quot;](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see &#x60;sgm/modules/diffusionmodules/denoiser.py&#x60;.\n    * The following features are now independent: weighting of the diffusion loss\n      function (&#x60;sgm/modules/diffusionmodules/denoiser_weighting.py&#x60;), preconditioning of the\n      network (&#x60;sgm/modules/diffusionmodules/denoiser_scaling.py&#x60;), and sampling of noise levels during\n      training (&#x60;sgm/modules/diffusionmodules/sigma_sampling.py&#x60;).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;\n\n#### 1. Clone the repo\n\n&#x60;&#x60;&#x60;shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n&#x60;&#x60;&#x60;\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the &#x60;generative-models&#x60; root after cloning it.\n\n**NOTE:** This is tested under &#x60;python3.10&#x60;. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n&#x60;&#x60;&#x60;shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n&#x60;&#x60;&#x60;\n\n#### 3. Install &#x60;sgm&#x60;\n\n&#x60;&#x60;&#x60;shell\npip3 install .\n&#x60;&#x60;&#x60;\n\n#### 4. Install &#x60;sdata&#x60; for training\n\n&#x60;&#x60;&#x60;shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n&#x60;&#x60;&#x60;\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install &#x60;hatch&#x60; and run &#x60;hatch build&#x60;\n(specifying &#x60;-t wheel&#x60; will skip building a sdist, which is not necessary).\n\n&#x60;&#x60;&#x60;\npip install hatch\nhatch build -t wheel\n&#x60;&#x60;&#x60;\n\nYou will find the built package in &#x60;dist/&#x60;. You can install the wheel with &#x60;pip install dist/*.whl&#x60;.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin &#x60;scripts/demo/sampling.py&#x60;.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  &#x60;&#x60;&#x60;\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  &#x60;&#x60;&#x60;\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  &#x60;&#x60;&#x60;\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [&#x60;CreativeML Open RAIL++-M&#x60; license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into &#x60;checkpoints/&#x60;.\nNext, start the demo using\n\n&#x60;&#x60;&#x60;\nstreamlit run scripts/demo/sampling.py --server.port &lt;your_port&gt;\n&#x60;&#x60;&#x60;\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n&#x60;&#x60;&#x60;bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install &quot;numpy&gt;=1.17&quot; &quot;PyWavelets&gt;=1.1.1&quot; &quot;opencv-python&gt;=4.1.0.25&quot;\npip install --no-deps invisible-watermark\n&#x60;&#x60;&#x60;\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don&#x27;t forget to activate your\nvirtual environment beforehand, e.g. &#x60;source .pt1/bin/activate&#x60;):\n\n&#x60;&#x60;&#x60;bash\n# test a single file\npython scripts/demo/detect.py &lt;your filename here&gt;\n# test multiple files at once\npython scripts/demo/detect.py &lt;filename 1&gt; &lt;filename 2&gt; ... &lt;filename n&gt;\n# test all files in a specific folder\npython scripts/demo/detect.py &lt;your folder name here&gt;/*\n&#x60;&#x60;&#x60;\n\n## Training:\n\nWe are providing example training configs in &#x60;configs/example_training&#x60;. To launch a training, run\n\n&#x60;&#x60;&#x60;\npython main.py --base configs/&lt;config1.yaml&gt; configs/&lt;config2.yaml&gt;\n&#x60;&#x60;&#x60;\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n&#x60;&#x60;&#x60;bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n&#x60;&#x60;&#x60;\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;, &#x60;configs/example_training/txt2img-clipl.yaml&#x60;\nand &#x60;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&#x60; for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing &#x60;USER:&#x60; in the respective config.\n\n**NOTE 2:** This repository supports both &#x60;pytorch1.13&#x60; and &#x60;pytorch2&#x60;for training generative models. However for\nautoencoder training as e.g. in &#x60;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&#x60;,\nonly &#x60;pytorch1.13&#x60; is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in &#x60;configs/example_training/imagenet-f8_cond.yaml&#x60;) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe &#x60;CKPT_PATH&#x60; placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe &#x60;GeneralConditioner&#x60; is configured through the &#x60;conditioner_config&#x60;. Its only attribute is &#x60;emb_models&#x60;, a list of\ndifferent embedders (all inherited from &#x60;AbstractEmbModel&#x60;) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (&#x60;is_trainable&#x60;, default &#x60;False&#x60;), a classifier-free\nguidance dropout rate is used (&#x60;ucg_rate&#x60;, default &#x60;0&#x60;), and an input key (&#x60;input_key&#x60;), for example, &#x60;txt&#x60; for\ntext-conditioning or &#x60;cls&#x60; for class-conditioning.\nWhen computing conditionings, the embedder will get &#x60;batch[input_key]&#x60; as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the &#x60;conditioner_config&#x60; is important.\n\n#### Network\n\nThe neural network is set through the &#x60;network_config&#x60;. This used to be called &#x60;unet_config&#x60;, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through &#x60;loss_config&#x60;. For standard diffusion model training, you will have to\nset &#x60;sigma_sampler_config&#x60;.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the &#x60;sampler_config&#x60;, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n&#x60;&#x60;&#x60;python\nexample = {&quot;jpg&quot;: x,  # this is a tensor -1...1 chw\n           &quot;txt&quot;: &quot;a beautiful image&quot;}\n&#x60;&#x60;&#x60;\n\nwhere we expect images in -1...1, channel-first format.\n', 'role': 'user'}]}} ts_ns=1702173808368226500
2023-12-09 21:04:05,339 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The generative-models repository by Stability AI contains research and development work focusing on generative models, specifically in the area of text-to-image and image-to-image model generation. The repository includes various model releases, such as SDXL-Turbo, Stable Video Diffusion, SDXL-base-1.0, SDXL-refiner-1.0, SDXL-base-0.9, and SDXL-refiner-0.9. \n\nThe SDXL-Turbo and Stable Video Diffusion models are the latest releases, with SDXL-Turbo providing a lightning-fast text-to-image model, and Stable Video Diffusion providing an image-to-video model for research purposes. The SDXL models are open-source and come with a CreativeML Open RAIL++-M license. \n\nThe repository adopts a config-driven approach for the construction and combination of submodules, using the instantiate_from_config() function on objects defined in yaml configs. The core diffusion model class is referred to as DiffusionEngine.\n\nThe repository also provides detailed installation instructions for setting up the environment, including steps for cloning the repository, setting up virtualenv, installing sgm, and installing sdata for training. After installation, sample training configurations can be found under the 'configs/example_training' directory.\n\nThe project uses PEP 517 compliant packaging using Hatch for building a distributable wheel. The repository also includes an Inference section, providing details on how to run the demo for text-to-image and image-to-image sampling. \n\nFor watermark detection, the repository includes a script that detects an invisible watermark embedded into the model output. \n\nLastly, the repository contains a training section, detailing how to launch training and build new diffusion models. It also provides a guide on dataset handling for large scale training.", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:05,339 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The generative-models repository by Stability AI contains research and development work focusing on generative models, specifically in the area of text-to-image and image-to-image model generation. The repository includes various model releases, such as SDXL-Turbo, Stable Video Diffusion, SDXL-base-1.0, SDXL-refiner-1.0, SDXL-base-0.9, and SDXL-refiner-0.9. \n\nThe SDXL-Turbo and Stable Video Diffusion models are the latest releases, with SDXL-Turbo providing a lightning-fast text-to-image model, and Stable Video Diffusion providing an image-to-video model for research purposes. The SDXL models are open-source and come with a CreativeML Open RAIL++-M license. \n\nThe repository adopts a config-driven approach for the construction and combination of submodules, using the instantiate_from_config() function on objects defined in yaml configs. The core diffusion model class is referred to as DiffusionEngine.\n\nThe repository also provides detailed installation instructions for setting up the environment, including steps for cloning the repository, setting up virtualenv, installing sgm, and installing sdata for training. After installation, sample training configurations can be found under the 'configs/example_training' directory.\n\nThe project uses PEP 517 compliant packaging using Hatch for building a distributable wheel. The repository also includes an Inference section, providing details on how to run the demo for text-to-image and image-to-image sampling. \n\nFor watermark detection, the repository includes a script that detects an invisible watermark embedded into the model output. \n\nLastly, the repository contains a training section, detailing how to launch training and build new diffusion models. It also provides a guide on dataset handling for large scale training.", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:05,339 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The generative-models repository by Stability AI contains research and development work focusing on generative models, specifically in the area of text-to-image and image-to-image model generation. The repository includes various model releases, such as SDXL-Turbo, Stable Video Diffusion, SDXL-base-1.0, SDXL-refiner-1.0, SDXL-base-0.9, and SDXL-refiner-0.9. \n\nThe SDXL-Turbo and Stable Video Diffusion models are the latest releases, with SDXL-Turbo providing a lightning-fast text-to-image model, and Stable Video Diffusion providing an image-to-video model for research purposes. The SDXL models are open-source and come with a CreativeML Open RAIL++-M license. \n\nThe repository adopts a config-driven approach for the construction and combination of submodules, using the instantiate_from_config() function on objects defined in yaml configs. The core diffusion model class is referred to as DiffusionEngine.\n\nThe repository also provides detailed installation instructions for setting up the environment, including steps for cloning the repository, setting up virtualenv, installing sgm, and installing sdata for training. After installation, sample training configurations can be found under the 'configs/example_training' directory.\n\nThe project uses PEP 517 compliant packaging using Hatch for building a distributable wheel. The repository also includes an Inference section, providing details on how to run the demo for text-to-image and image-to-image sampling. \n\nFor watermark detection, the repository includes a script that detects an invisible watermark embedded into the model output. \n\nLastly, the repository contains a training section, detailing how to launch training and build new diffusion models. It also provides a guide on dataset handling for large scale training.", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:05,339 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The generative-models repository by Stability AI contains research and development work focusing on generative models, specifically in the area of text-to-image and image-to-image model generation. The repository includes various model releases, such as SDXL-Turbo, Stable Video Diffusion, SDXL-base-1.0, SDXL-refiner-1.0, SDXL-base-0.9, and SDXL-refiner-0.9. \n\nThe SDXL-Turbo and Stable Video Diffusion models are the latest releases, with SDXL-Turbo providing a lightning-fast text-to-image model, and Stable Video Diffusion providing an image-to-video model for research purposes. The SDXL models are open-source and come with a CreativeML Open RAIL++-M license. \n\nThe repository adopts a config-driven approach for the construction and combination of submodules, using the instantiate_from_config() function on objects defined in yaml configs. The core diffusion model class is referred to as DiffusionEngine.\n\nThe repository also provides detailed installation instructions for setting up the environment, including steps for cloning the repository, setting up virtualenv, installing sgm, and installing sdata for training. After installation, sample training configurations can be found under the 'configs/example_training' directory.\n\nThe project uses PEP 517 compliant packaging using Hatch for building a distributable wheel. The repository also includes an Inference section, providing details on how to run the demo for text-to-image and image-to-image sampling. \n\nFor watermark detection, the repository includes a script that detects an invisible watermark embedded into the model output. \n\nLastly, the repository contains a training section, detailing how to launch training and build new diffusion models. It also provides a guide on dataset handling for large scale training.", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:05,339 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The generative-models repository by Stability AI contains research and development work focusing on generative models, specifically in the area of text-to-image and image-to-image model generation. The repository includes various model releases, such as SDXL-Turbo, Stable Video Diffusion, SDXL-base-1.0, SDXL-refiner-1.0, SDXL-base-0.9, and SDXL-refiner-0.9. \n\nThe SDXL-Turbo and Stable Video Diffusion models are the latest releases, with SDXL-Turbo providing a lightning-fast text-to-image model, and Stable Video Diffusion providing an image-to-video model for research purposes. The SDXL models are open-source and come with a CreativeML Open RAIL++-M license. \n\nThe repository adopts a config-driven approach for the construction and combination of submodules, using the instantiate_from_config() function on objects defined in yaml configs. The core diffusion model class is referred to as DiffusionEngine.\n\nThe repository also provides detailed installation instructions for setting up the environment, including steps for cloning the repository, setting up virtualenv, installing sgm, and installing sdata for training. After installation, sample training configurations can be found under the 'configs/example_training' directory.\n\nThe project uses PEP 517 compliant packaging using Hatch for building a distributable wheel. The repository also includes an Inference section, providing details on how to run the demo for text-to-image and image-to-image sampling. \n\nFor watermark detection, the repository includes a script that detects an invisible watermark embedded into the model output. \n\nLastly, the repository contains a training section, detailing how to launch training and build new diffusion models. It also provides a guide on dataset handling for large scale training.", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:05,339 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The generative-models repository by Stability AI contains research and development work focusing on generative models, specifically in the area of text-to-image and image-to-image model generation. The repository includes various model releases, such as SDXL-Turbo, Stable Video Diffusion, SDXL-base-1.0, SDXL-refiner-1.0, SDXL-base-0.9, and SDXL-refiner-0.9. \n\nThe SDXL-Turbo and Stable Video Diffusion models are the latest releases, with SDXL-Turbo providing a lightning-fast text-to-image model, and Stable Video Diffusion providing an image-to-video model for research purposes. The SDXL models are open-source and come with a CreativeML Open RAIL++-M license. \n\nThe repository adopts a config-driven approach for the construction and combination of submodules, using the instantiate_from_config() function on objects defined in yaml configs. The core diffusion model class is referred to as DiffusionEngine.\n\nThe repository also provides detailed installation instructions for setting up the environment, including steps for cloning the repository, setting up virtualenv, installing sgm, and installing sdata for training. After installation, sample training configurations can be found under the 'configs/example_training' directory.\n\nThe project uses PEP 517 compliant packaging using Hatch for building a distributable wheel. The repository also includes an Inference section, providing details on how to run the demo for text-to-image and image-to-image sampling. \n\nFor watermark detection, the repository includes a script that detects an invisible watermark embedded into the model output. \n\nLastly, the repository contains a training section, detailing how to launch training and build new diffusion models. It also provides a guide on dataset handling for large scale training.", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:05,339 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The generative-models repository by Stability AI contains research and development work focusing on generative models, specifically in the area of text-to-image and image-to-image model generation. The repository includes various model releases, such as SDXL-Turbo, Stable Video Diffusion, SDXL-base-1.0, SDXL-refiner-1.0, SDXL-base-0.9, and SDXL-refiner-0.9. \n\nThe SDXL-Turbo and Stable Video Diffusion models are the latest releases, with SDXL-Turbo providing a lightning-fast text-to-image model, and Stable Video Diffusion providing an image-to-video model for research purposes. The SDXL models are open-source and come with a CreativeML Open RAIL++-M license. \n\nThe repository adopts a config-driven approach for the construction and combination of submodules, using the instantiate_from_config() function on objects defined in yaml configs. The core diffusion model class is referred to as DiffusionEngine.\n\nThe repository also provides detailed installation instructions for setting up the environment, including steps for cloning the repository, setting up virtualenv, installing sgm, and installing sdata for training. After installation, sample training configurations can be found under the 'configs/example_training' directory.\n\nThe project uses PEP 517 compliant packaging using Hatch for building a distributable wheel. The repository also includes an Inference section, providing details on how to run the demo for text-to-image and image-to-image sampling. \n\nFor watermark detection, the repository includes a script that detects an invisible watermark embedded into the model output. \n\nLastly, the repository contains a training section, detailing how to launch training and build new diffusion models. It also provides a guide on dataset handling for large scale training.", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:05,339 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The generative-models repository by Stability AI contains research and development work focusing on generative models, specifically in the area of text-to-image and image-to-image model generation. The repository includes various model releases, such as SDXL-Turbo, Stable Video Diffusion, SDXL-base-1.0, SDXL-refiner-1.0, SDXL-base-0.9, and SDXL-refiner-0.9. \n\nThe SDXL-Turbo and Stable Video Diffusion models are the latest releases, with SDXL-Turbo providing a lightning-fast text-to-image model, and Stable Video Diffusion providing an image-to-video model for research purposes. The SDXL models are open-source and come with a CreativeML Open RAIL++-M license. \n\nThe repository adopts a config-driven approach for the construction and combination of submodules, using the instantiate_from_config() function on objects defined in yaml configs. The core diffusion model class is referred to as DiffusionEngine.\n\nThe repository also provides detailed installation instructions for setting up the environment, including steps for cloning the repository, setting up virtualenv, installing sgm, and installing sdata for training. After installation, sample training configurations can be found under the 'configs/example_training' directory.\n\nThe project uses PEP 517 compliant packaging using Hatch for building a distributable wheel. The repository also includes an Inference section, providing details on how to run the demo for text-to-image and image-to-image sampling. \n\nFor watermark detection, the repository includes a script that detects an invisible watermark embedded into the model output. \n\nLastly, the repository contains a training section, detailing how to launch training and build new diffusion models. It also provides a guide on dataset handling for large scale training.", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:05,339 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The generative-models repository by Stability AI contains research and development work focusing on generative models, specifically in the area of text-to-image and image-to-image model generation. The repository includes various model releases, such as SDXL-Turbo, Stable Video Diffusion, SDXL-base-1.0, SDXL-refiner-1.0, SDXL-base-0.9, and SDXL-refiner-0.9. \n\nThe SDXL-Turbo and Stable Video Diffusion models are the latest releases, with SDXL-Turbo providing a lightning-fast text-to-image model, and Stable Video Diffusion providing an image-to-video model for research purposes. The SDXL models are open-source and come with a CreativeML Open RAIL++-M license. \n\nThe repository adopts a config-driven approach for the construction and combination of submodules, using the instantiate_from_config() function on objects defined in yaml configs. The core diffusion model class is referred to as DiffusionEngine.\n\nThe repository also provides detailed installation instructions for setting up the environment, including steps for cloning the repository, setting up virtualenv, installing sgm, and installing sdata for training. After installation, sample training configurations can be found under the 'configs/example_training' directory.\n\nThe project uses PEP 517 compliant packaging using Hatch for building a distributable wheel. The repository also includes an Inference section, providing details on how to run the demo for text-to-image and image-to-image sampling. \n\nFor watermark detection, the repository includes a script that detects an invisible watermark embedded into the model output. \n\nLastly, the repository contains a training section, detailing how to launch training and build new diffusion models. It also provides a guide on dataset handling for large scale training.", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:05,339 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The generative-models repository by Stability AI contains research and development work focusing on generative models, specifically in the area of text-to-image and image-to-image model generation. The repository includes various model releases, such as SDXL-Turbo, Stable Video Diffusion, SDXL-base-1.0, SDXL-refiner-1.0, SDXL-base-0.9, and SDXL-refiner-0.9. \n\nThe SDXL-Turbo and Stable Video Diffusion models are the latest releases, with SDXL-Turbo providing a lightning-fast text-to-image model, and Stable Video Diffusion providing an image-to-video model for research purposes. The SDXL models are open-source and come with a CreativeML Open RAIL++-M license. \n\nThe repository adopts a config-driven approach for the construction and combination of submodules, using the instantiate_from_config() function on objects defined in yaml configs. The core diffusion model class is referred to as DiffusionEngine.\n\nThe repository also provides detailed installation instructions for setting up the environment, including steps for cloning the repository, setting up virtualenv, installing sgm, and installing sdata for training. After installation, sample training configurations can be found under the 'configs/example_training' directory.\n\nThe project uses PEP 517 compliant packaging using Hatch for building a distributable wheel. The repository also includes an Inference section, providing details on how to run the demo for text-to-image and image-to-image sampling. \n\nFor watermark detection, the repository includes a script that detects an invisible watermark embedded into the model output. \n\nLastly, the repository contains a training section, detailing how to launch training and build new diffusion models. It also provides a guide on dataset handling for large scale training.", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:05,339 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The generative-models repository by Stability AI contains research and development work focusing on generative models, specifically in the area of text-to-image and image-to-image model generation. The repository includes various model releases, such as SDXL-Turbo, Stable Video Diffusion, SDXL-base-1.0, SDXL-refiner-1.0, SDXL-base-0.9, and SDXL-refiner-0.9. \n\nThe SDXL-Turbo and Stable Video Diffusion models are the latest releases, with SDXL-Turbo providing a lightning-fast text-to-image model, and Stable Video Diffusion providing an image-to-video model for research purposes. The SDXL models are open-source and come with a CreativeML Open RAIL++-M license. \n\nThe repository adopts a config-driven approach for the construction and combination of submodules, using the instantiate_from_config() function on objects defined in yaml configs. The core diffusion model class is referred to as DiffusionEngine.\n\nThe repository also provides detailed installation instructions for setting up the environment, including steps for cloning the repository, setting up virtualenv, installing sgm, and installing sdata for training. After installation, sample training configurations can be found under the 'configs/example_training' directory.\n\nThe project uses PEP 517 compliant packaging using Hatch for building a distributable wheel. The repository also includes an Inference section, providing details on how to run the demo for text-to-image and image-to-image sampling. \n\nFor watermark detection, the repository includes a script that detects an invisible watermark embedded into the model output. \n\nLastly, the repository contains a training section, detailing how to launch training and build new diffusion models. It also provides a guide on dataset handling for large scale training.", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:05,339 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "The generative-models repository by Stability AI contains research and development work focusing on generative models, specifically in the area of text-to-image and image-to-image model generation. The repository includes various model releases, such as SDXL-Turbo, Stable Video Diffusion, SDXL-base-1.0, SDXL-refiner-1.0, SDXL-base-0.9, and SDXL-refiner-0.9. \n\nThe SDXL-Turbo and Stable Video Diffusion models are the latest releases, with SDXL-Turbo providing a lightning-fast text-to-image model, and Stable Video Diffusion providing an image-to-video model for research purposes. The SDXL models are open-source and come with a CreativeML Open RAIL++-M license. \n\nThe repository adopts a config-driven approach for the construction and combination of submodules, using the instantiate_from_config() function on objects defined in yaml configs. The core diffusion model class is referred to as DiffusionEngine.\n\nThe repository also provides detailed installation instructions for setting up the environment, including steps for cloning the repository, setting up virtualenv, installing sgm, and installing sdata for training. After installation, sample training configurations can be found under the 'configs/example_training' directory.\n\nThe project uses PEP 517 compliant packaging using Hatch for building a distributable wheel. The repository also includes an Inference section, providing details on how to run the demo for text-to-image and image-to-image sampling. \n\nFor watermark detection, the repository includes a script that detects an invisible watermark embedded into the model output. \n\nLastly, the repository contains a training section, detailing how to launch training and build new diffusion models. It also provides a guide on dataset handling for large scale training.", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'chatbot', 'params': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}, 'options': None, 'kwargs': {}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'chatbot', 'params': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}, 'options': None, 'kwargs': {}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'chatbot', 'params': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}, 'options': None, 'kwargs': {}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'chatbot', 'params': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}, 'options': None, 'kwargs': {}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'chatbot', 'params': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}, 'options': None, 'kwargs': {}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'chatbot', 'params': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}, 'options': None, 'kwargs': {}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.Config' data={'prompt_name': 'chatbot', 'params': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}, 'options': None, 'kwargs': {}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'options': None, 'parameters': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'options': None, 'parameters': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'options': None, 'parameters': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'options': None, 'parameters': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'options': None, 'parameters': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'options': None, 'parameters': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_run_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'options': None, 'parameters': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'params': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'params': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'params': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'params': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'params': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'params': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}} ts_ns=1702173808368226500
2023-12-09 21:04:26,886 - my-logger - INFO - Callback called. event
: name='on_deserialize_start' file='aiconfig.default_parsers.openai' data={'prompt': Prompt(name='chatbot', input='README Summary: {{summarize_readme.output}}\n\nFile Structure:\n{{file_structure}}\n\nUser Question: {{user_question}}', metadata=PromptMetadata(model=ModelMetadata(name='gpt-4', settings={'system_prompt': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.'}), tags=None, parameters={'user_question': '', 'file_structure': ''}, remember_chat_context=True), outputs=[]), 'params': {'user_question': 'Where can i update wrappers for diffusion models', 'file_structure': ''}} ts_ns=1702173808368226500
2023-12-09 21:04:26,899 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'frequency_penalty': 0, 'model': 'gpt-4', 'top_p': 1, 'presence_penalty': 0, 'temperature': 0.75, 'messages': [{'content': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is . The READMEfile is:\n', 'role': 'user'}, {'content': 'README Summary: \n\nFile Structure:\n\n\nUser Question: Where can i update wrappers for diffusion models', 'role': 'user'}]}} ts_ns=1702173808368226500
2023-12-09 21:04:26,899 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'frequency_penalty': 0, 'model': 'gpt-4', 'top_p': 1, 'presence_penalty': 0, 'temperature': 0.75, 'messages': [{'content': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is . The READMEfile is:\n', 'role': 'user'}, {'content': 'README Summary: \n\nFile Structure:\n\n\nUser Question: Where can i update wrappers for diffusion models', 'role': 'user'}]}} ts_ns=1702173808368226500
2023-12-09 21:04:26,899 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'frequency_penalty': 0, 'model': 'gpt-4', 'top_p': 1, 'presence_penalty': 0, 'temperature': 0.75, 'messages': [{'content': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is . The READMEfile is:\n', 'role': 'user'}, {'content': 'README Summary: \n\nFile Structure:\n\n\nUser Question: Where can i update wrappers for diffusion models', 'role': 'user'}]}} ts_ns=1702173808368226500
2023-12-09 21:04:26,899 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'frequency_penalty': 0, 'model': 'gpt-4', 'top_p': 1, 'presence_penalty': 0, 'temperature': 0.75, 'messages': [{'content': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is . The READMEfile is:\n', 'role': 'user'}, {'content': 'README Summary: \n\nFile Structure:\n\n\nUser Question: Where can i update wrappers for diffusion models', 'role': 'user'}]}} ts_ns=1702173808368226500
2023-12-09 21:04:26,899 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'frequency_penalty': 0, 'model': 'gpt-4', 'top_p': 1, 'presence_penalty': 0, 'temperature': 0.75, 'messages': [{'content': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is . The READMEfile is:\n', 'role': 'user'}, {'content': 'README Summary: \n\nFile Structure:\n\n\nUser Question: Where can i update wrappers for diffusion models', 'role': 'user'}]}} ts_ns=1702173808368226500
2023-12-09 21:04:26,899 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'frequency_penalty': 0, 'model': 'gpt-4', 'top_p': 1, 'presence_penalty': 0, 'temperature': 0.75, 'messages': [{'content': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is . The READMEfile is:\n', 'role': 'user'}, {'content': 'README Summary: \n\nFile Structure:\n\n\nUser Question: Where can i update wrappers for diffusion models', 'role': 'user'}]}} ts_ns=1702173808368226500
2023-12-09 21:04:26,899 - my-logger - INFO - Callback called. event
: name='on_deserialize_complete' file='aiconfig.default_parsers.openai' data={'output': {'frequency_penalty': 0, 'model': 'gpt-4', 'top_p': 1, 'presence_penalty': 0, 'temperature': 0.75, 'messages': [{'content': 'You are an expert in coding, file layouts, and working in GitHub on open-source projects. You are capable of answering questions about a repository given a summary of the README and the file structure of the repository. Try to make specific references to files/directories and steps to further answer user questions. Be friendly and ensure responses are detailed and help guide the user on how to explore where they want and make changes.', 'role': 'system'}, {'content': 'Please summarize this repository. The repository name is . The READMEfile is:\n', 'role': 'user'}, {'content': 'README Summary: \n\nFile Structure:\n\n\nUser Question: Where can i update wrappers for diffusion models', 'role': 'user'}]}} ts_ns=1702173808368226500
2023-12-09 21:04:29,194 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but you haven't provided any content for the README or the file structure of the repository. I need this information to be able to help you. Could you please provide the details?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:29,194 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but you haven't provided any content for the README or the file structure of the repository. I need this information to be able to help you. Could you please provide the details?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:29,194 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but you haven't provided any content for the README or the file structure of the repository. I need this information to be able to help you. Could you please provide the details?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:29,194 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but you haven't provided any content for the README or the file structure of the repository. I need this information to be able to help you. Could you please provide the details?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:29,194 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but you haven't provided any content for the README or the file structure of the repository. I need this information to be able to help you. Could you please provide the details?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:29,194 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but you haven't provided any content for the README or the file structure of the repository. I need this information to be able to help you. Could you please provide the details?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:29,194 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.default_parsers.openai' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but you haven't provided any content for the README or the file structure of the repository. I need this information to be able to help you. Could you please provide the details?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:29,195 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but you haven't provided any content for the README or the file structure of the repository. I need this information to be able to help you. Could you please provide the details?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:29,195 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but you haven't provided any content for the README or the file structure of the repository. I need this information to be able to help you. Could you please provide the details?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:29,195 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but you haven't provided any content for the README or the file structure of the repository. I need this information to be able to help you. Could you please provide the details?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:29,195 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but you haven't provided any content for the README or the file structure of the repository. I need this information to be able to help you. Could you please provide the details?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:29,195 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but you haven't provided any content for the README or the file structure of the repository. I need this information to be able to help you. Could you please provide the details?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:29,195 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but you haven't provided any content for the README or the file structure of the repository. I need this information to be able to help you. Could you please provide the details?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
2023-12-09 21:04:29,195 - my-logger - INFO - Callback called. event
: name='on_run_complete' file='aiconfig.Config' data={'result': [ExecuteResult(output_type='execute_result', execution_count=0, data={'content': "I'm sorry, but you haven't provided any content for the README or the file structure of the repository. I need this information to be able to help you. Could you please provide the details?", 'role': 'assistant'}, mime_type=None, metadata={'finish_reason': 'stop'})]} ts_ns=1702173808368226500
>>>>>>> 3a9929e94ca0aa299936061be63b6528d13e7666
